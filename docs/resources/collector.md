---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "criblio_collector Resource - terraform-provider-criblio"
subcategory: ""
description: |-
  Collector Resource
---

# criblio_collector (Resource)

Collector Resource

## Example Usage

```terraform
resource "criblio_collector" "my_collector" {
  group_id = "default"
  id       = "collector-2"
  input_collector_azure_blob = {
    collector = {
      conf = {
        auth_type         = "manual"
        connection_string = "DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=abc123;EndpointSuffix=core.windows.net"
        container_name    = "my-container"
        extractors = [
          {
            # ...
          }
        ]
        max_batch_size       = 500
        path                 = "container/logs/2025/10/"
        recurse              = true
        storage_account_name = "mystorageaccount"
      }
      type = "azureblob"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_cribl_lake = {
    collector = {
      conf = {
        dataset = "my-dataset"
      }
      type = "cribllake"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_database = {
    collector = {
      conf = {
        connection_id            = "myDatabaseConnection"
        query                    = "SELECT * FROM logs WHERE severity = 'ERROR'"
        query_validation_enabled = true
      }
      type = "database"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_gcs = {
    collector = {
      conf = {
        auth_type = "manual"
        bucket    = "my-gcs-bucket"
        extractors = [
          {
            # ...
          }
        ]
        max_batch_size              = 200
        path                        = "logs/2025/10/"
        recurse                     = true
        service_account_credentials = "***REDACTED***"
      }
      type = "gcs"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_health_check = {
    collector = {
      conf = {
        authentication      = "none"
        collect_method      = "get"
        collect_url         = "https://api.example.com/health"
        credentials_secret  = "healthCredSecret"
        password            = "healthPassword"
        reject_unauthorized = true
        timeout             = 10
        username            = "healthUser"
      }
      type = "healthcheck"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_rest = {
    collector = {
      conf = {
        auth_header_expr = "Bearer ${token}"
        auth_header_key  = "Authorization"
        auth_request_headers = [
          {
            name  = "Content-Type"
            value = "application/json"
          }
        ]
        auth_request_params = [
          {
            name  = "param1"
            value = "value1"
          }
        ]
        authentication           = "basic"
        capture_headers          = true
        client_secret_param_name = "client_secret"
        collect_method           = "get"
        collect_request_headers = [
          {
            name  = "Accept"
            value = "application/json"
          }
        ]
        collect_request_params = [
          {
            name  = "limit"
            value = "100"
          }
        ]
        collect_url         = "https://api.example.com/data"
        credentials_secret  = "restCredentialsSecret"
        decode_url          = true
        disable_time_filter = false
        discovery = {
          discover_body       = "{\"discover\":true}"
          discover_data_field = "results"
          discover_method     = "get"
          discover_request_headers = [
            {
              name  = "Authorization"
              value = "Bearer token"
            }
          ]
          discover_request_params = [
            {
              # ...
            }
          ]
          discover_type        = "http"
          discover_url         = "https://api.example.com/discover"
          enable_discover_code = true
          format_result_code   = "200"
          item_list = [
            "item1",
            "item2",
          ]
          pagination = {
            attribute = [
              "records",
            ]
            last_page_expr     = "$.data.isLastPage"
            limit              = 100
            limit_field        = "limit"
            max_pages          = 10
            offset             = 0
            offset_field       = "offset"
            page_field         = "page"
            size               = 50
            size_field         = "pageSize"
            total_record_field = "totalRecords"
            type               = "offset"
            zero_indexed       = true
          }
        }
        login_body = "{\"username\":\"user\",\"password\":\"pass\"}"
        login_url  = "https://api.example.com/login"
        pagination = {
          attribute = [
            "records",
          ]
          last_page_expr     = "$.data.isLastPage"
          limit              = 100
          limit_field        = "limit"
          max_pages          = 10
          offset             = 0
          offset_field       = "offset"
          page_field         = "page"
          size               = 50
          size_field         = "pageSize"
          total_record_field = "totalRecords"
          type               = "offset"
          zero_indexed       = true
        }
        password            = "restPassword"
        reject_unauthorized = true
        retry_rules = {
          codes = [
            429,
            500,
          ]
          enable_header         = true
          interval              = 500
          limit                 = 3
          max_interval_ms       = 10000
          multiplier            = 1.5
          retry_connect_reset   = false
          retry_connect_timeout = true
          retry_header_name     = "Retry-After"
          type                  = "backoff"
        }
        safe_headers = [
          "Content-Type",
          "Authorization",
        ]
        scheduling = {
          state_tracking = {
            # ...
          }
        }
        timeout              = 30
        token                = "restBearerToken"
        token_resp_attribute = "access_token"
        token_secret         = "restBearerTokenSecret"
        use_round_robin_dns  = false
        username             = "restUser"
      }
      type = "rest"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_s3 = {
    collector = {
      conf = {
        aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
        aws_authentication_method = "auto"
        aws_secret                = "awsSecretPairId"
        aws_secret_key            = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
        bucket                    = "my-s3-bucket"
        extractors = [
          {
            # ...
          }
        ]
        max_batch_size = 100
        path           = "logs/2025/10/"
        recurse        = true
        region         = "us-east-1"
      }
      type = "s3"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
  input_collector_splunk = {
    collector = {
      conf = {
        authentication       = "token"
        credentials_secret   = "splunkCredentialsSecret"
        disable_time_filter  = false
        earliest             = "-24h"
        endpoint             = "/services/search/jobs"
        handle_escaped_chars = true
        latest               = "now"
        output_mode          = "json"
        password             = "splunkPassword"
        reject_unauthorized  = true
        search               = "index=main sourcetype=syslog | stats count"
        search_head          = "https://splunk.example.com"
        timeout              = 60
        token                = "splunkBearerToken"
        token_secret         = "splunkBearerTokenSecret"
        use_round_robin_dns  = false
        username             = "splunkUser"
      }
      type = "splunk"
    }
    environment             = "production"
    id                      = "myInputCollectorJobId"
    ignore_group_jobs_limit = false
    input = {
      breaker_rulesets = [
        "rule1",
        "rule2",
      ]
      metadata = [
        {
          name  = "sourceType"
          value = "`value_expression`"
        }
      ]
      output   = "defaultDestination"
      pipeline = "defaultPipeline"
      preprocess = {
        args = [
          "--flag",
          "value",
        ]
        command  = "cat"
        disabled = true
      }
      send_to_routes         = true
      stale_channel_flush_ms = 20000
      throttle_rate_per_sec  = "42 MB"
      type                   = "collection"
    }
    remove_fields = [
      "field1",
      "field2",
    ]
    resume_on_boot = true
    saved_state = {
      # ...
    }
    schedule = {
      cron_schedule       = "0 * * * *"
      enabled             = true
      max_concurrent_runs = 2
      resume_missed       = true
      run = {
        earliest                 = 0
        expression               = "true"
        job_timeout              = "30m"
        latest                   = 10
        log_level                = "debug"
        max_task_reschedule      = 3
        max_task_size            = "10GB"
        min_task_size            = "1GB"
        mode                     = "list"
        reschedule_dropped_tasks = true
        state_tracking = {
          enabled                 = true
          state_merge_expression  = "merge(state,newState)"
          state_update_expression = "state = state + 1"
        }
        time_range_type = "relative"
        time_warning = {
          # ...
        }
      }
      skippable = false
    }
    streamtags = [
      "tag1",
      "tag2",
    ]
    ttl             = "2h"
    worker_affinity = false
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `group_id` (String) The consumer group to which this instance belongs. Defaults to 'default'.
- `id` (String) The id of this collector instance

### Optional

- `input_collector_azure_blob` (Attributes) (see [below for nested schema](#nestedatt--input_collector_azure_blob))
- `input_collector_cribl_lake` (Attributes) (see [below for nested schema](#nestedatt--input_collector_cribl_lake))
- `input_collector_database` (Attributes) (see [below for nested schema](#nestedatt--input_collector_database))
- `input_collector_gcs` (Attributes) (see [below for nested schema](#nestedatt--input_collector_gcs))
- `input_collector_health_check` (Attributes) (see [below for nested schema](#nestedatt--input_collector_health_check))
- `input_collector_rest` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest))
- `input_collector_s3` (Attributes) (see [below for nested schema](#nestedatt--input_collector_s3))
- `input_collector_splunk` (Attributes) (see [below for nested schema](#nestedatt--input_collector_splunk))

<a id="nestedatt--input_collector_azure_blob"></a>
### Nested Schema for `input_collector_azure_blob`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_azure_blob--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_azure_blob--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_azure_blob--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_azure_blob--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_azure_blob--collector"></a>
### Nested Schema for `input_collector_azure_blob.collector`

Required:

- `type` (String) must be "azureblob"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_azure_blob--collector--conf))

<a id="nestedatt--input_collector_azure_blob--collector--conf"></a>
### Nested Schema for `input_collector_azure_blob.collector.conf`

Optional:

- `auth_type` (String) must be one of ["manual", "secret", "clientSecret", "clientCert"]
- `connection_string` (String) Azure storage account Connection String
- `container_name` (String) Azure container to collect from
- `extractors` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_azure_blob--collector--conf--extractors))
- `max_batch_size` (Number)
- `path` (String) Directory where data will be collected
- `recurse` (Boolean)
- `storage_account_name` (String)

<a id="nestedatt--input_collector_azure_blob--collector--conf--extractors"></a>
### Nested Schema for `input_collector_azure_blob.collector.conf.extractors`




<a id="nestedatt--input_collector_azure_blob--input"></a>
### Nested Schema for `input_collector_azure_blob.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_azure_blob--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_azure_blob--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_azure_blob--input--metadata"></a>
### Nested Schema for `input_collector_azure_blob.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_azure_blob--input--preprocess"></a>
### Nested Schema for `input_collector_azure_blob.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_azure_blob--saved_state"></a>
### Nested Schema for `input_collector_azure_blob.saved_state`


<a id="nestedatt--input_collector_azure_blob--schedule"></a>
### Nested Schema for `input_collector_azure_blob.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_azure_blob--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_azure_blob--schedule--run"></a>
### Nested Schema for `input_collector_azure_blob.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_azure_blob--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_azure_blob--schedule--run--time_warning))

<a id="nestedatt--input_collector_azure_blob--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_azure_blob.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_azure_blob--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_azure_blob.schedule.run.time_warning`





<a id="nestedatt--input_collector_cribl_lake"></a>
### Nested Schema for `input_collector_cribl_lake`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_cribl_lake--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_cribl_lake--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_cribl_lake--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_cribl_lake--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_cribl_lake--collector"></a>
### Nested Schema for `input_collector_cribl_lake.collector`

Required:

- `type` (String) must be "cribllake"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_cribl_lake--collector--conf))

<a id="nestedatt--input_collector_cribl_lake--collector--conf"></a>
### Nested Schema for `input_collector_cribl_lake.collector.conf`

Optional:

- `dataset` (String) Lake dataset to collect data from



<a id="nestedatt--input_collector_cribl_lake--input"></a>
### Nested Schema for `input_collector_cribl_lake.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_cribl_lake--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_cribl_lake--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_cribl_lake--input--metadata"></a>
### Nested Schema for `input_collector_cribl_lake.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_cribl_lake--input--preprocess"></a>
### Nested Schema for `input_collector_cribl_lake.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_cribl_lake--saved_state"></a>
### Nested Schema for `input_collector_cribl_lake.saved_state`


<a id="nestedatt--input_collector_cribl_lake--schedule"></a>
### Nested Schema for `input_collector_cribl_lake.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_cribl_lake--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_cribl_lake--schedule--run"></a>
### Nested Schema for `input_collector_cribl_lake.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_cribl_lake--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_cribl_lake--schedule--run--time_warning))

<a id="nestedatt--input_collector_cribl_lake--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_cribl_lake.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_cribl_lake--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_cribl_lake.schedule.run.time_warning`





<a id="nestedatt--input_collector_database"></a>
### Nested Schema for `input_collector_database`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_database--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_database--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_database--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_database--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_database--collector"></a>
### Nested Schema for `input_collector_database.collector`

Required:

- `type` (String) must be "database"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_database--collector--conf))

<a id="nestedatt--input_collector_database--collector--conf"></a>
### Nested Schema for `input_collector_database.collector.conf`

Optional:

- `connection_id` (String) Select an existing Database Connection
- `query` (String) Query string for selecting data from the database
- `query_validation_enabled` (Boolean)



<a id="nestedatt--input_collector_database--input"></a>
### Nested Schema for `input_collector_database.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_database--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_database--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_database--input--metadata"></a>
### Nested Schema for `input_collector_database.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_database--input--preprocess"></a>
### Nested Schema for `input_collector_database.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_database--saved_state"></a>
### Nested Schema for `input_collector_database.saved_state`


<a id="nestedatt--input_collector_database--schedule"></a>
### Nested Schema for `input_collector_database.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_database--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_database--schedule--run"></a>
### Nested Schema for `input_collector_database.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_database--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_database--schedule--run--time_warning))

<a id="nestedatt--input_collector_database--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_database.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_database--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_database.schedule.run.time_warning`





<a id="nestedatt--input_collector_gcs"></a>
### Nested Schema for `input_collector_gcs`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_gcs--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_gcs--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_gcs--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_gcs--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_gcs--collector"></a>
### Nested Schema for `input_collector_gcs.collector`

Required:

- `type` (String) must be "gcs"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_gcs--collector--conf))

<a id="nestedatt--input_collector_gcs--collector--conf"></a>
### Nested Schema for `input_collector_gcs.collector.conf`

Optional:

- `auth_type` (String) must be one of ["manual", "secret", "clientSecret", "clientCert"]
- `bucket` (String) GCS Bucket from which to collect data
- `extractors` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_gcs--collector--conf--extractors))
- `max_batch_size` (Number)
- `path` (String) Directory where data will be collected
- `recurse` (Boolean)
- `service_account_credentials` (String)

<a id="nestedatt--input_collector_gcs--collector--conf--extractors"></a>
### Nested Schema for `input_collector_gcs.collector.conf.extractors`




<a id="nestedatt--input_collector_gcs--input"></a>
### Nested Schema for `input_collector_gcs.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_gcs--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_gcs--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_gcs--input--metadata"></a>
### Nested Schema for `input_collector_gcs.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_gcs--input--preprocess"></a>
### Nested Schema for `input_collector_gcs.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_gcs--saved_state"></a>
### Nested Schema for `input_collector_gcs.saved_state`


<a id="nestedatt--input_collector_gcs--schedule"></a>
### Nested Schema for `input_collector_gcs.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_gcs--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_gcs--schedule--run"></a>
### Nested Schema for `input_collector_gcs.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_gcs--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_gcs--schedule--run--time_warning))

<a id="nestedatt--input_collector_gcs--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_gcs.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_gcs--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_gcs.schedule.run.time_warning`





<a id="nestedatt--input_collector_health_check"></a>
### Nested Schema for `input_collector_health_check`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_health_check--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_health_check--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_health_check--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_health_check--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_health_check--collector"></a>
### Nested Schema for `input_collector_health_check.collector`

Required:

- `type` (String) must be "healthcheck"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_health_check--collector--conf))

<a id="nestedatt--input_collector_health_check--collector--conf"></a>
### Nested Schema for `input_collector_health_check.collector.conf`

Optional:

- `authentication` (String) must be one of ["none", "basic", "basicSecret", "token", "tokenSecret", "login", "loginSecret", "oauth", "oauthSecret", "google_oauth", "google_oauthSecret", "hmac"]
- `collect_method` (String) must be one of ["get", "post", "post_with_body", "other"]
- `collect_url` (String) URL to use for the Collect operation
- `credentials_secret` (String)
- `password` (String)
- `reject_unauthorized` (Boolean)
- `timeout` (Number)
- `username` (String)



<a id="nestedatt--input_collector_health_check--input"></a>
### Nested Schema for `input_collector_health_check.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_health_check--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_health_check--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_health_check--input--metadata"></a>
### Nested Schema for `input_collector_health_check.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_health_check--input--preprocess"></a>
### Nested Schema for `input_collector_health_check.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_health_check--saved_state"></a>
### Nested Schema for `input_collector_health_check.saved_state`


<a id="nestedatt--input_collector_health_check--schedule"></a>
### Nested Schema for `input_collector_health_check.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_health_check--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_health_check--schedule--run"></a>
### Nested Schema for `input_collector_health_check.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_health_check--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_health_check--schedule--run--time_warning))

<a id="nestedatt--input_collector_health_check--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_health_check.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_health_check--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_health_check.schedule.run.time_warning`





<a id="nestedatt--input_collector_rest"></a>
### Nested Schema for `input_collector_rest`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_rest--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_rest--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_rest--collector"></a>
### Nested Schema for `input_collector_rest.collector`

Required:

- `type` (String) must be "rest"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf))

<a id="nestedatt--input_collector_rest--collector--conf"></a>
### Nested Schema for `input_collector_rest.collector.conf`

Optional:

- `auth_header_expr` (String) Expression for auth header value
- `auth_header_key` (String) Header key for authentication
- `auth_request_headers` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--auth_request_headers))
- `auth_request_params` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--auth_request_params))
- `authentication` (String) must be one of ["none", "basic", "basicSecret", "token", "tokenSecret", "login", "loginSecret", "oauth", "oauthSecret", "google_oauth", "google_oauthSecret", "hmac"]
- `capture_headers` (Boolean) Default: false
- `client_secret_param_name` (String)
- `collect_method` (String) must be one of ["get", "post", "post_with_body", "other"]
- `collect_request_headers` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--collect_request_headers))
- `collect_request_params` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--collect_request_params))
- `collect_url` (String) URL to use for the Collect operation
- `credentials_secret` (String)
- `decode_url` (Boolean) Default: false
- `disable_time_filter` (Boolean)
- `discovery` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--discovery))
- `login_body` (String) Body content for login request
- `login_url` (String) URL for authentication login
- `pagination` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--pagination))
- `password` (String)
- `reject_unauthorized` (Boolean)
- `retry_rules` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--retry_rules))
- `safe_headers` (List of String)
- `scheduling` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--scheduling))
- `timeout` (Number)
- `token` (String)
- `token_resp_attribute` (String) Attribute name for token in response
- `token_secret` (String)
- `use_round_robin_dns` (Boolean)
- `username` (String)

<a id="nestedatt--input_collector_rest--collector--conf--auth_request_headers"></a>
### Nested Schema for `input_collector_rest.collector.conf.auth_request_headers`

Optional:

- `name` (String)
- `value` (String)


<a id="nestedatt--input_collector_rest--collector--conf--auth_request_params"></a>
### Nested Schema for `input_collector_rest.collector.conf.auth_request_params`

Optional:

- `name` (String)
- `value` (String)


<a id="nestedatt--input_collector_rest--collector--conf--collect_request_headers"></a>
### Nested Schema for `input_collector_rest.collector.conf.collect_request_headers`

Optional:

- `name` (String)
- `value` (String)


<a id="nestedatt--input_collector_rest--collector--conf--collect_request_params"></a>
### Nested Schema for `input_collector_rest.collector.conf.collect_request_params`

Optional:

- `name` (String)
- `value` (String)


<a id="nestedatt--input_collector_rest--collector--conf--discovery"></a>
### Nested Schema for `input_collector_rest.collector.conf.discovery`

Optional:

- `discover_body` (String)
- `discover_data_field` (String)
- `discover_method` (String) must be one of ["get", "post", "post_with_body", "other"]
- `discover_request_headers` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--discovery--discover_request_headers))
- `discover_request_params` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--discovery--discover_request_params))
- `discover_type` (String) must be "http"
- `discover_url` (String)
- `enable_discover_code` (Boolean) Default: false
- `format_result_code` (String)
- `item_list` (List of String)
- `pagination` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--discovery--pagination))

<a id="nestedatt--input_collector_rest--collector--conf--discovery--discover_request_headers"></a>
### Nested Schema for `input_collector_rest.collector.conf.discovery.discover_request_headers`

Optional:

- `name` (String)
- `value` (String)


<a id="nestedatt--input_collector_rest--collector--conf--discovery--discover_request_params"></a>
### Nested Schema for `input_collector_rest.collector.conf.discovery.discover_request_params`


<a id="nestedatt--input_collector_rest--collector--conf--discovery--pagination"></a>
### Nested Schema for `input_collector_rest.collector.conf.discovery.pagination`

Optional:

- `attribute` (List of String)
- `last_page_expr` (String)
- `limit` (Number) Default: 100
- `limit_field` (String)
- `max_pages` (Number) Default: 0
- `offset` (Number)
- `offset_field` (String)
- `page_field` (String)
- `size` (Number) Default: 50
- `size_field` (String)
- `total_record_field` (String)
- `type` (String) Default: "none"; must be one of ["none", "offset", "cursor", "page"]
- `zero_indexed` (Boolean) Default: false



<a id="nestedatt--input_collector_rest--collector--conf--pagination"></a>
### Nested Schema for `input_collector_rest.collector.conf.pagination`

Optional:

- `attribute` (List of String)
- `last_page_expr` (String)
- `limit` (Number) Default: 100
- `limit_field` (String)
- `max_pages` (Number) Default: 0
- `offset` (Number)
- `offset_field` (String)
- `page_field` (String)
- `size` (Number) Default: 50
- `size_field` (String)
- `total_record_field` (String)
- `type` (String) Default: "none"; must be one of ["none", "offset", "cursor", "page"]
- `zero_indexed` (Boolean) Default: false


<a id="nestedatt--input_collector_rest--collector--conf--retry_rules"></a>
### Nested Schema for `input_collector_rest.collector.conf.retry_rules`

Optional:

- `codes` (List of Number) Default: [429,503]
- `enable_header` (Boolean) Default: true
- `interval` (Number) Default: 1000
- `limit` (Number) Default: 5
- `max_interval_ms` (Number) Default: 20000
- `multiplier` (Number) Default: 2
- `retry_connect_reset` (Boolean) Default: false
- `retry_connect_timeout` (Boolean) Default: false
- `retry_header_name` (String) Default: "retry-after"
- `type` (String) Default: "backoff"; must be one of ["backoff", "fixed"]


<a id="nestedatt--input_collector_rest--collector--conf--scheduling"></a>
### Nested Schema for `input_collector_rest.collector.conf.scheduling`

Optional:

- `state_tracking` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--collector--conf--scheduling--state_tracking))

<a id="nestedatt--input_collector_rest--collector--conf--scheduling--state_tracking"></a>
### Nested Schema for `input_collector_rest.collector.conf.scheduling.state_tracking`





<a id="nestedatt--input_collector_rest--input"></a>
### Nested Schema for `input_collector_rest.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_rest--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_rest--input--metadata"></a>
### Nested Schema for `input_collector_rest.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_rest--input--preprocess"></a>
### Nested Schema for `input_collector_rest.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_rest--saved_state"></a>
### Nested Schema for `input_collector_rest.saved_state`


<a id="nestedatt--input_collector_rest--schedule"></a>
### Nested Schema for `input_collector_rest.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_rest--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_rest--schedule--run"></a>
### Nested Schema for `input_collector_rest.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_rest--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_rest--schedule--run--time_warning))

<a id="nestedatt--input_collector_rest--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_rest.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_rest--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_rest.schedule.run.time_warning`





<a id="nestedatt--input_collector_s3"></a>
### Nested Schema for `input_collector_s3`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_s3--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_s3--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_s3--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_s3--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_s3--collector"></a>
### Nested Schema for `input_collector_s3.collector`

Required:

- `type` (String) must be "s3"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_s3--collector--conf))

<a id="nestedatt--input_collector_s3--collector--conf"></a>
### Nested Schema for `input_collector_s3.collector.conf`

Optional:

- `aws_api_key` (String)
- `aws_authentication_method` (String) must be one of ["auto", "manual", "secret"]
- `aws_secret` (String)
- `aws_secret_key` (String)
- `bucket` (String) S3 Bucket from which to collect data
- `extractors` (Attributes List) (see [below for nested schema](#nestedatt--input_collector_s3--collector--conf--extractors))
- `max_batch_size` (Number)
- `path` (String) Directory where data will be collected
- `recurse` (Boolean)
- `region` (String) AWS region from which to retrieve data

<a id="nestedatt--input_collector_s3--collector--conf--extractors"></a>
### Nested Schema for `input_collector_s3.collector.conf.extractors`




<a id="nestedatt--input_collector_s3--input"></a>
### Nested Schema for `input_collector_s3.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_s3--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_s3--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_s3--input--metadata"></a>
### Nested Schema for `input_collector_s3.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_s3--input--preprocess"></a>
### Nested Schema for `input_collector_s3.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_s3--saved_state"></a>
### Nested Schema for `input_collector_s3.saved_state`


<a id="nestedatt--input_collector_s3--schedule"></a>
### Nested Schema for `input_collector_s3.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_s3--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_s3--schedule--run"></a>
### Nested Schema for `input_collector_s3.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_s3--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_s3--schedule--run--time_warning))

<a id="nestedatt--input_collector_s3--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_s3.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_s3--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_s3.schedule.run.time_warning`





<a id="nestedatt--input_collector_splunk"></a>
### Nested Schema for `input_collector_splunk`

Required:

- `collector` (Attributes) (see [below for nested schema](#nestedatt--input_collector_splunk--collector))

Optional:

- `environment` (String)
- `id` (String)
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input_collector_splunk--input))
- `remove_fields` (List of String) Default: []
- `resume_on_boot` (Boolean) Default: true
- `saved_state` (Attributes) Saved state for the collector (see [below for nested schema](#nestedatt--input_collector_splunk--saved_state))
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--input_collector_splunk--schedule))
- `streamtags` (List of String) Tags for filtering and grouping. Default: []
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--input_collector_splunk--collector"></a>
### Nested Schema for `input_collector_splunk.collector`

Required:

- `type` (String) must be "splunk"

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--input_collector_splunk--collector--conf))

<a id="nestedatt--input_collector_splunk--collector--conf"></a>
### Nested Schema for `input_collector_splunk.collector.conf`

Optional:

- `authentication` (String) must be one of ["none", "basic", "basicSecret", "token", "tokenSecret", "login", "loginSecret", "oauth", "oauthSecret", "google_oauth", "google_oauthSecret", "hmac"]
- `credentials_secret` (String)
- `disable_time_filter` (Boolean)
- `earliest` (String) Earliest time boundary for the search
- `endpoint` (String) REST API endpoint used to create a search
- `handle_escaped_chars` (Boolean)
- `latest` (String) Latest time boundary for the search
- `output_mode` (String) must be one of ["csv", "json"]
- `password` (String)
- `reject_unauthorized` (Boolean)
- `search` (String) Splunk search query
- `search_head` (String) Search head base URL
- `timeout` (Number)
- `token` (String)
- `token_secret` (String)
- `use_round_robin_dns` (Boolean)
- `username` (String)



<a id="nestedatt--input_collector_splunk--input"></a>
### Nested Schema for `input_collector_splunk.input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input_collector_splunk--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input_collector_splunk--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input_collector_splunk--input--metadata"></a>
### Nested Schema for `input_collector_splunk.input.metadata`

Required:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input_collector_splunk--input--preprocess"></a>
### Nested Schema for `input_collector_splunk.input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--input_collector_splunk--saved_state"></a>
### Nested Schema for `input_collector_splunk.saved_state`


<a id="nestedatt--input_collector_splunk--schedule"></a>
### Nested Schema for `input_collector_splunk.schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `resume_missed` (Boolean) Resume missed scheduled runs. Default: false
- `run` (Attributes) (see [below for nested schema](#nestedatt--input_collector_splunk--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--input_collector_splunk--schedule--run"></a>
### Nested Schema for `input_collector_splunk.schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone. Default: 0
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone. Default: 1
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"; must be one of ["list", "preview", "run"]
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `state_tracking` (Attributes) State tracking configuration (see [below for nested schema](#nestedatt--input_collector_splunk--schedule--run--state_tracking))
- `time_range_type` (String) Default: "relative"; must be one of ["relative", "absolute"]
- `time_warning` (Attributes) Time warning configuration (see [below for nested schema](#nestedatt--input_collector_splunk--schedule--run--time_warning))

<a id="nestedatt--input_collector_splunk--schedule--run--state_tracking"></a>
### Nested Schema for `input_collector_splunk.schedule.run.state_tracking`

Optional:

- `enabled` (Boolean) Default: false
- `state_merge_expression` (String)
- `state_update_expression` (String)


<a id="nestedatt--input_collector_splunk--schedule--run--time_warning"></a>
### Nested Schema for `input_collector_splunk.schedule.run.time_warning`

## Import

Import is supported using the following syntax:

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = criblio_collector.my_criblio_collector
  id = jsonencode({
    group_id = "myExistingGroupId"
    id = "myExistingJobId"
  })
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
terraform import criblio_collector.my_criblio_collector '{"group_id": "myExistingGroupId", "id": "myExistingJobId"}'
```
