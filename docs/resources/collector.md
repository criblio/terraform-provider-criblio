---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "criblio_collector Resource - terraform-provider-criblio"
subcategory: ""
description: |-
  Collector Resource
---

# criblio_collector (Resource)

Collector Resource

## Example Usage

```terraform
resource "criblio_collector" "my_collector" {
  collector = {
    conf = {
      auth_type                 = "secret"
      authentication            = "oauthSecret"
      aws_api_key               = "...my_aws_api_key..."
      aws_authentication_method = "secret"
      aws_secret                = "...my_aws_secret..."
      aws_secret_key            = "...my_aws_secret_key..."
      bucket                    = "...my_bucket..."
      collect_method            = "other"
      collect_url               = "...my_collect_url..."
      connection_id             = "...my_connection_id..."
      connection_string         = "...my_connection_string..."
      container_name            = "...my_container_name..."
      credentials_secret        = "...my_credentials_secret..."
      dataset                   = "...my_dataset..."
      disable_time_filter       = true
      earliest                  = "...my_earliest..."
      endpoint                  = "...my_endpoint..."
      extractors = [
        {
          # ...
        }
      ]
      handle_escaped_chars        = true
      latest                      = "...my_latest..."
      max_batch_size              = 9
      output_mode                 = "json"
      password                    = "...my_password..."
      path                        = "...my_path..."
      query                       = "...my_query..."
      query_validation_enabled    = false
      recurse                     = false
      region                      = "...my_region..."
      reject_unauthorized         = true
      search                      = "...my_search..."
      search_head                 = "...my_search_head..."
      service_account_credentials = "...my_service_account_credentials..."
      storage_account_name        = "...my_storage_account_name..."
      timeout                     = 61
      token                       = "...my_token..."
      token_secret                = "...my_token_secret..."
      use_round_robin_dns         = false
      username                    = "...my_username..."
    }
    destructive = true
    encoding    = "...my_encoding..."
    type        = "rest"
  }
  environment             = "...my_environment..."
  group_id                = "default"
  id                      = "...my_id..."
  ignore_group_jobs_limit = false
  input = {
    breaker_rulesets = [
      "..."
    ]
    metadata = [
      {
        name  = "...my_name..."
        value = "...my_value..."
      }
    ]
    output   = "...my_output..."
    pipeline = "...my_pipeline..."
    preprocess = {
      args = [
        "..."
      ]
      command  = "...my_command..."
      disabled = true
    }
    send_to_routes         = false
    stale_channel_flush_ms = 3239544.85
    throttle_rate_per_sec  = "...my_throttle_rate_per_sec..."
    type                   = "collection"
  }
  remove_fields = [
    "..."
  ]
  resume_on_boot = true
  schedule = {
    cron_schedule       = "...my_cron_schedule..."
    enabled             = true
    max_concurrent_runs = 3.71
    run = {
      earliest                 = 4.02
      expression               = "...my_expression..."
      job_timeout              = "...my_job_timeout..."
      latest                   = 5.4
      log_level                = "silly"
      max_task_reschedule      = 7.19
      max_task_size            = "...my_max_task_size..."
      min_task_size            = "...my_min_task_size..."
      mode                     = "...my_mode..."
      reschedule_dropped_tasks = false
      time_range_type          = "...my_time_range_type..."
    }
    skippable = false
  }
  streamtags = [
    "..."
  ]
  ttl             = "...my_ttl..."
  worker_affinity = false
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `collector` (Attributes) (see [below for nested schema](#nestedatt--collector))
- `group_id` (String) The consumer group to which this instance belongs. Defaults to 'default'.

### Optional

- `environment` (String)
- `id` (String) Unique ID to PATCH
- `ignore_group_jobs_limit` (Boolean) Default: false
- `input` (Attributes) (see [below for nested schema](#nestedatt--input))
- `remove_fields` (List of String)
- `resume_on_boot` (Boolean) Default: true
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--schedule))
- `streamtags` (List of String) Tags for filtering and grouping
- `ttl` (String) Default: "4h"
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node. Default: false

<a id="nestedatt--collector"></a>
### Nested Schema for `collector`

Required:

- `type` (String) must be one of ["splunk", "s3", "azureblob", "cribllake", "database", "gcs", "healthcheck", "rest"]

Optional:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--collector--conf))
- `destructive` (Boolean) Delete any files collected (where applicable). Default: false
- `encoding` (String) Character encoding to use when parsing ingested data. Default: "utf8"

<a id="nestedatt--collector--conf"></a>
### Nested Schema for `collector.conf`

Optional:

- `auth_type` (String) must be one of ["manual", "secret", "clientSecret", "clientCert"]
- `authentication` (String) must be one of ["none", "basic", "basicSecret", "token", "tokenSecret", "login", "loginSecret", "oauth", "oauthSecret", "google_oauth", "google_oauthSecret", "hmac"]
- `aws_api_key` (String)
- `aws_authentication_method` (String) must be one of ["auto", "manual", "secret"]
- `aws_secret` (String)
- `aws_secret_key` (String)
- `bucket` (String) S3 Bucket from which to collect data
- `collect_method` (String) must be one of ["get", "post", "post_with_body", "other"]
- `collect_url` (String) URL to use for the Collect operation
- `connection_id` (String) Select an existing Database Connection
- `connection_string` (String) Azure storage account Connection String
- `container_name` (String) Azure container to collect from
- `credentials_secret` (String)
- `dataset` (String) Lake dataset to collect data from
- `disable_time_filter` (Boolean)
- `earliest` (String) Earliest time boundary for the search
- `endpoint` (String) REST API endpoint used to create a search
- `extractors` (Attributes List) (see [below for nested schema](#nestedatt--collector--conf--extractors))
- `handle_escaped_chars` (Boolean)
- `latest` (String) Latest time boundary for the search
- `max_batch_size` (Number)
- `output_mode` (String) must be one of ["csv", "json"]
- `password` (String)
- `path` (String) Directory where data will be collected
- `query` (String) Query string for selecting data from the database
- `query_validation_enabled` (Boolean)
- `recurse` (Boolean)
- `region` (String) AWS region from which to retrieve data
- `reject_unauthorized` (Boolean)
- `search` (String) Splunk search query
- `search_head` (String) Search head base URL
- `service_account_credentials` (String)
- `storage_account_name` (String)
- `timeout` (Number)
- `token` (String)
- `token_secret` (String)
- `use_round_robin_dns` (Boolean)
- `username` (String)

<a id="nestedatt--collector--conf--extractors"></a>
### Nested Schema for `collector.conf.extractors`




<a id="nestedatt--input"></a>
### Nested Schema for `input`

Optional:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) Default: "collection"; must be "collection"

<a id="nestedatt--input--metadata"></a>
### Nested Schema for `input.metadata`

Optional:

- `name` (String) Not Null
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.). Not Null


<a id="nestedatt--input--preprocess"></a>
### Nested Schema for `input.preprocess`

Optional:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean) Default: true



<a id="nestedatt--schedule"></a>
### Nested Schema for `schedule`

Optional:

- `cron_schedule` (String) A cron schedule on which to run this job. Default: "*/5 * * * *"
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time. Default: 1
- `run` (Attributes) (see [below for nested schema](#nestedatt--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true

<a id="nestedatt--schedule--run"></a>
### Nested Schema for `schedule.run`

Optional:

- `earliest` (Number) Earliest time to collect data for the selected timezone
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected. Default: "true"
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"
- `latest` (Number) Latest time to collect data for the selected timezone
- `log_level` (String) Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled. Default: 1
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors. Default: true
- `time_range_type` (String) Default: "relative"

## Import

Import is supported using the following syntax:

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = criblio_collector.my_criblio_collector
  id = jsonencode({
    group_id = "default"
    id = "..."
  })
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
terraform import criblio_collector.my_criblio_collector '{"group_id": "default", "id": "..."}'
```
