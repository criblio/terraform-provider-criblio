---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "criblio_destination Resource - terraform-provider-criblio"
subcategory: ""
description: |-
  Destination Resource
---

# criblio_destination (Resource)

Destination Resource

## Example Usage

```terraform
resource "criblio_destination" "my_destination" {
  group_id = "Cribl"
  id       = "out-s3-main"
  output_azure_blob = {
    add_id_to_stage_path = true
    auth_type            = "manual"
    automatic_schema     = true
    azure_cloud          = "AzurePublicCloud"
    base_file_name       = "`CriblOut`"
    certificate = {
      certificate_name = "azure-app-cert"
    }
    client_id               = "11111111-1111-1111-1111-111111111111"
    client_text_secret      = "azure-sp-client-secret"
    compress                = "gzip"
    compression_level       = "normal"
    connection_string       = "DefaultEndpointsProtocol=https;AccountName=criblstore;AccountKey=***REDACTED***;EndpointSuffix=core.windows.net"
    container_name          = "cribl-data"
    create_container        = true
    deadletter_enabled      = true
    deadletter_path         = "/var/lib/cribl/state/outputs/dead-letter"
    description             = "Write objects to Azure Blob Storage with date-based partitioning"
    dest_path               = "logs/ingest"
    empty_dir_cleanup_sec   = 600
    enable_page_checksum    = true
    enable_statistics       = true
    enable_write_page_index = true
    endpoint_suffix         = "core.windows.net"
    environment             = "main"
    file_name_suffix        = ".json.gz"
    format                  = "json"
    header_line             = "timestamp,host,message"
    id                      = "azure-blob-out"
    key_value_metadata = [
      {
        key   = "team"
        value = "platform"
      }
    ]
    max_concurrent_file_parts = 4
    max_file_idle_time_sec    = 30
    max_file_open_time_sec    = 300
    max_file_size_mb          = 64
    max_open_files            = 200
    max_retry_num             = 20
    on_backpressure           = "block"
    on_disk_full_backpressure = "block"
    parquet_data_page_version = "DATA_PAGE_V2"
    parquet_page_size         = "4MB"
    parquet_row_group_length  = 10000
    parquet_version           = "PARQUET_2_6"
    partition_expr            = "2024/01/15"
    pipeline                  = "default"
    remove_empty_dirs         = true
    should_log_invalid_rows   = true
    stage_path                = "/var/lib/cribl/state/outputs/staging"
    storage_account_name      = "criblstore"
    storage_class             = "Hot"
    streamtags = [
      "azure",
      "blob",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    tenant_id             = "00000000-0000-0000-0000-000000000000"
    text_secret           = "azure-connstr-secret"
    type                  = "azure_blob"
    write_high_water_mark = 256
  }
  output_azure_data_explorer = {
    add_id_to_stage_path = true
    additional_properties = [
      {
        key   = "format"
        value = "json"
      }
    ]
    certificate = {
      certificate_name = "adx-app-cert"
    }
    client_id          = "11111111-1111-1111-1111-111111111111"
    client_secret      = "***REDACTED***"
    cluster_url        = "https://mycluster.eastus.kusto.windows.net"
    compress           = "gzip"
    concurrency        = 8
    database           = "telemetry"
    deadletter_enabled = true
    description        = "Ingest data into Azure Data Explorer (Kusto)"
    environment        = "main"
    extent_tags = [
      {
        prefix = "ingestBy"
        value  = "source:app1"
      }
    ]
    file_name_suffix  = ".json.gz"
    flush_immediately = true
    flush_period_sec  = 1
    format            = "json"
    id                = "adx-out"
    ingest_if_not_exists = [
      {
        value = "batchId:2025-10-02T00:00Z"
      }
    ]
    ingest_mode               = "batching"
    ingest_url                = "https://ingest-mycluster.eastus.kusto.windows.net"
    is_mapping_obj            = false
    keep_alive                = true
    mapping_ref               = "my_table_mapping"
    max_concurrent_file_parts = 4
    max_file_idle_time_sec    = 30
    max_file_open_time_sec    = 300
    max_file_size_mb          = 64
    max_open_files            = 200
    max_payload_events        = 0
    max_payload_size_kb       = 4096
    oauth_endpoint            = "https://login.microsoftonline.com"
    oauth_type                = "clientSecret"
    on_backpressure           = "block"
    on_disk_full_backpressure = "block"
    pipeline                  = "default"
    pq_compress               = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    remove_empty_dirs                 = true
    report_level                      = "failuresOnly"
    report_method                     = "queue"
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    retain_blob_on_success = true
    scope                  = "https://kusto.kusto.windows.net/.default"
    stage_path             = "/var/lib/cribl/state/outputs/staging"
    streamtags = [
      "azure",
      "adx",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    table       = "logs_raw"
    tenant_id   = "00000000-0000-0000-0000-000000000000"
    text_secret = "adx-client-secret"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec                = 30
    type                       = "azure_data_explorer"
    use_round_robin_dns        = true
    validate_database_settings = true
  }
  output_azure_eventhub = {
    ack                    = 1
    authentication_timeout = 10000
    backoff_rate           = 2
    brokers = [
      "myns.servicebus.windows.net:9093",
    ]
    connection_timeout = 10000
    description        = "Deliver events to Azure Event Hubs via Kafka protocol"
    environment        = "main"
    flush_event_count  = 1000
    flush_period_sec   = 1
    format             = "json"
    id                 = "eventhub-out"
    initial_backoff    = 1000
    max_back_off       = 60000
    max_record_size_kb = 768
    max_retries        = 5
    on_backpressure    = "block"
    pipeline           = "default"
    pq_compress        = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size           = "100 MB"
    pq_max_size                = "10GB"
    pq_mode                    = "backpressure"
    pq_on_backpressure         = "block"
    pq_path                    = "/opt/cribl/state/queues"
    reauthentication_threshold = 60000
    request_timeout            = 60000
    sasl = {
      disabled  = false
      mechanism = "plain"
    }
    streamtags = [
      "azure",
      "eventhub",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    tls = {
      disabled            = false
      reject_unauthorized = true
    }
    topic = "app-events"
    type  = "azure_eventhub"
  }
  output_azure_logs = {
    api_url     = ".ods.opinsights.azure.com"
    auth_type   = "manual"
    compress    = true
    concurrency = 8
    description = "Send logs to Azure Log Analytics workspace"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    id                          = "azure-logs-out"
    keypair_secret              = "azure-log-analytics-keys"
    log_type                    = "Cribl"
    max_payload_events          = 0
    max_payload_size_kb         = 1024
    on_backpressure             = "block"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    resource_id                       = "/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg1/providers/Microsoft.Compute/virtualMachines/vm1"
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "azure",
      "loganalytics",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    type                = "azure_logs"
    use_round_robin_dns = true
    workspace_id        = "22222222-2222-2222-2222-222222222222"
    workspace_key       = "***REDACTED***"
  }
  output_click_house = {
    async_inserts    = true
    auth_header_expr = "`Bearer ${token}`"
    auth_type        = "basic"
    column_mappings = [
      {
        column_name             = "timestamp"
        column_type             = "DateTime64(3)"
        column_value_expression = "toDateTime64(ts, 3)"
      }
    ]
    compress                   = true
    concurrency                = 8
    credentials_secret         = "clickhouse_basic_auth"
    database                   = "logs"
    describe_table             = "DESCRIBE TABLE app_events"
    description                = "Ingest logs to ClickHouse with async inserts and TLS"
    dump_format_errors_to_disk = true
    environment                = "main"
    exclude_mapping_fields = [
      "_raw",
      "ts",
    ]
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    format                      = "json-each-row"
    id                          = "clickhouse_ingest_prod"
    login_url                   = "https://auth.example.com/oauth/token"
    mapping_type                = "automatic"
    max_payload_events          = 1000
    max_payload_size_kb         = 2048
    oauth_headers = [
      {
        name  = "Accept"
        value = "application/json"
      }
    ]
    oauth_params = [
      {
        name  = "grant_type"
        value = "client_credentials"
      }
    ]
    on_backpressure = "block"
    password        = "s3cr3tPass!"
    pipeline        = "main"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    secret            = "s3cr3tClientSecret"
    secret_param_name = "client_secret"
    sql_username      = "clickuser"
    streamtags = [
      "prod",
      "clickhouse",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    table_name  = "app_events"
    text_secret = "clickhouse_bearer_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    tls = {
      ca_path          = "/etc/ssl/certs/ca-bundle.crt"
      cert_path        = "/opt/cribl/certs/client.crt"
      certificate_name = "clickhouse-client"
      disabled         = false
      max_version      = "TLSv1.3"
      min_version      = "TLSv1.2"
      passphrase       = "s3cr3t"
      priv_key_path    = "/opt/cribl/certs/client.key"
      servername       = "clickhouse.example.com"
    }
    token                  = "chBearerToken_abc123xyz"
    token_attribute_name   = "access_token"
    token_timeout_secs     = 3600
    type                   = "click_house"
    url                    = "https://clickhouse.example.com:8443"
    use_round_robin_dns    = true
    username               = "clickuser"
    wait_for_async_inserts = true
  }
  output_cloudwatch = {
    assume_role_arn           = "arn:aws:iam::123456789012:role/CloudWatchLogsWriter"
    assume_role_external_id   = "external-id-abc123"
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws_cloudwatch_credentials"
    aws_secret_key            = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    description               = "Send application logs to Amazon CloudWatch Logs"
    duration_seconds          = 3600
    enable_assume_role        = true
    endpoint                  = "https://logs.us-east-1.amazonaws.com"
    environment               = "main"
    flush_period_sec          = 2
    id                        = "cloudwatch_logs_prod"
    log_group_name            = "/aws/eks/cluster-1/app-logs"
    log_stream_name           = "app-logs"
    max_queue_size            = 10
    max_record_size_kb        = 1024
    on_backpressure           = "block"
    pipeline                  = "main"
    pq_compress               = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size    = "100 MB"
    pq_max_size         = "10GB"
    pq_mode             = "backpressure"
    pq_on_backpressure  = "block"
    pq_path             = "/opt/cribl/state/queues"
    region              = "us-east-1"
    reject_unauthorized = true
    reuse_connections   = true
    streamtags = [
      "prod",
      "cloudwatch",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type = "cloudwatch"
  }
  output_confluent_cloud = {
    ack                    = 1
    authentication_timeout = 10000
    backoff_rate           = 2
    brokers = [
      "mycluster.us-central1.gcp.confluent.cloud:9092",
    ]
    compression        = "gzip"
    connection_timeout = 10000
    description        = "Produce events to Confluent Cloud Kafka with Schema Registry"
    environment        = "main"
    flush_event_count  = 1000
    flush_period_sec   = 1
    format             = "json"
    id                 = "ccloud-out"
    initial_backoff    = 1000
    kafka_schema_registry = {
      auth = {
        credentials_secret = "ccloud-schema-registry-basic"
        disabled           = false
      }
      connection_timeout      = 30000
      default_key_schema_id   = 1
      default_value_schema_id = 100
      disabled                = false
      max_retries             = 3
      request_timeout         = 30000
      schema_registry_url     = "https://schema-registry.confluent.cloud"
      tls = {
        ca_path             = "/etc/ssl/certs/ca.pem"
        cert_path           = "/etc/ssl/certs/client.crt"
        certificate_name    = "ccloud-sr-cert"
        disabled            = false
        max_version         = "TLSv1.3"
        min_version         = "TLSv1.2"
        passphrase          = "***REDACTED***"
        priv_key_path       = "/etc/ssl/private/client.key"
        reject_unauthorized = true
        servername          = "schema-registry.confluent.cloud"
      }
    }
    max_back_off       = 60000
    max_record_size_kb = 768
    max_retries        = 5
    on_backpressure    = "block"
    pipeline           = "default"
    pq_compress        = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size           = "100 MB"
    pq_max_size                = "10GB"
    pq_mode                    = "backpressure"
    pq_on_backpressure         = "block"
    pq_path                    = "/opt/cribl/state/queues"
    protobuf_library_id        = "user-events-protos"
    reauthentication_threshold = 60000
    request_timeout            = 60000
    sasl = {
      disabled  = false
      mechanism = "plain"
    }
    streamtags = [
      "confluent",
      "kafka",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    tls = {
      ca_path             = "/etc/ssl/certs/ca.pem"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "ccloud-client-cert"
      disabled            = false
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "mycluster.us-central1.gcp.confluent.cloud"
    }
    topic = "app-events"
    type  = "confluent_cloud"
  }
  output_cribl_http = {
    compression            = "gzip"
    concurrency            = 8
    description            = "Send events to Cribl Worker HTTP endpoint with retries"
    dns_resolve_period_sec = 300
    environment            = "main"
    exclude_fields = [
      "__kube_*",
      "__metadata",
    ]
    exclude_self = false
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode   = "payloadAndHeaders"
    flush_period_sec              = 2
    id                            = "cribl_http_prod"
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    max_payload_events            = 1000
    max_payload_size_kb           = 2048
    on_backpressure               = "block"
    pipeline                      = "main"
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "cribl",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    tls = {
      ca_path             = "/etc/ssl/certs/ca-bundle.crt"
      cert_path           = "/opt/cribl/certs/client.crt"
      certificate_name    = "cribl-client"
      disabled            = true
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "s3cr3t"
      priv_key_path       = "/opt/cribl/certs/client.key"
      reject_unauthorized = true
      servername          = "collector.cribl.example.com"
    }
    token_ttl_minutes = 60
    type              = "cribl_http"
    url               = "https://edge.example.com:10200"
    urls = [
      {
        url    = "https://edge01.example.com:10200"
        weight = 2
      }
    ]
    use_round_robin_dns = true
  }
  output_cribl_lake = {
    description = "Cribl Lake destination"
    dest_path   = "security_logs"
    id          = "lake_ingest_prod"
    type        = "cribl_lake"
  }
  output_cribl_tcp = {
    compression            = "gzip"
    connection_timeout     = 10000
    description            = "Send events to Cribl Edge over TCP with TLS"
    dns_resolve_period_sec = 300
    environment            = "main"
    exclude_fields = [
      "__kube_*",
      "__metadata",
    ]
    exclude_self = false
    host         = "edge01.example.com"
    hosts = [
      {
        host       = "edge02.example.com"
        port       = 10300
        servername = "edge02.example.com"
        tls        = "inherit"
        weight     = 2
      }
    ]
    id                            = "cribl_tcp_prod"
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    log_failed_requests           = true
    max_concurrent_senders        = 4
    on_backpressure               = "block"
    pipeline                      = "main"
    port                          = 10300
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    streamtags = [
      "prod",
      "cribl",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    throttle_rate_per_sec = "10 MB"
    tls = {
      ca_path             = "/etc/ssl/certs/ca-bundle.crt"
      cert_path           = "/opt/cribl/certs/client.crt"
      certificate_name    = "cribl-client"
      disabled            = true
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "s3cr3t"
      priv_key_path       = "/opt/cribl/certs/client.key"
      reject_unauthorized = true
      servername          = "tcp.cribl.example.com"
    }
    token_ttl_minutes = 60
    type              = "cribl_tcp"
    write_timeout     = 30000
  }
  output_crowdstrike_next_gen_siem = {
    auth_type   = "secret"
    compress    = true
    concurrency = 8
    description = "Send events to CrowdStrike Next-Gen SIEM with token auth"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    format                      = "raw"
    id                          = "cs_nextgen_siem_prod"
    max_payload_events          = 1000
    max_payload_size_kb         = 8192
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "crowdstrike",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "crowdstrike_nextgen_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    token               = "csngs-0123456789abcdef0123456789abcdef"
    type                = "crowdstrike_next_gen_siem"
    url                 = "https://ingest.us-1.crowdstrike.com/api/ingest/hec/abcd1234/v1/services/collector"
    use_round_robin_dns = true
  }
  output_datadog = {
    allow_api_key_from_events = false
    api_key                   = "0123456789abcdef0123456789abcdef"
    auth_type                 = "secret"
    batch_by_tags             = true
    compress                  = true
    concurrency               = 8
    content_type              = "json"
    custom_url                = "https://http-intake.logs.datadoghq.com/api/v2/logs"
    description               = "Send logs to Datadog Logs API with token auth"
    environment               = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    host                        = "web-01.example.com"
    id                          = "datadog_logs_prod"
    max_payload_events          = 1000
    max_payload_size_kb         = 2048
    message                     = "_raw"
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    send_counters_as_count = true
    service                = "web-app"
    severity               = "info"
    site                   = "us"
    source                 = "nginx"
    streamtags = [
      "prod",
      "datadog",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    tags = [
      "env:prod",
      "team:platform",
    ]
    text_secret = "datadog_api_key"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    total_memory_limit_kb = 51200
    type                  = "datadog"
    use_round_robin_dns   = true
  }
  output_dataset = {
    api_key          = "ds-0123456789abcdef0123456789abcdef"
    auth_type        = "secret"
    compress         = true
    concurrency      = 8
    custom_url       = "https://api.dataset.com/v1/logs"
    default_severity = "info"
    description      = "Send events to DataSet with API key authentication"
    environment      = "main"
    exclude_fields = [
      "sev",
      "_time",
    ]
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    id                          = "dataset_logs_prod"
    max_payload_events          = 1000
    max_payload_size_kb         = 2048
    message_field               = "_raw"
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    server_host_field = "host"
    site              = "us"
    streamtags = [
      "prod",
      "dataset",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "dataset_api_key"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    timestamp_field       = "ts"
    total_memory_limit_kb = 51200
    type                  = "dataset"
    use_round_robin_dns   = true
  }
  output_default = {
    default_id  = "http-default"
    environment = "main"
    id          = "default-output"
    pipeline    = "default"
    streamtags = [
      "default",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    type = "default"
  }
  output_devnull = {
    environment = "main"
    id          = "devnull-out"
    pipeline    = "default"
    streamtags = [
      "discard",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    type = "devnull"
  }
  output_disk_spool = {
    compress       = "gzip"
    description    = "Local disk spool for short-term buffering and replay"
    environment    = "main"
    id             = "disk_spool_buffer"
    max_data_size  = "100GB"
    max_data_time  = "7d"
    partition_expr = ""
    pipeline       = "main"
    streamtags = [
      "prod",
      "spool",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    time_window = "10m"
    type        = "disk_spool"
  }
  output_dl_s3 = {
    add_id_to_stage_path      = true
    assume_role_arn           = "arn:aws:iam::123456789012:role/S3Writer"
    assume_role_external_id   = "external-id-abc123"
    automatic_schema          = true
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws_s3_credentials"
    aws_secret_key            = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    base_file_name            = "app-logs"
    bucket                    = "logs-archive-prod"
    compress                  = "gzip"
    compression_level         = "normal"
    deadletter_enabled        = true
    deadletter_path           = "/opt/cribl/state/outputs/dead-letter"
    description               = "Archive logs to S3 in Parquet with field-based partitions"
    dest_path                 = "year=%Y/month=%m/day=%d/app=orders"
    duration_seconds          = 3600
    empty_dir_cleanup_sec     = 600
    enable_assume_role        = true
    enable_page_checksum      = true
    enable_statistics         = true
    enable_write_page_index   = true
    endpoint                  = "https://s3.us-east-1.amazonaws.com"
    environment               = "main"
    file_name_suffix          = ".parquet.gz"
    format                    = "parquet"
    header_line               = "timestamp,host,level,message"
    id                        = "dls3_archive_prod"
    key_value_metadata = [
      {
        key   = "OCSF Event Class"
        value = "9001"
      }
    ]
    kms_key_id                        = "arn:aws:kms:us-east-1:123456789012:key/abcd-1234-efgh-5678"
    max_closing_files_to_backpressure = 500
    max_concurrent_file_parts         = 5
    max_file_idle_time_sec            = 120
    max_file_open_time_sec            = 600
    max_file_size_mb                  = 256
    max_open_files                    = 200
    max_retry_num                     = 20
    object_acl                        = "private"
    on_disk_full_backpressure         = "block"
    parquet_data_page_version         = "DATA_PAGE_V2"
    parquet_page_size                 = "128MB"
    parquet_row_group_length          = 100000
    parquet_version                   = "PARQUET_2_6"
    partitioning_fields = [
      "app",
      "env",
    ]
    pipeline                = "main"
    region                  = "us-east-1"
    reject_unauthorized     = true
    remove_empty_dirs       = true
    reuse_connections       = true
    server_side_encryption  = "aws:kms"
    should_log_invalid_rows = true
    signature_version       = "v4"
    stage_path              = "/opt/cribl/state/outputs/staging"
    storage_class           = "INTELLIGENT_TIERING"
    streamtags = [
      "prod",
      "archive",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type                  = "dl_s3"
    verify_permissions    = true
    write_high_water_mark = 256
  }
  output_dynatrace_http = {
    active_gate_domain = "https://activegate.example.com:9999/e/abc12345/api/v2/logs/ingest"
    auth_type          = "token"
    compress           = true
    concurrency        = 8
    description        = "Send logs to Dynatrace Logs Ingest API"
    endpoint           = "cloud"
    environment        = "main"
    environment_id     = "abc12345"
    extra_http_headers = [
      {
        name  = "Api-Token"
        value = "dt0c01.XXXX.YYYYZZZZ"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    format                      = "json_array"
    id                          = "dynatrace_http_metrics"
    keep_alive                  = true
    max_payload_events          = 10000
    max_payload_size_kb         = 4096
    method                      = "POST"
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "api-token",
    ]
    streamtags = [
      "prod",
      "dynatrace",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    telemetry_type = "logs"
    text_secret    = "dynatrace_api_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    token                 = "dt0c01.XXXX.YYYYZZZZ"
    total_memory_limit_kb = 51200
    type                  = "dynatrace_http"
    url                   = "https://abc.live.dynatrace.com/e/abc12345/api/v2/logs/ingest"
    use_round_robin_dns   = true
  }
  output_dynatrace_otlp = {
    auth_token_name    = "Authorization"
    compress           = "gzip"
    concurrency        = 5
    connection_timeout = 10000
    description        = "Send OTLP logs and metrics to Dynatrace SaaS"
    endpoint           = "https://abc123.live.dynatrace.com/api/v2/otlp"
    endpoint_type      = "saas"
    environment        = "main"
    extra_http_headers = [
      {
        name  = "Api-Token"
        value = "dt0c01.XXXX.YYYYZZZZ"
      }
    ]
    failed_request_logging_mode    = "payloadAndHeaders"
    flush_period_sec               = 2
    http_compress                  = "gzip"
    http_logs_endpoint_override    = "https://abc123.live.dynatrace.com/api/v2/otlp/v1/logs"
    http_metrics_endpoint_override = "https://abc123.live.dynatrace.com/api/v2/otlp/v1/metrics"
    http_traces_endpoint_override  = "https://abc123.live.dynatrace.com/api/v2/otlp/v1/traces"
    id                             = "dynatrace_otlp_export"
    keep_alive                     = true
    keep_alive_time                = 30
    max_payload_size_kb            = 2048
    metadata = [
      {
        key   = "x-tenant-id"
        value = "acme-prod"
      }
    ]
    on_backpressure = "block"
    otlp_version    = "1.3.1"
    pipeline        = "main"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    protocol                          = "http"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "api-token",
    ]
    streamtags = [
      "prod",
      "dynatrace",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    token_secret        = "dynatrace_otlp_token"
    type                = "dynatrace_otlp"
    use_round_robin_dns = true
  }
  output_elastic = {
    auth = {
      auth_type = "manualAPIKey"
      disabled  = false
    }
    compress               = true
    concurrency            = 8
    description            = "Send documents to Elasticsearch bulk API with retries and custom params"
    dns_resolve_period_sec = 300
    doc_type               = "_doc"
    elastic_pipeline       = "ingest-grok-pipeline"
    elastic_version        = "7"
    environment            = "main"
    exclude_self           = false
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    extra_params = [
      {
        name  = "filter_path"
        value = "errors,items.*.error,items.*._index,items.*.status"
      }
    ]
    failed_request_logging_mode   = "payload"
    flush_period_sec              = 1
    id                            = "elastic-out"
    include_doc_id                = true
    index                         = "\"logs-2024.01.15\""
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    max_payload_events            = 0
    max_payload_size_kb           = 4096
    on_backpressure               = "block"
    pipeline                      = "default"
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    retry_partial_errors = true
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "elastic",
      "es",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    type        = "elastic"
    url         = "https://es.example.com:9200/_bulk"
    urls = [
      {
        url    = "https://es-node-1.example.com:9200/_bulk"
        weight = 2
      }
    ]
    use_round_robin_dns = true
    write_action        = "create"
  }
  output_elastic_cloud = {
    auth = {
      auth_type = "manualAPIKey"
      disabled  = false
    }
    compress         = true
    concurrency      = 8
    description      = "Send documents to Elastic Cloud with retries and pipeline support"
    elastic_pipeline = "ingest-grok-pipeline"
    environment      = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    extra_params = [
      {
        name  = "filter_path"
        value = "errors,items.*.error,items.*._index,items.*.status"
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    id                          = "es-cloud-out"
    include_doc_id              = true
    index                       = "\"logs-2024.01.15\""
    max_payload_events          = 0
    max_payload_size_kb         = 4096
    on_backpressure             = "block"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "elastic",
      "cloud",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    type        = "elastic_cloud"
    url         = "my-deployment:ZXM0LmNsb3VkLmV... (truncated)"
  }
  output_exabeam = {
    add_id_to_stage_path      = true
    aws_api_key               = "***REDACTED***"
    aws_secret_key            = "***REDACTED***"
    bucket                    = "exabeam-data"
    collector_instance_id     = "11112222-3333-4444-5555-666677778888"
    deadletter_enabled        = true
    deadletter_path           = "/var/lib/cribl/state/outputs/dead-letter"
    description               = "Deliver logs to Exabeam Collector via GCS staging"
    empty_dir_cleanup_sec     = 600
    encoded_configuration     = "***REDACTED***"
    endpoint                  = "https://storage.googleapis.com"
    environment               = "main"
    id                        = "exabeam-out"
    max_file_idle_time_sec    = 30
    max_file_open_time_sec    = 300
    max_file_size_mb          = 64
    max_open_files            = 200
    max_retry_num             = 20
    object_acl                = "private"
    on_backpressure           = "block"
    on_disk_full_backpressure = "block"
    pipeline                  = "default"
    region                    = "us-central1"
    reject_unauthorized       = true
    remove_empty_dirs         = true
    reuse_connections         = true
    signature_version         = "v4"
    site_id                   = "site-123"
    site_name                 = "\"corp-east\""
    stage_path                = "/var/lib/cribl/state/outputs/staging"
    storage_class             = "NEARLINE"
    streamtags = [
      "exabeam",
      "gcs",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    timezone_offset = "-07:00"
    type            = "exabeam"
  }
  output_filesystem = {
    add_id_to_stage_path    = true
    automatic_schema        = true
    base_file_name          = "`CriblOut`"
    compress                = "gzip"
    compression_level       = "normal"
    deadletter_enabled      = true
    deadletter_path         = "/var/lib/cribl/state/outputs/dead-letter"
    description             = "Write events to local filesystem with daily partitioning"
    dest_path               = "/var/log/cribl/out"
    empty_dir_cleanup_sec   = 600
    enable_page_checksum    = true
    enable_statistics       = true
    enable_write_page_index = true
    environment             = "main"
    file_name_suffix        = ".json.gz"
    format                  = "json"
    header_line             = "timestamp,host,message"
    id                      = "filesystem-out"
    key_value_metadata = [
      {
        key   = "team"
        value = "platform"
      }
    ]
    max_file_idle_time_sec    = 30
    max_file_open_time_sec    = 300
    max_file_size_mb          = 64
    max_open_files            = 200
    max_retry_num             = 20
    on_backpressure           = "block"
    on_disk_full_backpressure = "block"
    parquet_data_page_version = "DATA_PAGE_V2"
    parquet_page_size         = "4MB"
    parquet_row_group_length  = 10000
    parquet_version           = "PARQUET_2_6"
    partition_expr            = "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d') + '/host=' + host"
    pipeline                  = "default"
    remove_empty_dirs         = true
    should_log_invalid_rows   = true
    stage_path                = "/var/log/cribl/stage"
    streamtags = [
      "filesystem",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    type                  = "filesystem"
    write_high_water_mark = 256
  }
  output_google_chronicle = {
    api_key               = "***REDACTED***"
    api_key_secret        = "chronicle-api-key"
    api_version           = "v1"
    authentication_method = "serviceAccount"
    compress              = true
    concurrency           = 8
    custom_labels = [
      {
        key   = "...my_key..."
        value = "...my_value..."
      }
    ]
    customer_id = "123e4567-e89b-12d3-a456-426614174000"
    description = "Send events to Google SecOps Chronicle"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    extra_log_types = [
      {
        description = "...my_description..."
        log_type    = "...my_log_type..."
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    id                          = "chronicle-out"
    log_format_type             = "unstructured"
    log_text_field              = "message"
    log_type                    = "CUSTOM_WEBLOG"
    max_payload_events          = 0
    max_payload_size_kb         = 1024
    namespace                   = "prod-us"
    on_backpressure             = "block"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    region                            = "us"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 30000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    service_account_credentials        = "***REDACTED***"
    service_account_credentials_secret = "chronicle-sa-credentials"
    streamtags = [
      "google",
      "chronicle",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 90
    total_memory_limit_kb = 5120
    type                  = "google_chronicle"
    use_round_robin_dns   = true
  }
  output_google_cloud_logging = {
    cache_fill_bytes_expression = "String(_raw.cache_fill_bytes)"
    cache_hit_expression        = "Boolean(_raw.cache_hit)"
    cache_lookup_expression     = "Boolean(_raw.cache_lookup)"
    cache_validated_expression  = "Boolean(_raw.cache_validated)"
    concurrency                 = 8
    connection_timeout          = 10000
    description                 = "Send logs to Google Cloud Logging with custom resource labels"
    environment                 = "main"
    file_expression             = "String(_raw.file)"
    first_expression            = "Boolean(_raw.operation_first)"
    flush_period_sec            = 1
    function_expression         = "String(_raw.function)"
    google_auth_method          = "secret"
    id                          = "gcl-out"
    id_expression               = "String(_raw.operation_id)"
    index_expression            = "Number(_raw.split_index)"
    insert_id_expression        = "Crypto.uuid()"
    last_expression             = "Boolean(_raw.operation_last)"
    latency_expression          = "(_raw.latency_ms/1000).toFixed(3) + \"s\""
    line_expression             = "String(_raw.line)"
    log_labels = [
      {
        label            = "environment"
        value_expression = "\"prod\""
      }
    ]
    log_location_expression = "\"projects/my-project\""
    log_location_type       = "project"
    log_name_expression     = "\"cribl_logs\""
    max_payload_events      = 0
    max_payload_size_kb     = 4096
    on_backpressure         = "block"
    payload_expression      = "{ message: _raw.message, severity: _raw.severity || \"DEFAULT\" }"
    payload_format          = "json"
    pipeline                = "default"
    pq_compress             = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size          = "100 MB"
    pq_max_size               = "10GB"
    pq_mode                   = "backpressure"
    pq_on_backpressure        = "block"
    pq_path                   = "/opt/cribl/state/queues"
    producer_expression       = "String(_raw.operation_producer)"
    protocol_expression       = "String(_raw.protocol)"
    referer_expression        = "String(_raw.referer)"
    remote_ip_expression      = "String(_raw.client_ip)"
    request_method_expression = "\"POST\""
    request_size_expression   = "String(length(_raw.request_body))"
    request_url_expression    = "\"https://example.com/api\""
    resource_type_expression  = "\"gce_instance\""
    resource_type_labels = [
      {
        label            = "instance_id"
        value_expression = "String(_raw.instance_id)"
      }
    ]
    response_size_expression    = "String(length(_raw.response_body))"
    secret                      = "gcl-service-account"
    server_ip_expression        = "String(_raw.server_ip)"
    service_account_credentials = "***REDACTED***"
    severity_expression         = "\"INFO\""
    span_id_expression          = "String(_raw.span_id)"
    status_expression           = "Number(_raw.status)"
    streamtags = [
      "gcp",
      "logging",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    throttle_rate_req_per_sec = 500
    timeout_sec               = 30
    total_memory_limit_kb     = 20480
    total_splits_expression   = "Number(_raw.split_total)"
    trace_expression          = "String(_raw.trace)"
    trace_sampled_expression  = "Boolean(_raw.trace_sampled)"
    type                      = "google_cloud_logging"
    uid_expression            = "String(_raw.split_uid)"
    user_agent_expression     = "String(_raw.user_agent)"
  }
  output_google_cloud_storage = {
    add_id_to_stage_path      = true
    automatic_schema          = true
    aws_api_key               = "***REDACTED***"
    aws_authentication_method = "manual"
    aws_secret                = "gcs-hmac-credentials"
    aws_secret_key            = "***REDACTED***"
    base_file_name            = "`CriblOut`"
    bucket                    = "cribl-data-bucket"
    compress                  = "gzip"
    compression_level         = "normal"
    deadletter_enabled        = true
    deadletter_path           = "/var/lib/cribl/state/outputs/dead-letter"
    description               = "Write objects to Google Cloud Storage with date-based partitioning"
    dest_path                 = "logs/ingest"
    empty_dir_cleanup_sec     = 600
    enable_page_checksum      = true
    enable_statistics         = true
    enable_write_page_index   = true
    endpoint                  = "https://storage.googleapis.com"
    environment               = "main"
    file_name_suffix          = ".json.gz"
    format                    = "json"
    header_line               = "timestamp,host,message"
    id                        = "gcs-out"
    key_value_metadata = [
      {
        key   = "team"
        value = "platform"
      }
    ]
    max_file_idle_time_sec    = 30
    max_file_open_time_sec    = 300
    max_file_size_mb          = 64
    max_open_files            = 200
    max_retry_num             = 20
    object_acl                = "private"
    on_backpressure           = "block"
    on_disk_full_backpressure = "block"
    parquet_data_page_version = "DATA_PAGE_V2"
    parquet_page_size         = "4MB"
    parquet_row_group_length  = 10000
    parquet_version           = "PARQUET_2_6"
    partition_expr            = "2024/01/15"
    pipeline                  = "default"
    region                    = "us-central1"
    reject_unauthorized       = true
    remove_empty_dirs         = true
    reuse_connections         = true
    should_log_invalid_rows   = true
    signature_version         = "v4"
    stage_path                = "/var/lib/cribl/state/outputs/staging"
    storage_class             = "NEARLINE"
    streamtags = [
      "gcp",
      "gcs",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    type                  = "google_cloud_storage"
    verify_permissions    = true
    write_high_water_mark = 256
  }
  output_google_pubsub = {
    batch_size         = 1000
    batch_timeout      = 200
    create_topic       = true
    description        = "Publish events to Google Pub/Sub with ordered delivery"
    environment        = "main"
    flush_period_sec   = 1
    google_auth_method = "secret"
    id                 = "gpubsub-out"
    max_in_progress    = 20
    max_queue_size     = 500
    max_record_size_kb = 256
    on_backpressure    = "block"
    ordered_delivery   = true
    pipeline           = "default"
    pq_compress        = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size            = "100 MB"
    pq_max_size                 = "10GB"
    pq_mode                     = "backpressure"
    pq_on_backpressure          = "block"
    pq_path                     = "/opt/cribl/state/queues"
    region                      = "us-central1"
    secret                      = "gcp-pubsub-sa"
    service_account_credentials = "***REDACTED***"
    streamtags = [
      "gcp",
      "pubsub",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    topic_name = "app-events"
    type       = "google_pubsub"
  }
  output_grafana_cloud = {
    output_grafana_cloud_grafana_cloud2 = {
      compress    = true
      concurrency = 2
      description = "Send logs and metrics to Grafana Cloud Loki and Prometheus"
      environment = "main"
      extra_http_headers = [
        {
          name  = "X-Request-ID"
          value = "123e4567-e89b-12d3-a456-426614174000"
        }
      ]
      failed_request_logging_mode = "payloadAndHeaders"
      flush_period_sec            = 10
      id                          = "grafana_cloud_logs_prod"
      labels = [
        {
          name  = "host"
          value = "web-01"
        }
      ]
      loki_auth = {
        auth_type          = "basic"
        credentials_secret = "grafana_loki_credentials"
        password           = "glc_efgh5678"
        text_secret        = "grafana_loki_token"
        token              = "12345:glc_efgh5678"
        username           = 12345
      }
      loki_url            = "https://logs-prod-us-central1.grafana.net"
      max_payload_events  = 1000
      max_payload_size_kb = 2048
      message             = "_raw"
      message_format      = "protobuf"
      metric_rename_expr  = "name.replace(/[^a-zA-Z0-9_]/g, '_')"
      on_backpressure     = "block"
      pipeline            = "main"
      pq_compress         = "gzip"
      pq_controls = {
        # ...
      }
      pq_max_file_size   = "100 MB"
      pq_max_size        = "10GB"
      pq_mode            = "backpressure"
      pq_on_backpressure = "block"
      pq_path            = "/opt/cribl/state/queues"
      prometheus_auth = {
        auth_type          = "basic"
        credentials_secret = "grafana_prom_credentials"
        password           = "glc_abcd1234"
        text_secret        = "grafana_prom_token"
        token              = "12345:glc_abcd1234"
        username           = 12345
      }
      prometheus_url                    = "https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push"
      reject_unauthorized               = true
      response_honor_retry_after_header = true
      response_retry_settings = [
        {
          backoff_rate    = 2
          http_status     = 429
          initial_backoff = 1000
          max_backoff     = 30000
        }
      ]
      safe_headers = [
        "content-type",
        "x-request-id",
      ]
      streamtags = [
        "prod",
        "grafana",
      ]
      system_fields = [
        "cribl_host",
        "cribl_wp",
      ]
      timeout_retry_settings = {
        backoff_rate    = 2
        initial_backoff = 1000
        max_backoff     = 30000
        timeout_retry   = true
      }
      timeout_sec         = 30
      type                = "grafana_cloud"
      use_round_robin_dns = true
    }
  }
  output_graphite = {
    connection_timeout     = 10000
    description            = "Send metrics to Graphite in plaintext protocol"
    dns_resolve_period_sec = 300
    environment            = "main"
    flush_period_sec       = 1
    host                   = "graphite.example.com"
    id                     = "graphite_metrics_prod"
    mtu                    = 1400
    on_backpressure        = "block"
    pipeline               = "metrics"
    port                   = 2003
    pq_compress            = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    protocol           = "tcp"
    streamtags = [
      "prod",
      "graphite",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    throttle_rate_per_sec = "10 MB"
    type                  = "graphite"
    write_timeout         = 30000
  }
  output_honeycomb = {
    auth_type   = "manual"
    compress    = true
    concurrency = 8
    dataset     = "observability"
    description = "Send events to Honeycomb dataset"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    id                          = "honeycomb-out"
    max_payload_events          = 0
    max_payload_size_kb         = 4096
    on_backpressure             = "block"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "honeycomb",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    team        = "***REDACTED***"
    text_secret = "honeycomb-api-key"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    type                = "honeycomb"
    use_round_robin_dns = true
  }
  output_humio_hec = {
    auth_type   = "secret"
    compress    = true
    concurrency = 8
    description = "Send logs to CrowdStrike Falcon LogScale via HEC endpoint"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    format                      = "JSON"
    id                          = "humio_hec_prod"
    max_payload_events          = 1000
    max_payload_size_kb         = 8192
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "humio",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "humio_hec_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    token               = "humio-0123456789abcdef0123456789abcdef"
    type                = "humio_hec"
    url                 = "https://cloud.us.humio.com/api/v1/ingest/hec"
    use_round_robin_dns = true
  }
  output_influxdb = {
    auth_header_expr         = "`Bearer ${token}`"
    auth_type                = "token"
    bucket                   = "metrics_prod"
    compress                 = true
    concurrency              = 8
    credentials_secret       = "influxdb_basic_auth"
    database                 = "telegraf"
    description              = "Send metrics to InfluxDB with v2 API and token auth"
    dynamic_value_field_name = true
    environment              = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    id                          = "influxdb_metrics_prod"
    login_url                   = "https://influxdb.example.com/oauth/token"
    max_payload_events          = 5000
    max_payload_size_kb         = 8192
    oauth_headers = [
      {
        name  = "Accept"
        value = "application/json"
      }
    ]
    oauth_params = [
      {
        name  = "grant_type"
        value = "client_credentials"
      }
    ]
    on_backpressure = "block"
    org             = "acme-observability"
    password        = "s3cr3tPass!"
    pipeline        = "metrics"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    secret            = "s3cr3tClientSecret"
    secret_param_name = "client_secret"
    streamtags = [
      "prod",
      "influxdb",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "influxdb_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec          = 30
    timestamp_precision  = "ms"
    token                = "influxV2Token_abc123xyz"
    token_attribute_name = "access_token"
    token_timeout_secs   = 3600
    type                 = "influxdb"
    url                  = "https://influxdb.example.com:8086/write"
    use_round_robin_dns  = true
    use_v2_api           = true
    username             = "influx_writer"
    value_field_name     = "value"
  }
  output_kafka = {
    ack                    = 1
    authentication_timeout = 10000
    backoff_rate           = 2
    brokers = [
      "kafka-1.example.com:9092",
      "kafka-2.example.com:9092",
    ]
    compression        = "gzip"
    connection_timeout = 10000
    description        = "Produce events to Kafka with retries and TLS"
    environment        = "main"
    flush_event_count  = 1000
    flush_period_sec   = 1
    format             = "json"
    id                 = "kafka-out"
    initial_backoff    = 1000
    kafka_schema_registry = {
      auth = {
        credentials_secret = "schema-registry-basic-auth"
        disabled           = false
      }
      connection_timeout      = 30000
      default_key_schema_id   = 1
      default_value_schema_id = 2
      disabled                = true
      max_retries             = 3
      request_timeout         = 30000
      schema_registry_url     = "https://schema-registry.example.com:8081"
      tls = {
        ca_path             = "/etc/ssl/certs/ca.pem"
        cert_path           = "/etc/ssl/certs/client.crt"
        certificate_name    = "kafka-client-cert"
        disabled            = false
        max_version         = "TLSv1.3"
        min_version         = "TLSv1.2"
        passphrase          = "***REDACTED***"
        priv_key_path       = "/etc/ssl/private/client.key"
        reject_unauthorized = true
        servername          = "schema-registry.example.com"
      }
    }
    max_back_off       = 60000
    max_record_size_kb = 768
    max_retries        = 5
    on_backpressure    = "block"
    pipeline           = "default"
    pq_compress        = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size           = "100 MB"
    pq_max_size                = "10GB"
    pq_mode                    = "backpressure"
    pq_on_backpressure         = "block"
    pq_path                    = "/opt/cribl/state/queues"
    protobuf_library_id        = "user-events-protos"
    reauthentication_threshold = 60000
    request_timeout            = 60000
    sasl = {
      disabled  = false
      mechanism = "scram-sha-512"
    }
    streamtags = [
      "kafka",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    tls = {
      ca_path             = "/etc/ssl/certs/ca.pem"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "kafka-client-cert"
      disabled            = false
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "kafka-broker.example.com"
    }
    topic = "app-events"
    type  = "kafka"
  }
  output_kinesis = {
    as_ndjson                 = true
    assume_role_arn           = "arn:aws:iam::123456789012:role/cribl-kinesis-writer"
    assume_role_external_id   = "cribl-external-123"
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws-credentials-secret"
    aws_secret_key            = "***REDACTED***"
    compression               = "gzip"
    concurrency               = 8
    description               = "Deliver events to AWS Kinesis Data Streams"
    duration_seconds          = 3600
    enable_assume_role        = true
    endpoint                  = "https://kinesis.us-east-1.amazonaws.com"
    environment               = "main"
    flush_period_sec          = 1
    id                        = "kinesis-out"
    max_record_size_kb        = 1024
    on_backpressure           = "block"
    pipeline                  = "default"
    pq_compress               = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size    = "100 MB"
    pq_max_size         = "10GB"
    pq_mode             = "backpressure"
    pq_on_backpressure  = "block"
    pq_path             = "/opt/cribl/state/queues"
    region              = "us-east-1"
    reject_unauthorized = true
    reuse_connections   = true
    signature_version   = "v4"
    stream_name         = "app-events-stream"
    streamtags = [
      "aws",
      "kinesis",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    type            = "kinesis"
    use_list_shards = true
  }
  output_loki = {
    auth_type          = "token"
    compress           = true
    concurrency        = 2
    credentials_secret = "grafana_loki_credentials"
    description        = "Send logs to Loki with labels and batching"
    environment        = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 10
    id                          = "loki_logs_prod"
    labels = [
      {
        name  = "host"
        value = "web-01"
      }
    ]
    max_payload_events  = 1000
    max_payload_size_kb = 2048
    message             = "_raw"
    message_format      = "protobuf"
    on_backpressure     = "block"
    password            = "glc_abcd1234"
    pipeline            = "main"
    pq_compress         = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "loki",
    ]
    system_fields = [
      "cribl_host",
      "cribl_wp",
    ]
    text_secret = "grafana_loki_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    token                 = "12345:glc_abcd1234"
    total_memory_limit_kb = 51200
    type                  = "loki"
    url                   = "https://loki.example.com/loki/api/v1/push"
    use_round_robin_dns   = true
    username              = 12345
  }
  output_minio = {
    add_id_to_stage_path      = true
    automatic_schema          = true
    aws_api_key               = "minio_access_key"
    aws_authentication_method = "manual"
    aws_secret                = "minio_credentials"
    aws_secret_key            = "minio_secret_key_123"
    base_file_name            = "app-logs"
    bucket                    = "logs-prod"
    compress                  = "gzip"
    compression_level         = "normal"
    deadletter_enabled        = true
    deadletter_path           = "/opt/cribl/state/outputs/dead-letter"
    description               = "Archive logs to MinIO in Parquet with date-based partitioning"
    dest_path                 = "year=%Y/month=%m/day=%d/app=orders"
    empty_dir_cleanup_sec     = 600
    enable_page_checksum      = true
    enable_statistics         = true
    enable_write_page_index   = true
    endpoint                  = "http://minio.example.com:9000"
    environment               = "main"
    file_name_suffix          = ".json.gz"
    format                    = "json"
    header_line               = "timestamp,host,level,message"
    id                        = "minio_archive_prod"
    key_value_metadata = [
      {
        key   = "OCSF Event Class"
        value = "9001"
      }
    ]
    max_concurrent_file_parts = 5
    max_file_idle_time_sec    = 120
    max_file_open_time_sec    = 600
    max_file_size_mb          = 128
    max_open_files            = 200
    max_retry_num             = 20
    object_acl                = "private"
    on_backpressure           = "block"
    on_disk_full_backpressure = "block"
    parquet_data_page_version = "DATA_PAGE_V2"
    parquet_page_size         = "128MB"
    parquet_row_group_length  = 100000
    parquet_version           = "PARQUET_2_6"
    partition_expr            = "2024/01/15"
    pipeline                  = "main"
    region                    = "us-east-1"
    reject_unauthorized       = true
    remove_empty_dirs         = true
    reuse_connections         = true
    server_side_encryption    = "AES256"
    should_log_invalid_rows   = true
    signature_version         = "v4"
    stage_path                = "/opt/cribl/state/outputs/staging"
    storage_class             = "STANDARD"
    streamtags = [
      "prod",
      "minio",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type                  = "minio"
    verify_permissions    = true
    write_high_water_mark = 256
  }
  output_msk = {
    ack                       = 1
    assume_role_arn           = "arn:aws:iam::123456789012:role/cribl-msk-access"
    assume_role_external_id   = "cribl-external-123"
    authentication_timeout    = 10000
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws-credentials-secret"
    aws_secret_key            = "***REDACTED***"
    backoff_rate              = 2
    brokers = [
      "b-1.mskcluster.abcde.c2.kafka.us-east-1.amazonaws.com:9092",
      "b-2.mskcluster.abcde.c2.kafka.us-east-1.amazonaws.com:9092",
    ]
    compression        = "gzip"
    connection_timeout = 10000
    description        = "Produce events to Amazon MSK with retries and TLS"
    duration_seconds   = 3600
    enable_assume_role = true
    endpoint           = "https://kafka.us-east-1.amazonaws.com"
    environment        = "main"
    flush_event_count  = 1000
    flush_period_sec   = 1
    format             = "json"
    id                 = "msk-out"
    initial_backoff    = 1000
    kafka_schema_registry = {
      auth = {
        credentials_secret = "msk-schema-registry-basic"
        disabled           = false
      }
      connection_timeout      = 30000
      default_key_schema_id   = 1
      default_value_schema_id = 100
      disabled                = true
      max_retries             = 3
      request_timeout         = 30000
      schema_registry_url     = "https://schema-registry.example.com:8081"
      tls = {
        ca_path             = "/etc/ssl/certs/ca.pem"
        cert_path           = "/etc/ssl/certs/client.crt"
        certificate_name    = "msk-client-cert"
        disabled            = false
        max_version         = "TLSv1.3"
        min_version         = "TLSv1.2"
        passphrase          = "***REDACTED***"
        priv_key_path       = "/etc/ssl/private/client.key"
        reject_unauthorized = true
        servername          = "schema-registry.example.com"
      }
    }
    max_back_off       = 60000
    max_record_size_kb = 768
    max_retries        = 5
    on_backpressure    = "block"
    pipeline           = "default"
    pq_compress        = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size           = "100 MB"
    pq_max_size                = "10GB"
    pq_mode                    = "backpressure"
    pq_on_backpressure         = "block"
    pq_path                    = "/opt/cribl/state/queues"
    protobuf_library_id        = "user-events-protos"
    reauthentication_threshold = 60000
    region                     = "us-east-1"
    reject_unauthorized        = true
    request_timeout            = 60000
    reuse_connections          = true
    signature_version          = "v4"
    streamtags = [
      "aws",
      "msk",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    tls = {
      ca_path             = "/etc/ssl/certs/ca.pem"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "msk-client-cert"
      disabled            = false
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "b-1.mskcluster.abcde.c2.kafka.us-east-1.amazonaws.com"
    }
    topic = "app-events"
    type  = "msk"
  }
  output_netflow = {
    description            = "Forward NetFlow v5/v9/IPFIX to downstream collectors"
    dns_resolve_period_sec = 300
    environment            = "main"
    hosts = [
      {
        host = "netflow-collector.example.com"
        port = 2055
      }
    ]
    id       = "netflow_export_prod"
    pipeline = "main"
    streamtags = [
      "prod",
      "netflow",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type = "netflow"
  }
  output_newrelic = {
    api_key     = "NRAK-0123456789abcdef0123456789abcdef"
    auth_type   = "manual"
    compress    = true
    concurrency = 8
    custom_url  = "https://log-api.newrelic.com/log/v1"
    description = "Send logs to New Relic Logs with custom endpoint"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    id                          = "newrelic_logs_prod"
    log_type                    = "access_log"
    max_payload_events          = 500
    max_payload_size_kb         = 512
    message_field               = "_raw"
    metadata = [
      {
        name  = "service"
        value = "`\"orders-service\"`"
      }
    ]
    on_backpressure = "block"
    pipeline        = "main"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    region                            = "US"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "newrelic",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "newrelic_api_key"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    total_memory_limit_kb = 51200
    type                  = "newrelic"
    use_round_robin_dns   = true
  }
  output_newrelic_events = {
    account_id  = "12345678"
    api_key     = "NRAK-0123456789abcdef0123456789abcdef"
    auth_type   = "secret"
    compress    = true
    concurrency = 8
    custom_url  = "https://insights-collector.newrelic.com/v1/accounts/12345678/events"
    description = "Send custom events to New Relic Events API"
    environment = "main"
    event_type  = "CriblCustomEvent"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    id                          = "newrelic_events_prod"
    max_payload_events          = 500
    max_payload_size_kb         = 512
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    region                            = "US"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "newrelic",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "newrelic_api_key"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    type                = "newrelic_events"
    use_round_robin_dns = true
  }
  output_open_telemetry = {
    auth_header_expr   = "`Bearer ${token}`"
    auth_type          = "token"
    compress           = "gzip"
    concurrency        = 5
    connection_timeout = 10000
    credentials_secret = "otel_basic_auth"
    description        = "Export telemetry to OTel Collector with OAuth and keepalive"
    endpoint           = "https://otel-collector.example.com:4317"
    environment        = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode    = "payloadAndHeaders"
    flush_period_sec               = 2
    http_compress                  = "gzip"
    http_logs_endpoint_override    = "https://otel-collector.example.com/v1/logs"
    http_metrics_endpoint_override = "https://otel-collector.example.com/v1/metrics"
    http_traces_endpoint_override  = "https://otel-collector.example.com/v1/traces"
    id                             = "otel_export_prod"
    keep_alive                     = true
    keep_alive_time                = 30
    login_url                      = "https://auth.example.com/oauth/token"
    max_payload_size_kb            = 2048
    metadata = [
      {
        key   = "x-tenant-id"
        value = "acme-prod"
      }
    ]
    oauth_headers = [
      {
        name  = "Accept"
        value = "application/json"
      }
    ]
    oauth_params = [
      {
        name  = "grant_type"
        value = "client_credentials"
      }
    ]
    on_backpressure = "block"
    otlp_version    = "1.3.1"
    password        = "s3cr3tPass!"
    pipeline        = "main"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    protocol                          = "grpc"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    secret            = "s3cr3tClientSecret"
    secret_param_name = "client_secret"
    streamtags = [
      "prod",
      "otel",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret = "otel_bearer_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    tls = {
      ca_path             = "/etc/ssl/certs/ca-bundle.crt"
      cert_path           = "/opt/cribl/certs/client.crt"
      certificate_name    = "otel-client"
      disabled            = true
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "s3cr3t"
      priv_key_path       = "/opt/cribl/certs/client.key"
      reject_unauthorized = true
    }
    token                = "otelBearerToken_abc123xyz"
    token_attribute_name = "access_token"
    token_timeout_secs   = 3600
    type                 = "open_telemetry"
    use_round_robin_dns  = true
    username             = "otel_user"
  }
  output_prometheus = {
    auth_header_expr   = "`Bearer ${token}`"
    auth_type          = "basic"
    concurrency        = 8
    credentials_secret = "prometheus_basic_auth"
    description        = "Send metrics to Prometheus remote_write with basic auth"
    environment        = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    id                          = "prometheus_metrics_prod"
    login_url                   = "https://auth.example.com/oauth/token"
    max_payload_events          = 1000
    max_payload_size_kb         = 2048
    metric_rename_expr          = "name.replace(/[^a-zA-Z0-9_]/g, '_')"
    metrics_flush_period_sec    = 60
    oauth_headers = [
      {
        name  = "Accept"
        value = "application/json"
      }
    ]
    oauth_params = [
      {
        name  = "grant_type"
        value = "client_credentials"
      }
    ]
    on_backpressure = "block"
    password        = "mimir_api_key_abcd1234"
    pipeline        = "metrics"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    secret            = "s3cr3tClientSecret"
    secret_param_name = "client_secret"
    send_metadata     = true
    streamtags = [
      "prod",
      "prometheus",
    ]
    system_fields = [
      "cribl_host",
      "cribl_wp",
    ]
    text_secret = "prometheus_bearer_token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec          = 30
    token                = "promBearerToken_abc123xyz"
    token_attribute_name = "access_token"
    token_timeout_secs   = 3600
    type                 = "prometheus"
    url                  = "https://prometheus.example.com/api/v1/write"
    use_round_robin_dns  = true
    username             = "prometheus"
  }
  output_ring = {
    compress        = "gzip"
    description     = "Local ring buffer for short-term retention and replay"
    dest_path       = "/opt/cribl/state/ring_buffer_prod"
    environment     = "main"
    format          = "json"
    id              = "ring_buffer_prod"
    max_data_size   = "100GB"
    max_data_time   = "7d"
    on_backpressure = "block"
    partition_expr  = ""
    pipeline        = "main"
    streamtags = [
      "prod",
      "ring",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type = "ring"
  }
  output_router = {
    description = "Route events to outputs based on filter rules"
    environment = "main"
    id          = "router_main"
    pipeline    = "main"
    rules = [
      {
        description = "Route application errors to Splunk"
        filter      = "`_source == \"app\" && level == \"error\"`"
        final       = true
        output      = "OutputSplunk"
      }
    ]
    streamtags = [
      "prod",
      "routing",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type = "router"
  }
  output_s3 = {
    add_id_to_stage_path      = true
    assume_role_arn           = "arn:aws:iam::123456789012:role/cribl-s3-writer"
    assume_role_external_id   = "cribl-external-123"
    automatic_schema          = true
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws-credentials-secret"
    aws_secret_key            = "***REDACTED***"
    base_file_name            = "`CriblOut`"
    bucket                    = "cribl-data-bucket"
    compress                  = "gzip"
    compression_level         = "normal"
    deadletter_enabled        = true
    deadletter_path           = "/var/lib/cribl/state/outputs/dead-letter"
    description               = "Write objects to S3 with date-based partitioning"
    dest_path                 = "logs/ingest"
    duration_seconds          = 3600
    empty_dir_cleanup_sec     = 600
    enable_assume_role        = true
    enable_page_checksum      = true
    enable_statistics         = true
    enable_write_page_index   = true
    endpoint                  = "https://s3.us-east-1.amazonaws.com"
    environment               = "main"
    file_name_suffix          = ".json.gz"
    format                    = "json"
    header_line               = "timestamp,host,message"
    id                        = "s3-out"
    key_value_metadata = [
      {
        key   = "team"
        value = "platform"
      }
    ]
    kms_key_id                        = "arn:aws:kms:us-east-1:123456789012:key/abcd1234-5678-90ab-cdef-EXAMPLEKEY"
    max_closing_files_to_backpressure = 500
    max_concurrent_file_parts         = 4
    max_file_idle_time_sec            = 30
    max_file_open_time_sec            = 300
    max_file_size_mb                  = 64
    max_open_files                    = 200
    max_retry_num                     = 20
    object_acl                        = "private"
    on_backpressure                   = "block"
    on_disk_full_backpressure         = "block"
    parquet_data_page_version         = "DATA_PAGE_V2"
    parquet_page_size                 = "4MB"
    parquet_row_group_length          = 10000
    parquet_version                   = "PARQUET_2_6"
    partition_expr                    = "2024/01/15"
    pipeline                          = "default"
    region                            = "us-east-1"
    reject_unauthorized               = true
    remove_empty_dirs                 = true
    reuse_connections                 = true
    server_side_encryption            = "AES256"
    should_log_invalid_rows           = true
    signature_version                 = "v4"
    stage_path                        = "/var/lib/cribl/state/outputs/staging"
    storage_class                     = "STANDARD_IA"
    streamtags = [
      "s3",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    type                  = "s3"
    verify_permissions    = true
    write_high_water_mark = 256
  }
  output_security_lake = {
    account_id                = "123456789012"
    add_id_to_stage_path      = true
    assume_role_arn           = "arn:aws:iam::123456789012:role/SecurityLakeIngestRole"
    assume_role_external_id   = "external-id-abc123"
    automatic_schema          = true
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws_security_lake_credentials"
    aws_secret_key            = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    base_file_name            = "app-logs"
    bucket                    = "security-lake-us-east-1-123456789012"
    custom_source             = "cribl_custom_source"
    deadletter_enabled        = true
    deadletter_path           = "/opt/cribl/state/outputs/dead-letter"
    description               = "Deliver OCSF-compliant logs to Amazon Security Lake"
    duration_seconds          = 3600
    empty_dir_cleanup_sec     = 600
    enable_assume_role        = true
    enable_page_checksum      = true
    enable_statistics         = true
    enable_write_page_index   = true
    endpoint                  = "https://security-lake.us-east-1.amazonaws.com"
    environment               = "main"
    header_line               = "timestamp,host,level,message"
    id                        = "security_lake_export_prod"
    key_value_metadata = [
      {
        key   = "OCSF Event Class"
        value = "9001"
      }
    ]
    kms_key_id                        = "arn:aws:kms:us-east-1:123456789012:key/abcd-1234-efgh-5678"
    max_closing_files_to_backpressure = 500
    max_concurrent_file_parts         = 5
    max_file_idle_time_sec            = 120
    max_file_open_time_sec            = 600
    max_file_size_mb                  = 256
    max_open_files                    = 200
    max_retry_num                     = 20
    object_acl                        = "private"
    on_backpressure                   = "block"
    on_disk_full_backpressure         = "block"
    parquet_data_page_version         = "DATA_PAGE_V2"
    parquet_page_size                 = "128MB"
    parquet_row_group_length          = 100000
    parquet_schema                    = "ocsf_1_1_0"
    parquet_version                   = "PARQUET_2_6"
    pipeline                          = "main"
    region                            = "us-east-1"
    reject_unauthorized               = true
    remove_empty_dirs                 = true
    reuse_connections                 = true
    server_side_encryption            = "aws:kms"
    should_log_invalid_rows           = true
    signature_version                 = "v4"
    stage_path                        = "/opt/cribl/state/outputs/staging"
    storage_class                     = "INTELLIGENT_TIERING"
    streamtags = [
      "prod",
      "securitylake",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type                  = "security_lake"
    verify_permissions    = true
    write_high_water_mark = 256
  }
  output_sentinel = {
    advanced_content_type      = "application/json"
    auth_type                  = "oauth"
    client_id                  = "11111111-2222-3333-4444-555555555555"
    compress                   = true
    concurrency                = 8
    custom_content_type        = "application/x-ndjson"
    custom_drop_when_null      = false
    custom_event_delimiter     = "\n"
    custom_payload_expression  = "`{ \"items\": [${events}] }`"
    custom_source_expression   = "raw=${_raw}"
    dce_endpoint               = "https://mydce-abc123.eastus.ingest.monitor.azure.com"
    dcr_id                     = "12345678-90ab-cdef-1234-567890abcdef"
    description                = "Send events to Microsoft Sentinel (DCR/DCE)"
    endpoint_url_configuration = "url"
    environment                = "main"
    extra_http_headers = [
      {
        name  = "Content-Type"
        value = "application/json"
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    format                      = "ndjson"
    format_event_code           = "if (__e.level === 'error') { __e.__eventOut = JSON.stringify(__e); }"
    format_payload_code         = "__e.__payloadOut = JSON.stringify({ records: __e.payload });"
    id                          = "sentinel-out"
    keep_alive                  = true
    login_url                   = "https://login.microsoftonline.com/<tenant>/oauth2/v2.0/token"
    max_payload_events          = 500
    max_payload_size_kb         = 1000
    on_backpressure             = "queue"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    scope       = "https://monitor.azure.com/.default"
    secret      = "***REDACTED***"
    stream_name = "Custom-MyTable_CL"
    streamtags = [
      "azure",
      "sentinel",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    total_memory_limit_kb = 20480
    type                  = "sentinel"
    url                   = "https://example.dce.ingest.monitor.azure.com"
    use_round_robin_dns   = true
  }
  output_service_now = {
    auth_token_name    = "lightstep-access-token"
    compress           = "gzip"
    concurrency        = 5
    connection_timeout = 10000
    description        = "Export telemetry to ServiceNow (Lightstep) OTLP ingest"
    endpoint           = "ingest.lightstep.com:443"
    environment        = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode    = "payloadAndHeaders"
    flush_period_sec               = 2
    http_compress                  = "gzip"
    http_logs_endpoint_override    = "https://ingest.lightstep.com/v1/logs"
    http_metrics_endpoint_override = "https://ingest.lightstep.com/v1/metrics"
    http_traces_endpoint_override  = "https://ingest.lightstep.com/v1/traces"
    id                             = "servicenow_otel_export"
    keep_alive                     = true
    keep_alive_time                = 30
    max_payload_size_kb            = 2048
    metadata = [
      {
        key   = "x-tenant-id"
        value = "acme-prod"
      }
    ]
    on_backpressure = "block"
    otlp_version    = "1.3.1"
    pipeline        = "main"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    protocol                          = "grpc"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "servicenow",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    tls = {
      ca_path             = "/etc/ssl/certs/ca-bundle.crt"
      cert_path           = "/opt/cribl/certs/client.crt"
      certificate_name    = "otel-client"
      disabled            = true
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "s3cr3t"
      priv_key_path       = "/opt/cribl/certs/client.key"
      reject_unauthorized = true
    }
    token_secret        = "servicenow_access_token"
    type                = "service_now"
    use_round_robin_dns = true
  }
  output_signalfx = {
    auth_type   = "manual"
    compress    = true
    concurrency = 8
    description = "Send metrics to Splunk Observability (SignalFx)"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    id                          = "signalfx-out"
    max_payload_events          = 0
    max_payload_size_kb         = 4096
    on_backpressure             = "block"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    realm                             = "us1"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "signalfx",
      "metrics",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    text_secret = "signalfx-api-token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    token               = "***REDACTED***"
    type                = "signalfx"
    use_round_robin_dns = true
  }
  output_snmp = {
    description            = "Forward SNMP traps to network monitoring systems"
    dns_resolve_period_sec = 300
    environment            = "main"
    hosts = [
      {
        host = "snmp01.example.com"
        port = 162
      }
    ]
    id       = "snmp_trap_forwarder"
    pipeline = "main"
    streamtags = [
      "prod",
      "snmp",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type = "snmp"
  }
  output_sns = {
    assume_role_arn           = "arn:aws:iam::123456789012:role/SNSPublisher"
    assume_role_external_id   = "external-id-abc123"
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws_sns_credentials"
    aws_secret_key            = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    description               = "Publish alerts to Amazon SNS FIFO topic"
    duration_seconds          = 3600
    enable_assume_role        = true
    endpoint                  = "https://sns.us-east-1.amazonaws.com"
    environment               = "main"
    id                        = "sns_alerts_prod"
    max_retries               = 5
    message_group_id          = "`alerts-${C.vars.service}`"
    on_backpressure           = "block"
    pipeline                  = "main"
    pq_compress               = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size    = "100 MB"
    pq_max_size         = "10GB"
    pq_mode             = "backpressure"
    pq_on_backpressure  = "block"
    pq_path             = "/opt/cribl/state/queues"
    region              = "us-east-1"
    reject_unauthorized = true
    reuse_connections   = true
    signature_version   = "v4"
    streamtags = [
      "prod",
      "alerts",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    topic_arn = "arn:aws:sns:us-east-1:123456789012:alerts-topic"
    type      = "sns"
  }
  output_splunk = {
    auth_token               = "***REDACTED***"
    auth_type                = "manual"
    compress                 = "auto"
    connection_timeout       = 10000
    description              = "Send events to Splunk indexers over S2S"
    enable_ack               = true
    enable_multi_metrics     = false
    environment              = "main"
    host                     = "splunk-indexer.example.com"
    id                       = "splunk-main"
    log_failed_requests      = false
    max_failed_health_checks = 1
    max_s2_sversion          = "v3"
    nested_fields            = "none"
    on_backpressure          = "block"
    pipeline                 = "default"
    port                     = 9997
    pq_compress              = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    streamtags = [
      "splunk",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    text_secret           = "splunk-indexer-token"
    throttle_rate_per_sec = "50 MB"
    tls = {
      ca_path             = "/etc/ssl/certs/ca.pem"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "splunk-client-cert"
      disabled            = false
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "splunk-indexer.example.com"
    }
    type          = "splunk"
    write_timeout = 60000
  }
  output_splunk_hec = {
    auth_type              = "manual"
    compress               = true
    concurrency            = 8
    description            = "Send events to Splunk HEC"
    dns_resolve_period_sec = 300
    enable_multi_metrics   = false
    environment            = "main"
    exclude_self           = false
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    failed_request_logging_mode   = "payload"
    flush_period_sec              = 1
    id                            = "splunk-hec-main"
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    max_payload_events            = 0
    max_payload_size_kb           = 4096
    next_queue                    = "indexQueue"
    on_backpressure               = "block"
    pipeline                      = "default"
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "splunk",
      "hec",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    tcp_routing = "default_route"
    text_secret = "splunk-hec-token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    token       = "***REDACTED***"
    type        = "splunk_hec"
    url         = "http://splunk-hec.example.com:8088/services/collector/event"
    urls = [
      {
        url    = "http://splunk-hec-2.example.com:8088/services/collector/event"
        weight = 2
      }
    ]
    use_round_robin_dns = true
  }
  output_splunk_lb = {
    auth_token             = "***REDACTED***"
    auth_type              = "manual"
    compress               = "auto"
    connection_timeout     = 10000
    description            = "Load-balance events across Splunk indexers"
    dns_resolve_period_sec = 300
    enable_ack             = true
    enable_multi_metrics   = false
    environment            = "main"
    exclude_self           = false
    hosts = [
      {
        host       = "idx1.example.com"
        port       = 9997
        servername = "idx1.example.com"
        tls        = "inherit"
        weight     = 2
      }
    ]
    id                = "splunk-lb-main"
    indexer_discovery = true
    indexer_discovery_configs = {
      auth_token = "***REDACTED***"
      auth_tokens = [
        {
          auth_type = "secret"
        }
      ]
      auth_type            = "manual"
      master_uri           = "https://cm.example.com:8089"
      refresh_interval_sec = 300
      reject_unauthorized  = true
      site                 = "site1"
      text_secret          = "cluster-manager-token"
    }
    load_balance_stats_period_sec = 300
    log_failed_requests           = false
    max_concurrent_senders        = 8
    max_failed_health_checks      = 1
    max_s2_sversion               = "v3"
    nested_fields                 = "none"
    on_backpressure               = "block"
    pipeline                      = "default"
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                = "100 MB"
    pq_max_size                     = "10GB"
    pq_mode                         = "backpressure"
    pq_on_backpressure              = "block"
    pq_path                         = "/opt/cribl/state/queues"
    sender_unhealthy_time_allowance = 500
    streamtags = [
      "splunk",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    text_secret           = "splunk-indexer-token"
    throttle_rate_per_sec = "50 MB"
    tls = {
      ca_path             = "/etc/ssl/certs/ca.pem"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "splunk-client-cert"
      disabled            = false
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "splunk-lb.example.com"
    }
    type          = "splunk_lb"
    write_timeout = 60000
  }
  output_sqs = {
    assume_role_arn           = "arn:aws:iam::123456789012:role/SQSPublisher"
    assume_role_external_id   = "external-id-abc123"
    aws_account_id            = "123456789012"
    aws_api_key               = "AKIAIOSFODNN7EXAMPLE"
    aws_authentication_method = "auto"
    aws_secret                = "aws_sqs_credentials"
    aws_secret_key            = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    create_queue              = true
    description               = "Send events to Amazon SQS FIFO queue with batching"
    duration_seconds          = 3600
    enable_assume_role        = true
    endpoint                  = "https://sqs.us-east-1.amazonaws.com"
    environment               = "main"
    flush_period_sec          = 2
    id                        = "sqs_events_prod"
    max_in_progress           = 20
    max_queue_size            = 200
    max_record_size_kb        = 256
    message_group_id          = "logs"
    on_backpressure           = "block"
    pipeline                  = "main"
    pq_compress               = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size    = "100 MB"
    pq_max_size         = "10GB"
    pq_mode             = "backpressure"
    pq_on_backpressure  = "block"
    pq_path             = "/opt/cribl/state/queues"
    queue_name          = "https://sqs.us-east-1.amazonaws.com/123456789012/my-queue.fifo"
    queue_type          = "fifo"
    region              = "us-east-1"
    reject_unauthorized = true
    reuse_connections   = true
    signature_version   = "v4"
    streamtags = [
      "prod",
      "sqs",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    type = "sqs"
  }
  output_statsd = {
    connection_timeout     = 10000
    description            = "Send StatsD metrics to central aggregator"
    dns_resolve_period_sec = 300
    environment            = "main"
    flush_period_sec       = 1
    host                   = "statsd.example.com"
    id                     = "statsd_metrics_prod"
    mtu                    = 1400
    on_backpressure        = "block"
    pipeline               = "metrics"
    port                   = 8125
    pq_compress            = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    protocol           = "udp"
    streamtags = [
      "prod",
      "statsd",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    throttle_rate_per_sec = "10 MB"
    type                  = "statsd"
    write_timeout         = 30000
  }
  output_statsd_ext = {
    connection_timeout     = 10000
    description            = "Send extended StatsD metrics to external aggregator"
    dns_resolve_period_sec = 300
    environment            = "main"
    flush_period_sec       = 1
    host                   = "statsd-ext.example.com"
    id                     = "statsd_ext_metrics_prod"
    mtu                    = 1400
    on_backpressure        = "block"
    pipeline               = "metrics"
    port                   = 8125
    pq_compress            = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    protocol           = "udp"
    streamtags = [
      "prod",
      "statsd",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    throttle_rate_per_sec = "10 MB"
    type                  = "statsd_ext"
    write_timeout         = 30000
  }
  output_sumo_logic = {
    compress        = true
    concurrency     = 8
    custom_category = "prod/app/logs"
    custom_source   = "cribl-stream"
    description     = "Send logs to Sumo Logic with retries and batching"
    environment     = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode = "payloadAndHeaders"
    flush_period_sec            = 2
    format                      = "json"
    id                          = "sumologic_logs_prod"
    max_payload_events          = 500
    max_payload_size_kb         = 512
    on_backpressure             = "block"
    pipeline                    = "main"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "sumologic",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    total_memory_limit_kb = 51200
    type                  = "sumo_logic"
    url                   = "https://endpoint1.collection.us2.sumologic.com/receiver/v1/http/ABCDEFG1234567890"
    use_round_robin_dns   = true
  }
  output_syslog = {
    app_name            = "Cribl"
    connection_timeout  = 10000
    description         = "Send syslog to upstream collector"
    environment         = "main"
    facility            = 1
    host                = "syslog.receiver.example.com"
    id                  = "syslog-out"
    load_balanced       = true
    log_failed_requests = false
    max_record_size     = 1200
    message_format      = "rfc3164"
    octet_count_framing = true
    on_backpressure     = "block"
    pipeline            = "default"
    port                = 514
    pq_compress         = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    protocol           = "tcp"
    severity           = 5
    streamtags = [
      "syslog",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    throttle_rate_per_sec = "0"
    timestamp_format      = "syslog"
    tls = {
      ca_path             = "/etc/ssl/certs/ca-bundle.crt"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "syslog-client-cert"
      disabled            = true
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "syslog.example.com"
    }
    type                       = "syslog"
    udp_dns_resolve_period_sec = 300
    write_timeout              = 60000
  }
  output_tcpjson = {
    auth_token             = "***REDACTED***"
    auth_type              = "manual"
    compression            = "gzip"
    connection_timeout     = 10000
    description            = "Send JSON events over TCP to downstream services"
    dns_resolve_period_sec = 300
    environment            = "main"
    exclude_self           = false
    host                   = "tcp.receiver.example.com"
    hosts = [
      {
        host       = "tcp1.example.com"
        port       = 10300
        servername = "tcp1.example.com"
        tls        = "inherit"
        weight     = 2
      }
    ]
    id                            = "tcpjson-out"
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    log_failed_requests           = false
    max_concurrent_senders        = 8
    on_backpressure               = "block"
    pipeline                      = "default"
    port                          = 10300
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size   = "100 MB"
    pq_max_size        = "10GB"
    pq_mode            = "backpressure"
    pq_on_backpressure = "block"
    pq_path            = "/opt/cribl/state/queues"
    send_header        = true
    streamtags = [
      "tcpjson",
      "prod",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    text_secret           = "tcpjson-auth-token"
    throttle_rate_per_sec = "50 MB"
    tls = {
      ca_path             = "/etc/ssl/certs/ca.pem"
      cert_path           = "/etc/ssl/certs/client.crt"
      certificate_name    = "tcpjson-client-cert"
      disabled            = false
      max_version         = "TLSv1.3"
      min_version         = "TLSv1.2"
      passphrase          = "***REDACTED***"
      priv_key_path       = "/etc/ssl/private/client.key"
      reject_unauthorized = true
      servername          = "tcp.receiver.example.com"
    }
    token_ttl_minutes = 60
    type              = "tcpjson"
    write_timeout     = 60000
  }
  output_wavefront = {
    auth_type   = "manual"
    compress    = true
    concurrency = 8
    description = "Send metrics to WaveFront"
    domain      = "longboard"
    environment = "main"
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "abc123"
      }
    ]
    failed_request_logging_mode = "payload"
    flush_period_sec            = 1
    id                          = "wavefront-out"
    max_payload_events          = 0
    max_payload_size_kb         = 4096
    on_backpressure             = "block"
    pipeline                    = "default"
    pq_compress                 = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 503
        initial_backoff = 1000
        max_backoff     = 60000
      }
    ]
    safe_headers = [
      "X-Request-ID",
    ]
    streamtags = [
      "wavefront",
      "metrics",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    text_secret = "wavefront-api-token"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec         = 30
    token               = "***REDACTED***"
    type                = "wavefront"
    use_round_robin_dns = true
  }
  output_webhook = {
    advanced_content_type     = "application/json"
    auth_header_expr          = "`Bearer ${token}`"
    auth_type                 = "token"
    compress                  = true
    concurrency               = 10
    credentials_secret        = "webhook-credentials"
    custom_content_type       = "application/x-ndjson"
    custom_drop_when_null     = false
    custom_event_delimiter    = "\n"
    custom_payload_expression = "`{ \"items\": [${events}] }`"
    custom_source_expression  = "raw=${_raw}"
    description               = "Robust webhook delivery with backoff and retries"
    dns_resolve_period_sec    = 600
    environment               = "main"
    exclude_self              = true
    extra_http_headers = [
      {
        name  = "X-Custom-Header"
        value = "demo"
      }
    ]
    failed_request_logging_mode   = "payload"
    flush_period_sec              = 1
    format                        = "ndjson"
    format_event_code             = "if (__e.severity === 'error') { __e.__eventOut = JSON.stringify(__e); }"
    format_payload_code           = "__e.__payloadOut = JSON.stringify({ items: __e.payload });"
    id                            = "webhook-out"
    keep_alive                    = true
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    login_url                     = "https://auth.example.com/oauth/token"
    max_payload_events            = 1000
    max_payload_size_kb           = 8192
    method                        = "POST"
    oauth_headers = [
      {
        name  = "Accept"
        value = "application/json"
      }
    ]
    oauth_params = [
      {
        name  = "grant_type"
        value = "client_credentials"
      }
    ]
    on_backpressure = "queue"
    password        = "***REDACTED***"
    pipeline        = "default"
    pq_compress     = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "X-Trace-Id",
    ]
    secret            = "s3cr3t"
    secret_param_name = "client_secret"
    streamtags = [
      "webhook",
    ]
    system_fields = [
      "cribl_pipe",
    ]
    text_secret = "webhook-token-secret"
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec = 30
    tls = {
      ca_path          = "/etc/ssl/certs/ca-bundle.crt"
      cert_path        = "/etc/ssl/certs/client.crt"
      certificate_name = "webhook-client-cert"
      disabled         = true
      max_version      = "TLSv1.3"
      min_version      = "TLSv1.2"
      passphrase       = "***REDACTED***"
      priv_key_path    = "/etc/ssl/private/client.key"
      servername       = "api.example.com"
    }
    token                 = "***REDACTED***"
    token_attribute_name  = "access_token"
    token_timeout_secs    = 3600
    total_memory_limit_kb = 20480
    type                  = "webhook"
    url                   = "https://hooks.example.com/ingest"
    urls = [
      {
        url    = "https://hooks1.example.com/ingest"
        weight = 2
      }
    ]
    use_round_robin_dns = true
    username            = "api-user"
  }
  output_xsiam = {
    auth_type              = "secret"
    concurrency            = 8
    description            = "Send logs to Palo Alto Networks XSIAM with token auth"
    dns_resolve_period_sec = 300
    environment            = "main"
    exclude_self           = false
    extra_http_headers = [
      {
        name  = "X-Request-ID"
        value = "123e4567-e89b-12d3-a456-426614174000"
      }
    ]
    failed_request_logging_mode   = "payloadAndHeaders"
    flush_period_sec              = 2
    id                            = "xsiam_export_prod"
    load_balance_stats_period_sec = 300
    load_balanced                 = true
    max_payload_events            = 2000
    max_payload_size_kb           = 8192
    on_backpressure               = "block"
    pipeline                      = "main"
    pq_compress                   = "gzip"
    pq_controls = {
      # ...
    }
    pq_max_file_size                  = "100 MB"
    pq_max_size                       = "10GB"
    pq_mode                           = "backpressure"
    pq_on_backpressure                = "block"
    pq_path                           = "/opt/cribl/state/queues"
    reject_unauthorized               = true
    response_honor_retry_after_header = true
    response_retry_settings = [
      {
        backoff_rate    = 2
        http_status     = 429
        initial_backoff = 1000
        max_backoff     = 30000
      }
    ]
    safe_headers = [
      "content-type",
      "x-request-id",
    ]
    streamtags = [
      "prod",
      "xsiam",
    ]
    system_fields = [
      "cribl_pipe",
      "cribl_breaker",
    ]
    text_secret               = "xsiam_token"
    throttle_rate_req_per_sec = 500
    timeout_retry_settings = {
      backoff_rate    = 2
      initial_backoff = 1000
      max_backoff     = 30000
      timeout_retry   = true
    }
    timeout_sec           = 30
    token                 = "xsiam-0123456789abcdef0123456789abcdef"
    total_memory_limit_kb = 51200
    type                  = "xsiam"
    url                   = "https://api-tenant.paloaltonetworks.com/logs/v1/event"
    urls = [
      {
        url    = "{ \"see\": \"documentation\" }"
        weight = 3.29
      }
    ]
    use_round_robin_dns = true
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `group_id` (String) The consumer group to which this instance belongs. Defaults to 'Cribl'.
- `id` (String) The consumer group id to create

### Optional

- `output_azure_blob` (Attributes) (see [below for nested schema](#nestedatt--output_azure_blob))
- `output_azure_data_explorer` (Attributes) (see [below for nested schema](#nestedatt--output_azure_data_explorer))
- `output_azure_eventhub` (Attributes) (see [below for nested schema](#nestedatt--output_azure_eventhub))
- `output_azure_logs` (Attributes) (see [below for nested schema](#nestedatt--output_azure_logs))
- `output_click_house` (Attributes) (see [below for nested schema](#nestedatt--output_click_house))
- `output_cloudwatch` (Attributes) (see [below for nested schema](#nestedatt--output_cloudwatch))
- `output_confluent_cloud` (Attributes) (see [below for nested schema](#nestedatt--output_confluent_cloud))
- `output_cribl_http` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_http))
- `output_cribl_lake` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_lake))
- `output_cribl_tcp` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_tcp))
- `output_crowdstrike_next_gen_siem` (Attributes) (see [below for nested schema](#nestedatt--output_crowdstrike_next_gen_siem))
- `output_datadog` (Attributes) (see [below for nested schema](#nestedatt--output_datadog))
- `output_dataset` (Attributes) (see [below for nested schema](#nestedatt--output_dataset))
- `output_default` (Attributes) (see [below for nested schema](#nestedatt--output_default))
- `output_devnull` (Attributes) (see [below for nested schema](#nestedatt--output_devnull))
- `output_disk_spool` (Attributes) (see [below for nested schema](#nestedatt--output_disk_spool))
- `output_dl_s3` (Attributes) (see [below for nested schema](#nestedatt--output_dl_s3))
- `output_dynatrace_http` (Attributes) (see [below for nested schema](#nestedatt--output_dynatrace_http))
- `output_dynatrace_otlp` (Attributes) (see [below for nested schema](#nestedatt--output_dynatrace_otlp))
- `output_elastic` (Attributes) (see [below for nested schema](#nestedatt--output_elastic))
- `output_elastic_cloud` (Attributes) (see [below for nested schema](#nestedatt--output_elastic_cloud))
- `output_exabeam` (Attributes) (see [below for nested schema](#nestedatt--output_exabeam))
- `output_filesystem` (Attributes) (see [below for nested schema](#nestedatt--output_filesystem))
- `output_google_chronicle` (Attributes) (see [below for nested schema](#nestedatt--output_google_chronicle))
- `output_google_cloud_logging` (Attributes) (see [below for nested schema](#nestedatt--output_google_cloud_logging))
- `output_google_cloud_storage` (Attributes) (see [below for nested schema](#nestedatt--output_google_cloud_storage))
- `output_google_pubsub` (Attributes) (see [below for nested schema](#nestedatt--output_google_pubsub))
- `output_grafana_cloud` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud))
- `output_graphite` (Attributes) (see [below for nested schema](#nestedatt--output_graphite))
- `output_honeycomb` (Attributes) (see [below for nested schema](#nestedatt--output_honeycomb))
- `output_humio_hec` (Attributes) (see [below for nested schema](#nestedatt--output_humio_hec))
- `output_influxdb` (Attributes) (see [below for nested schema](#nestedatt--output_influxdb))
- `output_kafka` (Attributes) (see [below for nested schema](#nestedatt--output_kafka))
- `output_kinesis` (Attributes) (see [below for nested schema](#nestedatt--output_kinesis))
- `output_loki` (Attributes) (see [below for nested schema](#nestedatt--output_loki))
- `output_minio` (Attributes) (see [below for nested schema](#nestedatt--output_minio))
- `output_msk` (Attributes) (see [below for nested schema](#nestedatt--output_msk))
- `output_netflow` (Attributes) (see [below for nested schema](#nestedatt--output_netflow))
- `output_newrelic` (Attributes) (see [below for nested schema](#nestedatt--output_newrelic))
- `output_newrelic_events` (Attributes) (see [below for nested schema](#nestedatt--output_newrelic_events))
- `output_open_telemetry` (Attributes) (see [below for nested schema](#nestedatt--output_open_telemetry))
- `output_prometheus` (Attributes) (see [below for nested schema](#nestedatt--output_prometheus))
- `output_ring` (Attributes) (see [below for nested schema](#nestedatt--output_ring))
- `output_router` (Attributes) (see [below for nested schema](#nestedatt--output_router))
- `output_s3` (Attributes) (see [below for nested schema](#nestedatt--output_s3))
- `output_security_lake` (Attributes) (see [below for nested schema](#nestedatt--output_security_lake))
- `output_sentinel` (Attributes) (see [below for nested schema](#nestedatt--output_sentinel))
- `output_service_now` (Attributes) (see [below for nested schema](#nestedatt--output_service_now))
- `output_signalfx` (Attributes) (see [below for nested schema](#nestedatt--output_signalfx))
- `output_snmp` (Attributes) (see [below for nested schema](#nestedatt--output_snmp))
- `output_sns` (Attributes) (see [below for nested schema](#nestedatt--output_sns))
- `output_splunk` (Attributes) (see [below for nested schema](#nestedatt--output_splunk))
- `output_splunk_hec` (Attributes) (see [below for nested schema](#nestedatt--output_splunk_hec))
- `output_splunk_lb` (Attributes) (see [below for nested schema](#nestedatt--output_splunk_lb))
- `output_sqs` (Attributes) (see [below for nested schema](#nestedatt--output_sqs))
- `output_statsd` (Attributes) (see [below for nested schema](#nestedatt--output_statsd))
- `output_statsd_ext` (Attributes) (see [below for nested schema](#nestedatt--output_statsd_ext))
- `output_sumo_logic` (Attributes) (see [below for nested schema](#nestedatt--output_sumo_logic))
- `output_syslog` (Attributes) (see [below for nested schema](#nestedatt--output_syslog))
- `output_tcpjson` (Attributes) (see [below for nested schema](#nestedatt--output_tcpjson))
- `output_wavefront` (Attributes) (see [below for nested schema](#nestedatt--output_wavefront))
- `output_webhook` (Attributes) (see [below for nested schema](#nestedatt--output_webhook))
- `output_xsiam` (Attributes) (see [below for nested schema](#nestedatt--output_xsiam))

### Read-Only

- `items` (List of Map of String)

<a id="nestedatt--output_azure_blob"></a>
### Nested Schema for `output_azure_blob`

Required:

- `container_name` (String) The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backtickss, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `auth_type` (String) Default: "manual"; must be one of ["manual", "secret", "clientSecret", "clientCert"]
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `azure_cloud` (String) The Azure cloud to use. Defaults to Azure Public Cloud.
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `certificate` (Attributes) (see [below for nested schema](#nestedatt--output_azure_blob--certificate))
- `client_id` (String) The service principal's client ID
- `client_text_secret` (String) Select or create a stored text secret
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `compression_level` (String) Compression level to apply before moving files to final destination. Default: "best_speed"; must be one of ["best_speed", "normal", "best_compression"]
- `connection_string` (String) Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
- `create_container` (Boolean) Create the configured container in Azure Blob Storage if it does not already exist. Default: false
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `dest_path` (String) Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `endpoint_suffix` (String) Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_azure_blob--key_value_metadata))
- `max_concurrent_file_parts` (Number) Maximum number of parts to upload in parallel per file. Default: 1
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `partition_expr` (String) JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory. Default: "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `stage_path` (String) Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_account_name` (String) The name of your Azure storage account
- `storage_class` (String) Default: "Inferred"; must be one of ["Inferred", "Hot", "Cool", "Cold", "Archive"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tenant_id` (String) The service principal's tenant ID
- `text_secret` (String) Select or create a stored text secret
- `type` (String) must be "azure_blob"
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_azure_blob--certificate"></a>
### Nested Schema for `output_azure_blob.certificate`

Required:

- `certificate_name` (String) The certificate you registered as credentials for your app in the Azure portal


<a id="nestedatt--output_azure_blob--key_value_metadata"></a>
### Nested Schema for `output_azure_blob.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_azure_data_explorer"></a>
### Nested Schema for `output_azure_data_explorer`

Required:

- `client_id` (String) client_id to pass in the OAuth request parameter
- `cluster_url` (String) The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
- `database` (String) Name of the database containing the table where data will be ingested
- `scope` (String) Scope to pass in the OAuth request parameter
- `table` (String) Name of the table to ingest data into
- `tenant_id` (String) Directory ID (tenant identifier) in Azure Active Directory

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `additional_properties` (Attributes List) Optionally, enter additional configuration properties to send to the ingestion service (see [below for nested schema](#nestedatt--output_azure_data_explorer--additional_properties))
- `certificate` (Attributes) (see [below for nested schema](#nestedatt--output_azure_data_explorer--certificate))
- `client_secret` (String) The client secret that you generated for your app in the Azure portal
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extent_tags` (Attributes List) Strings or tags associated with the extent (ingested data shard) (see [below for nested schema](#nestedatt--output_azure_data_explorer--extent_tags))
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `flush_immediately` (Boolean) Bypass the data management service's aggregation mechanism. Default: false
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `id` (String) Unique ID for this output
- `ingest_if_not_exists` (Attributes List) Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists (see [below for nested schema](#nestedatt--output_azure_data_explorer--ingest_if_not_exists))
- `ingest_mode` (String) Default: "batching"; must be one of ["batching", "streaming"]
- `ingest_url` (String) The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
- `is_mapping_obj` (Boolean) Send a JSON mapping object instead of specifying an existing named data mapping. Default: false
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `mapping_ref` (String) Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
- `max_concurrent_file_parts` (Number) Maximum number of parts to upload in parallel per file. Default: 1
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `oauth_endpoint` (String) Endpoint used to acquire authentication tokens from Azure. Default: "https://login.microsoftonline.com"; must be one of ["https://login.microsoftonline.com", "https://login.microsoftonline.us", "https://login.partner.microsoftonline.cn"]
- `oauth_type` (String) The type of OAuth 2.0 client credentials grant flow to use. Default: "clientSecret"; must be one of ["clientSecret", "clientTextSecret", "certificate"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_azure_data_explorer--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).


        Enabled by default. When this setting is also present in TLS Settings (Client Side),
        that value will take precedence.
Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `report_level` (String) Level of ingestion status reporting. Defaults to FailuresOnly. Default: "failuresOnly"; must be one of ["failuresOnly", "doNotReport", "failuresAndSuccesses"]
- `report_method` (String) Target of the ingestion status reporting. Defaults to Queue. Default: "queue"; must be one of ["queue", "table", "queueAndTable"]
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_azure_data_explorer--response_retry_settings))
- `retain_blob_on_success` (Boolean) Prevent blob deletion after ingestion is complete. Default: false
- `stage_path` (String) Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_azure_data_explorer--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `type` (String) must be "azure_data_explorer"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `validate_database_settings` (Boolean) When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role. Default: true

<a id="nestedatt--output_azure_data_explorer--additional_properties"></a>
### Nested Schema for `output_azure_data_explorer.additional_properties`

Required:

- `key` (String)
- `value` (String)


<a id="nestedatt--output_azure_data_explorer--certificate"></a>
### Nested Schema for `output_azure_data_explorer.certificate`

Optional:

- `certificate_name` (String) The certificate you registered as credentials for your app in the Azure portal


<a id="nestedatt--output_azure_data_explorer--extent_tags"></a>
### Nested Schema for `output_azure_data_explorer.extent_tags`

Required:

- `value` (String)

Optional:

- `prefix` (String) must be one of ["dropBy", "ingestBy"]


<a id="nestedatt--output_azure_data_explorer--ingest_if_not_exists"></a>
### Nested Schema for `output_azure_data_explorer.ingest_if_not_exists`

Required:

- `value` (String)


<a id="nestedatt--output_azure_data_explorer--pq_controls"></a>
### Nested Schema for `output_azure_data_explorer.pq_controls`


<a id="nestedatt--output_azure_data_explorer--response_retry_settings"></a>
### Nested Schema for `output_azure_data_explorer.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_azure_data_explorer--timeout_retry_settings"></a>
### Nested Schema for `output_azure_data_explorer.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_azure_eventhub"></a>
### Nested Schema for `output_azure_eventhub`

Required:

- `brokers` (List of String) List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
- `topic` (String) The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.

Optional:

- `ack` (Number) Control the number of required acknowledgments. Default: 1; must be one of ["1", "0", "-1"]
- `authentication_timeout` (Number) Maximum time to wait for Kafka to respond to an authentication request. Default: 10000
- `backoff_rate` (Number) Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details. Default: 2
- `connection_timeout` (Number) Maximum time to wait for a connection to complete successfully. Default: 10000
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_event_count` (Number) Maximum number of events in a batch before forcing a flush. Default: 1000
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size. Default: 1
- `format` (String) Format to use to serialize events before writing to the Event Hubs Kafka brokers. Default: "json"; must be one of ["json", "raw"]
- `id` (String) Unique ID for this output
- `initial_backoff` (Number) Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes). Default: 300
- `max_back_off` (Number) The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds). Default: 30000
- `max_record_size_kb` (Number) Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers. Default: 768
- `max_retries` (Number) If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data. Default: 5
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_azure_eventhub--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reauthentication_threshold` (Number) Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire. Default: 10000
- `request_timeout` (Number) Maximum time to wait for Kafka to respond to a request. Default: 60000
- `sasl` (Attributes) Authentication parameters to use when connecting to brokers. Using TLS is highly recommended. (see [below for nested schema](#nestedatt--output_azure_eventhub--sasl))
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_azure_eventhub--tls))
- `type` (String) must be "azure_eventhub"

<a id="nestedatt--output_azure_eventhub--pq_controls"></a>
### Nested Schema for `output_azure_eventhub.pq_controls`


<a id="nestedatt--output_azure_eventhub--sasl"></a>
### Nested Schema for `output_azure_eventhub.sasl`

Optional:

- `disabled` (Boolean) Default: false
- `mechanism` (String) Default: "plain"; must be one of ["plain", "oauthbearer"]


<a id="nestedatt--output_azure_eventhub--tls"></a>
### Nested Schema for `output_azure_eventhub.tls`

Optional:

- `disabled` (Boolean) Default: false
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's). Default: true



<a id="nestedatt--output_azure_logs"></a>
### Nested Schema for `output_azure_logs`

Required:

- `type` (String) must be "azure_logs"

Optional:

- `api_url` (String) The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>. Default: ".ods.opinsights.azure.com"
- `auth_type` (String) Enter workspace ID and workspace key directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean)
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_azure_logs--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `keypair_secret` (String) Select or create a stored secret that references your access key and secret key
- `log_type` (String) The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType. Default: "Cribl"
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 1024
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_azure_logs--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `resource_id` (String) Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_azure_logs--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_azure_logs--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `workspace_id` (String) Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.
- `workspace_key` (String) Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.

<a id="nestedatt--output_azure_logs--extra_http_headers"></a>
### Nested Schema for `output_azure_logs.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_azure_logs--pq_controls"></a>
### Nested Schema for `output_azure_logs.pq_controls`


<a id="nestedatt--output_azure_logs--response_retry_settings"></a>
### Nested Schema for `output_azure_logs.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_azure_logs--timeout_retry_settings"></a>
### Nested Schema for `output_azure_logs.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_click_house"></a>
### Nested Schema for `output_click_house`

Required:

- `database` (String)
- `table_name` (String) Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".
- `url` (String) URL of the ClickHouse instance. Example: http://localhost:8123/

Optional:

- `async_inserts` (Boolean) Collect data into batches for later processing. Disable to write to a ClickHouse table immediately. Default: false
- `auth_header_expr` (String) JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`. Default: "`Bearer ${token}`"
- `auth_type` (String) Default: "none"; must be one of ["none", "basic", "credentialsSecret", "sslUserCertificate", "token", "textSecret", "oauth"]
- `column_mappings` (Attributes List) (see [below for nested schema](#nestedatt--output_click_house--column_mappings))
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `credentials_secret` (String) Select or create a secret that references your credentials
- `describe_table` (String) Retrieves the table schema from ClickHouse and populates the Column Mapping table
- `description` (String)
- `dump_format_errors_to_disk` (Boolean) Log the most recent event that fails to match the table schema. Default: false
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_mapping_fields` (List of String) Fields to exclude from sending to ClickHouse. Default: []
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_click_house--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) Data format to use when sending data to ClickHouse. Defaults to JSON Compact. Default: "json-compact-each-row-with-names"; must be one of ["json-compact-each-row-with-names", "json-each-row"]
- `id` (String) Unique ID for this output
- `login_url` (String) URL for OAuth
- `mapping_type` (String) How event fields are mapped to ClickHouse columns. Default: "automatic"; must be one of ["automatic", "custom"]
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `oauth_headers` (Attributes List) Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_click_house--oauth_headers))
- `oauth_params` (Attributes List) Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_click_house--oauth_params))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `password` (String)
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_click_house--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).


        Enabled by default. When this setting is also present in TLS Settings (Client Side),
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_click_house--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `secret` (String) Secret parameter value to pass in request body
- `secret_param_name` (String) Secret parameter name to pass in request body
- `sql_username` (String) Username for certificate authentication
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_click_house--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_click_house--tls))
- `token` (String) Bearer token to include in the authorization header
- `token_attribute_name` (String) Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
- `token_timeout_secs` (Number) How often the OAuth token should be refreshed. Default: 3600
- `type` (String) must be "click_house"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `username` (String)
- `wait_for_async_inserts` (Boolean) Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted. Default: true

<a id="nestedatt--output_click_house--column_mappings"></a>
### Nested Schema for `output_click_house.column_mappings`

Required:

- `column_name` (String) Name of the column in ClickHouse that will store field value
- `column_value_expression` (String) JavaScript expression to compute value to be inserted into ClickHouse table

Optional:

- `column_type` (String) Type of the column in the ClickHouse database


<a id="nestedatt--output_click_house--extra_http_headers"></a>
### Nested Schema for `output_click_house.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_click_house--oauth_headers"></a>
### Nested Schema for `output_click_house.oauth_headers`

Required:

- `name` (String) OAuth header name
- `value` (String) OAuth header value


<a id="nestedatt--output_click_house--oauth_params"></a>
### Nested Schema for `output_click_house.oauth_params`

Required:

- `name` (String) OAuth parameter name
- `value` (String) OAuth parameter value


<a id="nestedatt--output_click_house--pq_controls"></a>
### Nested Schema for `output_click_house.pq_controls`


<a id="nestedatt--output_click_house--response_retry_settings"></a>
### Nested Schema for `output_click_house.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_click_house--timeout_retry_settings"></a>
### Nested Schema for `output_click_house.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_click_house--tls"></a>
### Nested Schema for `output_click_house.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_cloudwatch"></a>
### Nested Schema for `output_cloudwatch`

Required:

- `log_group_name` (String) CloudWatch log group to associate events with
- `log_stream_name` (String) Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId
- `region` (String) Region where the CloudWatchLogs is located

Optional:

- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `aws_api_key` (String)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String)
- `description` (String)
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `enable_assume_role` (Boolean) Use Assume Role credentials to access CloudWatchLogs. Default: false
- `endpoint` (String) CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size. Default: 1
- `id` (String) Unique ID for this output
- `max_queue_size` (Number) Maximum number of queued batches before blocking. Default: 5
- `max_record_size_kb` (Number) Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size. Default: 1024
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_cloudwatch--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "cloudwatch"

<a id="nestedatt--output_cloudwatch--pq_controls"></a>
### Nested Schema for `output_cloudwatch.pq_controls`



<a id="nestedatt--output_confluent_cloud"></a>
### Nested Schema for `output_confluent_cloud`

Required:

- `brokers` (List of String) List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.
- `topic` (String) The topic to publish events to. Can be overridden using the __topicOut field.

Optional:

- `ack` (Number) Control the number of required acknowledgments. Default: 1; must be one of ["1", "0", "-1"]
- `authentication_timeout` (Number) Maximum time to wait for Kafka to respond to an authentication request. Default: 10000
- `backoff_rate` (Number) Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details. Default: 2
- `compression` (String) Codec to use to compress the data before sending to Kafka. Default: "gzip"; must be one of ["none", "gzip", "snappy", "lz4"]
- `connection_timeout` (Number) Maximum time to wait for a connection to complete successfully. Default: 10000
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_event_count` (Number) The maximum number of events you want the Destination to allow in a batch before forcing a flush. Default: 1000
- `flush_period_sec` (Number) The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent. Default: 1
- `format` (String) Format to use to serialize events before writing to Kafka. Default: "json"; must be one of ["json", "raw", "protobuf"]
- `id` (String) Unique ID for this output
- `initial_backoff` (Number) Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes). Default: 300
- `kafka_schema_registry` (Attributes) (see [below for nested schema](#nestedatt--output_confluent_cloud--kafka_schema_registry))
- `max_back_off` (Number) The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds). Default: 30000
- `max_record_size_kb` (Number) Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting. Default: 768
- `max_retries` (Number) If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data. Default: 5
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_confluent_cloud--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protobuf_library_id` (String) Select a set of Protobuf definitions for the events you want to send
- `reauthentication_threshold` (Number) Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire. Default: 10000
- `request_timeout` (Number) Maximum time to wait for Kafka to respond to a request. Default: 60000
- `sasl` (Attributes) Authentication parameters to use when connecting to brokers. Using TLS is highly recommended. (see [below for nested schema](#nestedatt--output_confluent_cloud--sasl))
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_confluent_cloud--tls))
- `type` (String) must be "confluent_cloud"

<a id="nestedatt--output_confluent_cloud--kafka_schema_registry"></a>
### Nested Schema for `output_confluent_cloud.kafka_schema_registry`

Optional:

- `auth` (Attributes) Credentials to use when authenticating with the schema registry using basic HTTP authentication (see [below for nested schema](#nestedatt--output_confluent_cloud--kafka_schema_registry--auth))
- `connection_timeout` (Number) Maximum time to wait for a Schema Registry connection to complete successfully. Default: 30000
- `default_key_schema_id` (Number) Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
- `default_value_schema_id` (Number) Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
- `disabled` (Boolean) Default: true
- `max_retries` (Number) Maximum number of times to try fetching schemas from the Schema Registry. Default: 1
- `request_timeout` (Number) Maximum time to wait for the Schema Registry to respond to a request. Default: 30000
- `schema_registry_url` (String) URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http. Default: "http://localhost:8081"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_confluent_cloud--kafka_schema_registry--tls))

<a id="nestedatt--output_confluent_cloud--kafka_schema_registry--auth"></a>
### Nested Schema for `output_confluent_cloud.kafka_schema_registry.auth`

Optional:

- `credentials_secret` (String) Select or create a secret that references your credentials
- `disabled` (Boolean) Default: true


<a id="nestedatt--output_confluent_cloud--kafka_schema_registry--tls"></a>
### Nested Schema for `output_confluent_cloud.kafka_schema_registry.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_confluent_cloud--pq_controls"></a>
### Nested Schema for `output_confluent_cloud.pq_controls`


<a id="nestedatt--output_confluent_cloud--sasl"></a>
### Nested Schema for `output_confluent_cloud.sasl`

Optional:

- `disabled` (Boolean) Default: true
- `mechanism` (String) Default: "plain"; must be one of ["plain", "scram-sha-256", "scram-sha-512", "kerberos"]


<a id="nestedatt--output_confluent_cloud--tls"></a>
### Nested Schema for `output_confluent_cloud.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: false
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_cribl_http"></a>
### Nested Schema for `output_cribl_http`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "cribl_http"

Optional:

- `compression` (String) Codec to use to compress the data before sending. Default: "gzip"; must be one of ["none", "gzip"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_fields` (List of String) Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported. Default: ["__kube_*","__metadata","__winEvent"]
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_cribl_http--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS. Default: true
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_http--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_cribl_http--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_http--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_http--tls))
- `token_ttl_minutes` (Number) The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60. Default: 60
- `url` (String) URL of a Cribl Worker to send events to, such as http://localhost:10200
- `urls` (Attributes List) (see [below for nested schema](#nestedatt--output_cribl_http--urls))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_cribl_http--extra_http_headers"></a>
### Nested Schema for `output_cribl_http.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_cribl_http--pq_controls"></a>
### Nested Schema for `output_cribl_http.pq_controls`


<a id="nestedatt--output_cribl_http--response_retry_settings"></a>
### Nested Schema for `output_cribl_http.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_cribl_http--timeout_retry_settings"></a>
### Nested Schema for `output_cribl_http.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_cribl_http--tls"></a>
### Nested Schema for `output_cribl_http.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.


<a id="nestedatt--output_cribl_http--urls"></a>
### Nested Schema for `output_cribl_http.urls`

Required:

- `url` (String) URL of a Cribl Worker to send events to, such as http://localhost:10200

Optional:

- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1



<a id="nestedatt--output_cribl_lake"></a>
### Nested Schema for `output_cribl_lake`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "cribl_lake"

Optional:

- `description` (String)
- `dest_path` (String) Lake dataset to send the data to.


<a id="nestedatt--output_cribl_tcp"></a>
### Nested Schema for `output_cribl_tcp`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "cribl_tcp"

Optional:

- `compression` (String) Codec to use to compress the data before sending. Default: "gzip"; must be one of ["none", "gzip"]
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_fields` (List of String) Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported. Default: ["__kube_*","__metadata","__winEvent"]
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `host` (String) The hostname of the receiver
- `hosts` (Attributes List) Set of hosts to load-balance data to (see [below for nested schema](#nestedatt--output_cribl_tcp--hosts))
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) Use load-balanced destinations. Default: true
- `log_failed_requests` (Boolean) Use to troubleshoot issues with sending data. Default: false
- `max_concurrent_senders` (Number) Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited. Default: 0
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) The port to connect to on the provided host. Default: 10300
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_tcp--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_cribl_tcp--tls))
- `token_ttl_minutes` (Number) The number of minutes before the internally generated authentication token expires, valid values between 1 and 60. Default: 60
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_cribl_tcp--hosts"></a>
### Nested Schema for `output_cribl_tcp.hosts`

Required:

- `host` (String) The hostname of the receiver

Optional:

- `port` (Number) The port to connect to on the provided host. Default: 10300
- `servername` (String) Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
- `tls` (String) Whether to inherit TLS configs from group setting or disable TLS. Default: "inherit"; must be one of ["inherit", "off"]
- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1


<a id="nestedatt--output_cribl_tcp--pq_controls"></a>
### Nested Schema for `output_cribl_tcp.pq_controls`


<a id="nestedatt--output_cribl_tcp--tls"></a>
### Nested Schema for `output_cribl_tcp.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_crowdstrike_next_gen_siem"></a>
### Nested Schema for `output_crowdstrike_next_gen_siem`

Required:

- `url` (String) URL provided from a CrowdStrike data connector. 
Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector

Optional:

- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_crowdstrike_next_gen_siem--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent. Default: "raw"; must be one of ["JSON", "raw"]
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_crowdstrike_next_gen_siem--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_crowdstrike_next_gen_siem--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_crowdstrike_next_gen_siem--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String)
- `type` (String) must be "crowdstrike_next_gen_siem"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: true

<a id="nestedatt--output_crowdstrike_next_gen_siem--extra_http_headers"></a>
### Nested Schema for `output_crowdstrike_next_gen_siem.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_crowdstrike_next_gen_siem--pq_controls"></a>
### Nested Schema for `output_crowdstrike_next_gen_siem.pq_controls`


<a id="nestedatt--output_crowdstrike_next_gen_siem--response_retry_settings"></a>
### Nested Schema for `output_crowdstrike_next_gen_siem.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_crowdstrike_next_gen_siem--timeout_retry_settings"></a>
### Nested Schema for `output_crowdstrike_next_gen_siem.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_datadog"></a>
### Nested Schema for `output_datadog`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "datadog"

Optional:

- `allow_api_key_from_events` (Boolean) Allow API key to be set from the event's '__agent_api_key' field. Default: false
- `api_key` (String) Organization's API key in Datadog
- `auth_type` (String) Enter API key directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret"]
- `batch_by_tags` (Boolean) Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance. Default: true
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `content_type` (String) The content type to use when sending logs. Default: "json"; must be one of ["text", "json"]
- `custom_url` (String)
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_datadog--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `host` (String) Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `message` (String) Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_datadog--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_datadog--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `send_counters_as_count` (Boolean) If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count). Default: false
- `service` (String) Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.
- `severity` (String) Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value. must be one of ["emergency", "alert", "critical", "error", "warning", "notice", "info", "debug"]
- `site` (String) Datadog site to which events should be sent. Default: "us"; must be one of ["us", "us3", "us5", "eu", "fed1", "ap1", "custom"]
- `source` (String) Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tags` (List of String) List of tags to send with logs, such as 'env:prod' and 'env_staging:east'. Default: []
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_datadog--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_datadog--extra_http_headers"></a>
### Nested Schema for `output_datadog.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_datadog--pq_controls"></a>
### Nested Schema for `output_datadog.pq_controls`


<a id="nestedatt--output_datadog--response_retry_settings"></a>
### Nested Schema for `output_datadog.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_datadog--timeout_retry_settings"></a>
### Nested Schema for `output_datadog.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_dataset"></a>
### Nested Schema for `output_dataset`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "dataset"

Optional:

- `api_key` (String) A 'Log Write Access' API key for the DataSet account
- `auth_type` (String) Enter API key directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `custom_url` (String)
- `default_severity` (String) Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value. Default: "info"; must be one of ["finest", "finer", "fine", "info", "warning", "error", "fatal"]
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_fields` (List of String) Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields. Default: ["sev","_time","ts","thread"]
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_dataset--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `message_field` (String) Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_dataset--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_dataset--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `server_host_field` (String) Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.
- `site` (String) DataSet site to which events should be sent. Default: "us"; must be one of ["us", "eu", "custom"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_dataset--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `timestamp_field` (String) Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_dataset--extra_http_headers"></a>
### Nested Schema for `output_dataset.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_dataset--pq_controls"></a>
### Nested Schema for `output_dataset.pq_controls`


<a id="nestedatt--output_dataset--response_retry_settings"></a>
### Nested Schema for `output_dataset.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_dataset--timeout_retry_settings"></a>
### Nested Schema for `output_dataset.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_default"></a>
### Nested Schema for `output_default`

Required:

- `default_id` (String) ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.
- `type` (String) must be "default"

Optional:

- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]


<a id="nestedatt--output_devnull"></a>
### Nested Schema for `output_devnull`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "devnull"

Optional:

- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]


<a id="nestedatt--output_disk_spool"></a>
### Nested Schema for `output_disk_spool`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "disk_spool"

Optional:

- `compress` (String) Data compression format. Default is gzip. Default: "gzip"; must be one of ["none", "gzip"]
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `max_data_size` (String) Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB. Default: "1GB"
- `max_data_time` (String) Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h. Default: "24h"
- `partition_expr` (String) JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `time_window` (String) Time period for grouping spooled events. Default is 10m. Default: "10m"


<a id="nestedatt--output_dl_s3"></a>
### Nested Schema for `output_dl_s3`

Required:

- `bucket` (String) Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `aws_api_key` (String) This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String) Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `compression_level` (String) Compression level to apply before moving files to final destination. Default: "best_speed"; must be one of ["best_speed", "normal", "best_compression"]
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `dest_path` (String) Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`. Default: ""
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_assume_role` (Boolean) Use Assume Role credentials to access S3. Default: false
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `endpoint` (String) S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_dl_s3--key_value_metadata))
- `kms_key_id` (String) ID or ARN of the KMS customer-managed key to use for encryption
- `max_closing_files_to_backpressure` (Number) Maximum number of files that can be waiting for upload before backpressure is applied. Default: 100
- `max_concurrent_file_parts` (Number) Maximum number of parts to upload in parallel per file. Minimum part size is 5MB. Default: 4
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `object_acl` (String) Object ACL to assign to uploaded objects. Default: "private"; must be one of ["private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `partitioning_fields` (List of String) List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>. Default: []
- `pipeline` (String) Pipeline to process data before sending out to this output
- `region` (String) Region where the S3 bucket is located
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `server_side_encryption` (String) must be one of ["AES256", "aws:kms"]
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `signature_version` (String) Signature version to use for signing S3 requests. Default: "v4"; must be one of ["v2", "v4"]
- `stage_path` (String) Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_class` (String) Storage class to select for uploaded objects. must be one of ["STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "dl_s3"
- `verify_permissions` (Boolean) Disable if you can access files within the bucket but not the bucket itself. Default: true
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_dl_s3--key_value_metadata"></a>
### Nested Schema for `output_dl_s3.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_dynatrace_http"></a>
### Nested Schema for `output_dynatrace_http`

Optional:

- `active_gate_domain` (String) ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.
- `auth_type` (String) Default: "token"; must be one of ["token", "textSecret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `endpoint` (String) Default: "cloud"; must be one of ["cloud", "activeGate", "manual"]
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `environment_id` (String) ID of the environment to send to
- `extra_http_headers` (Attributes List) Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields). (see [below for nested schema](#nestedatt--output_dynatrace_http--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) How to format events before sending. Defaults to JSON. Plaintext is not currently supported. Default: "json_array"; must be one of ["json_array", "plaintext"]
- `id` (String) Unique ID for this output
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `method` (String) The method to use when sending events. Default: "POST"; must be one of ["POST", "PUT", "PATCH"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_dynatrace_http--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_dynatrace_http--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `telemetry_type` (String) Default: "logs"; must be one of ["logs", "metrics"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_dynatrace_http--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) Bearer token to include in the authorization header
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `type` (String) must be "dynatrace_http"
- `url` (String) URL to send events to. Can be overwritten by an event's __url field.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_dynatrace_http--extra_http_headers"></a>
### Nested Schema for `output_dynatrace_http.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_dynatrace_http--pq_controls"></a>
### Nested Schema for `output_dynatrace_http.pq_controls`


<a id="nestedatt--output_dynatrace_http--response_retry_settings"></a>
### Nested Schema for `output_dynatrace_http.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_dynatrace_http--timeout_retry_settings"></a>
### Nested Schema for `output_dynatrace_http.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_dynatrace_otlp"></a>
### Nested Schema for `output_dynatrace_otlp`

Required:

- `token_secret` (String) Select or create a stored text secret

Optional:

- `auth_token_name` (String) Default: "Authorization"
- `compress` (String) Type of compression to apply to messages sent to the OpenTelemetry endpoint. Default: "gzip"; must be one of ["none", "deflate", "gzip"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `endpoint` (String) The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Default: "https://{your-environment-id}.live.dynatrace.com/api/v2/otlp"
- `endpoint_type` (String) Select the type of Dynatrace endpoint configured. Default: "saas"; must be one of ["saas", "ag"]
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_dynatrace_otlp--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `http_compress` (String) Type of compression to apply to messages sent to the OpenTelemetry endpoint. Default: "gzip"; must be one of ["none", "gzip"]
- `http_logs_endpoint_override` (String) If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `http_metrics_endpoint_override` (String) If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `http_traces_endpoint_override` (String) If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `id` (String) Unique ID for this output
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `keep_alive_time` (Number) How often the sender should ping the peer to keep the connection open. Default: 30
- `max_payload_size_kb` (Number) Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped. Default: 2048
- `metadata` (Attributes List) List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'. (see [below for nested schema](#nestedatt--output_dynatrace_otlp--metadata))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `otlp_version` (String) The version of OTLP Protobuf definitions to use when structuring data to send. Default: "1.3.1"; must be "1.3.1"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_dynatrace_otlp--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) Select a transport option for Dynatrace. Default: "http"; must be "http"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_dynatrace_otlp--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_dynatrace_otlp--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `type` (String) must be "dynatrace_otlp"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_dynatrace_otlp--extra_http_headers"></a>
### Nested Schema for `output_dynatrace_otlp.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_dynatrace_otlp--metadata"></a>
### Nested Schema for `output_dynatrace_otlp.metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""


<a id="nestedatt--output_dynatrace_otlp--pq_controls"></a>
### Nested Schema for `output_dynatrace_otlp.pq_controls`


<a id="nestedatt--output_dynatrace_otlp--response_retry_settings"></a>
### Nested Schema for `output_dynatrace_otlp.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_dynatrace_otlp--timeout_retry_settings"></a>
### Nested Schema for `output_dynatrace_otlp.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_elastic"></a>
### Nested Schema for `output_elastic`

Required:

- `index` (String) Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
- `type` (String) must be "elastic"

Optional:

- `auth` (Attributes) (see [below for nested schema](#nestedatt--output_elastic--auth))
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `doc_type` (String) Document type to use for events. Can be overwritten by an event's __type field.
- `elastic_pipeline` (String) Optional Elasticsearch destination pipeline
- `elastic_version` (String) Optional Elasticsearch version, used to format events. If not specified, will auto-discover version. Default: "auto"; must be one of ["auto", "6", "7"]
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_elastic--extra_http_headers))
- `extra_params` (Attributes List) (see [below for nested schema](#nestedatt--output_elastic--extra_params))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `include_doc_id` (Boolean) Include the `document_id` field when sending events to an Elastic TSDS (time series data stream). Default: false
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS. Default: true
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_elastic--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_elastic--response_retry_settings))
- `retry_partial_errors` (Boolean) Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch. Default: false
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_elastic--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `url` (String) The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk
- `urls` (Attributes List) (see [below for nested schema](#nestedatt--output_elastic--urls))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `write_action` (String) Action to use when writing events. Must be set to `Create` when writing to a data stream. Default: "create"; must be one of ["index", "create"]

<a id="nestedatt--output_elastic--auth"></a>
### Nested Schema for `output_elastic.auth`

Optional:

- `auth_type` (String) Enter credentials directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret", "manualAPIKey", "textSecret"]
- `disabled` (Boolean) Default: true


<a id="nestedatt--output_elastic--extra_http_headers"></a>
### Nested Schema for `output_elastic.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_elastic--extra_params"></a>
### Nested Schema for `output_elastic.extra_params`

Required:

- `name` (String)
- `value` (String)


<a id="nestedatt--output_elastic--pq_controls"></a>
### Nested Schema for `output_elastic.pq_controls`


<a id="nestedatt--output_elastic--response_retry_settings"></a>
### Nested Schema for `output_elastic.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_elastic--timeout_retry_settings"></a>
### Nested Schema for `output_elastic.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_elastic--urls"></a>
### Nested Schema for `output_elastic.urls`

Required:

- `url` (String) The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk

Optional:

- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1



<a id="nestedatt--output_elastic_cloud"></a>
### Nested Schema for `output_elastic_cloud`

Required:

- `index` (String) Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
- `url` (String) Enter Cloud ID of the Elastic Cloud environment to send events to

Optional:

- `auth` (Attributes) (see [below for nested schema](#nestedatt--output_elastic_cloud--auth))
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `elastic_pipeline` (String) Optional Elastic Cloud Destination pipeline
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_elastic_cloud--extra_http_headers))
- `extra_params` (Attributes List) Extra parameters to use in HTTP requests (see [below for nested schema](#nestedatt--output_elastic_cloud--extra_params))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `include_doc_id` (Boolean) Include the `document_id` field when sending events to an Elastic TSDS (time series data stream). Default: true
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_elastic_cloud--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_elastic_cloud--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_elastic_cloud--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `type` (String) must be "elastic_cloud"

<a id="nestedatt--output_elastic_cloud--auth"></a>
### Nested Schema for `output_elastic_cloud.auth`

Optional:

- `auth_type` (String) Enter credentials directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret", "manualAPIKey", "textSecret"]
- `disabled` (Boolean) Default: false


<a id="nestedatt--output_elastic_cloud--extra_http_headers"></a>
### Nested Schema for `output_elastic_cloud.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_elastic_cloud--extra_params"></a>
### Nested Schema for `output_elastic_cloud.extra_params`

Required:

- `name` (String)
- `value` (String)


<a id="nestedatt--output_elastic_cloud--pq_controls"></a>
### Nested Schema for `output_elastic_cloud.pq_controls`


<a id="nestedatt--output_elastic_cloud--response_retry_settings"></a>
### Nested Schema for `output_elastic_cloud.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_elastic_cloud--timeout_retry_settings"></a>
### Nested Schema for `output_elastic_cloud.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_exabeam"></a>
### Nested Schema for `output_exabeam`

Required:

- `bucket` (String) Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.
- `collector_instance_id` (String) ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888
- `region` (String) Region where the bucket is located

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `aws_api_key` (String) HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
- `aws_secret_key` (String) HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `encoded_configuration` (String) Enter an encoded string containing Exabeam configurations
- `endpoint` (String) Google Cloud Storage service endpoint. Default: "https://storage.googleapis.com"
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 10
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `object_acl` (String) Object ACL to assign to uploaded objects. Default: "private"; must be one of ["private", "bucket-owner-read", "bucket-owner-full-control", "project-private", "authenticated-read", "public-read"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `signature_version` (String) Signature version to use for signing Google Cloud Storage requests. Default: "v4"; must be one of ["v2", "v4"]
- `site_id` (String) Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.
- `site_name` (String) Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.
- `stage_path` (String) Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_class` (String) Storage class to select for uploaded objects. must be one of ["STANDARD", "NEARLINE", "COLDLINE", "ARCHIVE"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timezone_offset` (String)
- `type` (String) must be "exabeam"


<a id="nestedatt--output_filesystem"></a>
### Nested Schema for `output_filesystem`

Required:

- `dest_path` (String) Final destination for the output files
- `type` (String) must be "filesystem"

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `compression_level` (String) Compression level to apply before moving files to final destination. Default: "best_speed"; must be one of ["best_speed", "normal", "best_compression"]
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_filesystem--key_value_metadata))
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `partition_expr` (String) JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory. Default: "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `stage_path` (String) Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_filesystem--key_value_metadata"></a>
### Nested Schema for `output_filesystem.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_google_chronicle"></a>
### Nested Schema for `output_google_chronicle`

Required:

- `type` (String) must be "google_chronicle"

Optional:

- `api_key` (String) Organization's API key in Google SecOps
- `api_key_secret` (String) Select or create a stored text secret
- `api_version` (String) Default: "v1"; must be one of ["v1", "v2"]
- `authentication_method` (String) Default: "serviceAccount"; must be one of ["manual", "secret", "serviceAccount", "serviceAccountSecret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `custom_labels` (Attributes List) Custom labels to be added to every batch (see [below for nested schema](#nestedatt--output_google_chronicle--custom_labels))
- `customer_id` (String) Unique identifier (UUID) corresponding to a particular SecOps instance. Provided by your SecOps representative.
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_google_chronicle--extra_http_headers))
- `extra_log_types` (Attributes List) Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type. (see [below for nested schema](#nestedatt--output_google_chronicle--extra_log_types))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `log_format_type` (String) Default: "unstructured"; must be one of ["unstructured", "udm"]
- `log_text_field` (String) Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
- `log_type` (String) Default log type value to send to SecOps. Can be overwritten by event field __logType.
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 1024
- `namespace` (String) User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_google_chronicle--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `region` (String) Regional endpoint to send events to
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_google_chronicle--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `service_account_credentials` (String) Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
- `service_account_credentials_secret` (String) Select or create a stored text secret
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_google_chronicle--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 90
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. Default: false

<a id="nestedatt--output_google_chronicle--custom_labels"></a>
### Nested Schema for `output_google_chronicle.custom_labels`

Required:

- `key` (String)
- `value` (String)


<a id="nestedatt--output_google_chronicle--extra_http_headers"></a>
### Nested Schema for `output_google_chronicle.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_google_chronicle--extra_log_types"></a>
### Nested Schema for `output_google_chronicle.extra_log_types`

Required:

- `log_type` (String)

Optional:

- `description` (String)


<a id="nestedatt--output_google_chronicle--pq_controls"></a>
### Nested Schema for `output_google_chronicle.pq_controls`


<a id="nestedatt--output_google_chronicle--response_retry_settings"></a>
### Nested Schema for `output_google_chronicle.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_google_chronicle--timeout_retry_settings"></a>
### Nested Schema for `output_google_chronicle.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_google_cloud_logging"></a>
### Nested Schema for `output_google_cloud_logging`

Required:

- `log_location_expression` (String) JavaScript expression to compute the value of the folder ID with which log entries should be associated.
- `log_location_type` (String) must be one of ["project", "organization", "billingAccount", "folder"]
- `log_name_expression` (String) JavaScript expression to compute the value of the log name.

Optional:

- `cache_fill_bytes_expression` (String) A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `cache_hit_expression` (String) A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `cache_lookup_expression` (String) A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `cache_validated_expression` (String) A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_expression` (String) A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
- `first_expression` (String) A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size. Default: 1
- `function_expression` (String) A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
- `google_auth_method` (String) Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials. Default: "manual"; must be one of ["auto", "manual", "secret"]
- `id` (String) Unique ID for this output
- `id_expression` (String) A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
- `index_expression` (String) A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
- `insert_id_expression` (String) JavaScript expression to compute the value of the insert ID field.
- `last_expression` (String) A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
- `latency_expression` (String) A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `line_expression` (String) A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
- `log_labels` (Attributes List) Labels to apply to the log entry (see [below for nested schema](#nestedatt--output_google_cloud_logging--log_labels))
- `max_payload_events` (Number) Max number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `payload_expression` (String) JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.
- `payload_format` (String) Format to use when sending payload. Defaults to Text. Default: "text"; must be one of ["text", "json"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_google_cloud_logging--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `producer_expression` (String) A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
- `protocol_expression` (String) A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `referer_expression` (String) A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `remote_ip_expression` (String) A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `request_method_expression` (String) A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `request_size_expression` (String) A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `request_url_expression` (String) A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `resource_type_expression` (String) JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".
- `resource_type_labels` (Attributes List) Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging. (see [below for nested schema](#nestedatt--output_google_cloud_logging--resource_type_labels))
- `response_size_expression` (String) A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `secret` (String) Select or create a stored text secret
- `server_ip_expression` (String) A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `service_account_credentials` (String) Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
- `severity_expression` (String) JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".
- `span_id_expression` (String) A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
- `status_expression` (String) A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `throttle_rate_req_per_sec` (Number) Maximum number of requests to limit to per second.
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `total_splits_expression` (String) A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
- `trace_expression` (String) A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
- `trace_sampled_expression` (String) A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
- `type` (String) must be "google_cloud_logging"
- `uid_expression` (String) A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
- `user_agent_expression` (String) A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.

<a id="nestedatt--output_google_cloud_logging--log_labels"></a>
### Nested Schema for `output_google_cloud_logging.log_labels`

Required:

- `label` (String) Label name
- `value_expression` (String) JavaScript expression to compute the label's value.


<a id="nestedatt--output_google_cloud_logging--pq_controls"></a>
### Nested Schema for `output_google_cloud_logging.pq_controls`


<a id="nestedatt--output_google_cloud_logging--resource_type_labels"></a>
### Nested Schema for `output_google_cloud_logging.resource_type_labels`

Required:

- `label` (String) Label name
- `value_expression` (String) JavaScript expression to compute the label's value.



<a id="nestedatt--output_google_cloud_storage"></a>
### Nested Schema for `output_google_cloud_storage`

Required:

- `bucket` (String) Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.
- `region` (String) Region where the bucket is located

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `aws_api_key` (String) HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
- `aws_authentication_method` (String) Default: "manual"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String) HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `compression_level` (String) Compression level to apply before moving files to final destination. Default: "best_speed"; must be one of ["best_speed", "normal", "best_compression"]
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `dest_path` (String) Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`. Default: ""
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `endpoint` (String) Google Cloud Storage service endpoint. Default: "https://storage.googleapis.com"
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_google_cloud_storage--key_value_metadata))
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `object_acl` (String) Object ACL to assign to uploaded objects. Default: "private"; must be one of ["private", "bucket-owner-read", "bucket-owner-full-control", "project-private", "authenticated-read", "public-read"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `partition_expr` (String) JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory. Default: "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `signature_version` (String) Signature version to use for signing Google Cloud Storage requests. Default: "v4"; must be one of ["v2", "v4"]
- `stage_path` (String) Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_class` (String) Storage class to select for uploaded objects. must be one of ["STANDARD", "NEARLINE", "COLDLINE", "ARCHIVE"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "google_cloud_storage"
- `verify_permissions` (Boolean) Disable if you can access files within the bucket but not the bucket itself. Default: true
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_google_cloud_storage--key_value_metadata"></a>
### Nested Schema for `output_google_cloud_storage.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_google_pubsub"></a>
### Nested Schema for `output_google_pubsub`

Required:

- `topic_name` (String) ID of the topic to send events to.
- `type` (String) must be "google_pubsub"

Optional:

- `batch_size` (Number) The maximum number of items the Google API should batch before it sends them to the topic. Default: 1000
- `batch_timeout` (Number) The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached). Default: 100
- `create_topic` (Boolean) If enabled, create topic if it does not exist. Default: false
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) Maximum time to wait before sending a batch (when batch size limit is not reached). Default: 1
- `google_auth_method` (String) Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials. Default: "manual"; must be one of ["auto", "manual", "secret"]
- `id` (String) Unique ID for this output
- `max_in_progress` (Number) The maximum number of in-progress API requests before backpressure is applied. Default: 10
- `max_queue_size` (Number) Maximum number of queued batches before blocking. Default: 100
- `max_record_size_kb` (Number) Maximum size (KB) of batches to send. Default: 256
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `ordered_delivery` (Boolean) If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled. Default: false
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_google_pubsub--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `region` (String) Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
- `secret` (String) Select or create a stored text secret
- `service_account_credentials` (String) Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]

<a id="nestedatt--output_google_pubsub--pq_controls"></a>
### Nested Schema for `output_google_pubsub.pq_controls`



<a id="nestedatt--output_grafana_cloud"></a>
### Nested Schema for `output_grafana_cloud`

Optional:

- `output_grafana_cloud_grafana_cloud1` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1))
- `output_grafana_cloud_grafana_cloud2` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2))

<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1`

Required:

- `id` (String) Unique ID for this output
- `loki_url` (String) The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net
- `type` (String) must be "grafana_cloud"

Optional:

- `compress` (Boolean) Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 1
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 15
- `labels` (Attributes List) List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the events __labels field. Example: "__labels: {host: "cribl.io", level: "error"}" (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--labels))
- `loki_auth` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--loki_auth))
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 4096
- `message` (String) Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
- `message_format` (String) Format to use when sending logs to Loki (Protobuf or JSON). Default: "protobuf"; must be one of ["protobuf", "json"]
- `metric_rename_expr` (String) JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>. Default: "name.replace(/[^a-zA-Z0-9_]/g, '_')"
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `prometheus_auth` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--prometheus_auth))
- `prometheus_url` (String) The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively. Default: ["cribl_host","cribl_wp"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--extra_http_headers"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--labels"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.labels`

Required:

- `value` (String)

Optional:

- `name` (String) Default: ""


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--loki_auth"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.loki_auth`

Optional:

- `auth_type` (String) Default: "basic"; must be one of ["none", "token", "textSecret", "basic", "credentialsSecret"]
- `credentials_secret` (String) Select or create a secret that references your credentials
- `password` (String) Password (API key in Grafana Cloud domain) for authentication
- `text_secret` (String) Select or create a stored text secret
- `token` (String) Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
- `username` (String) Username for authentication


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--pq_controls"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.pq_controls`


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--prometheus_auth"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.prometheus_auth`

Optional:

- `auth_type` (String) Default: "basic"; must be one of ["none", "token", "textSecret", "basic", "credentialsSecret"]
- `credentials_secret` (String) Select or create a secret that references your credentials
- `password` (String) Password (API key in Grafana Cloud domain) for authentication
- `text_secret` (String) Select or create a stored text secret
- `token` (String) Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
- `username` (String) Username for authentication


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--response_retry_settings"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud1--timeout_retry_settings"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud1.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2`

Required:

- `id` (String) Unique ID for this output
- `prometheus_url` (String) The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
- `type` (String) must be "grafana_cloud"

Optional:

- `compress` (Boolean) Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 1
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 15
- `labels` (Attributes List) List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the events __labels field. Example: "__labels: {host: "cribl.io", level: "error"}" (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--labels))
- `loki_auth` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--loki_auth))
- `loki_url` (String) The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order. Default: 4096
- `message` (String) Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
- `message_format` (String) Format to use when sending logs to Loki (Protobuf or JSON). Default: "protobuf"; must be one of ["protobuf", "json"]
- `metric_rename_expr` (String) JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>. Default: "name.replace(/[^a-zA-Z0-9_]/g, '_')"
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `prometheus_auth` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--prometheus_auth))
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively. Default: ["cribl_host","cribl_wp"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--extra_http_headers"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--labels"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.labels`

Required:

- `value` (String)

Optional:

- `name` (String) Default: ""


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--loki_auth"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.loki_auth`

Optional:

- `auth_type` (String) Default: "basic"; must be one of ["none", "token", "textSecret", "basic", "credentialsSecret"]
- `credentials_secret` (String) Select or create a secret that references your credentials
- `password` (String) Password (API key in Grafana Cloud domain) for authentication
- `text_secret` (String) Select or create a stored text secret
- `token` (String) Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
- `username` (String) Username for authentication


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--pq_controls"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.pq_controls`


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--prometheus_auth"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.prometheus_auth`

Optional:

- `auth_type` (String) Default: "basic"; must be one of ["none", "token", "textSecret", "basic", "credentialsSecret"]
- `credentials_secret` (String) Select or create a secret that references your credentials
- `password` (String) Password (API key in Grafana Cloud domain) for authentication
- `text_secret` (String) Select or create a stored text secret
- `token` (String) Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
- `username` (String) Username for authentication


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--response_retry_settings"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_grafana_cloud--output_grafana_cloud_grafana_cloud2--timeout_retry_settings"></a>
### Nested Schema for `output_grafana_cloud.output_grafana_cloud_grafana_cloud2.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false




<a id="nestedatt--output_graphite"></a>
### Nested Schema for `output_graphite`

Required:

- `host` (String) The hostname of the destination.

Optional:

- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `dns_resolve_period_sec` (Number) How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup. Default: 0
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination. Default: 1
- `id` (String) Unique ID for this output
- `mtu` (Number) When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system. Default: 512
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) Destination port. Default: 8125
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_graphite--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) Protocol to use when communicating with the destination. Default: "udp"; must be one of ["udp", "tcp"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) must be "graphite"
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_graphite--pq_controls"></a>
### Nested Schema for `output_graphite.pq_controls`



<a id="nestedatt--output_honeycomb"></a>
### Nested Schema for `output_honeycomb`

Required:

- `dataset` (String) Name of the dataset to send events to – e.g., observability
- `type` (String) must be "honeycomb"

Optional:

- `auth_type` (String) Enter API key directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_honeycomb--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_honeycomb--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_honeycomb--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `team` (String) Team API key where the dataset belongs
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_honeycomb--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_honeycomb--extra_http_headers"></a>
### Nested Schema for `output_honeycomb.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_honeycomb--pq_controls"></a>
### Nested Schema for `output_honeycomb.pq_controls`


<a id="nestedatt--output_honeycomb--response_retry_settings"></a>
### Nested Schema for `output_honeycomb.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_honeycomb--timeout_retry_settings"></a>
### Nested Schema for `output_honeycomb.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_humio_hec"></a>
### Nested Schema for `output_humio_hec`

Optional:

- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_humio_hec--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent. Default: "JSON"; must be one of ["JSON", "raw"]
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_humio_hec--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_humio_hec--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_humio_hec--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) CrowdStrike Falcon LogScale authentication token
- `type` (String) must be "humio_hec"
- `url` (String) URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw. Default: "https://cloud.us.humio.com/api/v1/ingest/hec"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: true

<a id="nestedatt--output_humio_hec--extra_http_headers"></a>
### Nested Schema for `output_humio_hec.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_humio_hec--pq_controls"></a>
### Nested Schema for `output_humio_hec.pq_controls`


<a id="nestedatt--output_humio_hec--response_retry_settings"></a>
### Nested Schema for `output_humio_hec.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_humio_hec--timeout_retry_settings"></a>
### Nested Schema for `output_humio_hec.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_influxdb"></a>
### Nested Schema for `output_influxdb`

Required:

- `type` (String) must be "influxdb"
- `url` (String) URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write

Optional:

- `auth_header_expr` (String) JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`. Default: "`Bearer ${token}`"
- `auth_type` (String) InfluxDB authentication type. Default: "none"; must be one of ["none", "basic", "credentialsSecret", "token", "textSecret", "oauth"]
- `bucket` (String) Bucket to write to.
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `credentials_secret` (String) Select or create a secret that references your credentials
- `database` (String) Database to write to.
- `description` (String)
- `dynamic_value_field_name` (Boolean) Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field. Default: true
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_influxdb--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `login_url` (String) URL for OAuth
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `oauth_headers` (Attributes List) Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_influxdb--oauth_headers))
- `oauth_params` (Attributes List) Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_influxdb--oauth_params))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `org` (String) Organization ID for this bucket.
- `password` (String)
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_influxdb--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_influxdb--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `secret` (String) Secret parameter value to pass in request body
- `secret_param_name` (String) Secret parameter name to pass in request body
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_influxdb--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `timestamp_precision` (String) Sets the precision for the supplied Unix time values. Defaults to milliseconds. Default: "ms"; must be one of ["ns", "u", "ms", "s", "m", "h"]
- `token` (String) Bearer token to include in the authorization header
- `token_attribute_name` (String) Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
- `token_timeout_secs` (Number) How often the OAuth token should be refreshed. Default: 3600
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `use_v2_api` (Boolean) The v2 API can be enabled with InfluxDB versions 1.8 and later. Default: false
- `username` (String)
- `value_field_name` (String) Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback. Default: "value"

<a id="nestedatt--output_influxdb--extra_http_headers"></a>
### Nested Schema for `output_influxdb.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_influxdb--oauth_headers"></a>
### Nested Schema for `output_influxdb.oauth_headers`

Required:

- `name` (String) OAuth header name
- `value` (String) OAuth header value


<a id="nestedatt--output_influxdb--oauth_params"></a>
### Nested Schema for `output_influxdb.oauth_params`

Required:

- `name` (String) OAuth parameter name
- `value` (String) OAuth parameter value


<a id="nestedatt--output_influxdb--pq_controls"></a>
### Nested Schema for `output_influxdb.pq_controls`


<a id="nestedatt--output_influxdb--response_retry_settings"></a>
### Nested Schema for `output_influxdb.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_influxdb--timeout_retry_settings"></a>
### Nested Schema for `output_influxdb.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_kafka"></a>
### Nested Schema for `output_kafka`

Required:

- `brokers` (List of String) Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
- `topic` (String) The topic to publish events to. Can be overridden using the __topicOut field.

Optional:

- `ack` (Number) Control the number of required acknowledgments. Default: 1; must be one of ["1", "0", "-1"]
- `authentication_timeout` (Number) Maximum time to wait for Kafka to respond to an authentication request. Default: 10000
- `backoff_rate` (Number) Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details. Default: 2
- `compression` (String) Codec to use to compress the data before sending to Kafka. Default: "gzip"; must be one of ["none", "gzip", "snappy", "lz4"]
- `connection_timeout` (Number) Maximum time to wait for a connection to complete successfully. Default: 10000
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_event_count` (Number) The maximum number of events you want the Destination to allow in a batch before forcing a flush. Default: 1000
- `flush_period_sec` (Number) The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent. Default: 1
- `format` (String) Format to use to serialize events before writing to Kafka. Default: "json"; must be one of ["json", "raw", "protobuf"]
- `id` (String) Unique ID for this output
- `initial_backoff` (Number) Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes). Default: 300
- `kafka_schema_registry` (Attributes) (see [below for nested schema](#nestedatt--output_kafka--kafka_schema_registry))
- `max_back_off` (Number) The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds). Default: 30000
- `max_record_size_kb` (Number) Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting. Default: 768
- `max_retries` (Number) If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data. Default: 5
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_kafka--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protobuf_library_id` (String) Select a set of Protobuf definitions for the events you want to send
- `reauthentication_threshold` (Number) Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire. Default: 10000
- `request_timeout` (Number) Maximum time to wait for Kafka to respond to a request. Default: 60000
- `sasl` (Attributes) Authentication parameters to use when connecting to brokers. Using TLS is highly recommended. (see [below for nested schema](#nestedatt--output_kafka--sasl))
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_kafka--tls))
- `type` (String) must be "kafka"

<a id="nestedatt--output_kafka--kafka_schema_registry"></a>
### Nested Schema for `output_kafka.kafka_schema_registry`

Optional:

- `auth` (Attributes) Credentials to use when authenticating with the schema registry using basic HTTP authentication (see [below for nested schema](#nestedatt--output_kafka--kafka_schema_registry--auth))
- `connection_timeout` (Number) Maximum time to wait for a Schema Registry connection to complete successfully. Default: 30000
- `default_key_schema_id` (Number) Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
- `default_value_schema_id` (Number) Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
- `disabled` (Boolean) Default: true
- `max_retries` (Number) Maximum number of times to try fetching schemas from the Schema Registry. Default: 1
- `request_timeout` (Number) Maximum time to wait for the Schema Registry to respond to a request. Default: 30000
- `schema_registry_url` (String) URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http. Default: "http://localhost:8081"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_kafka--kafka_schema_registry--tls))

<a id="nestedatt--output_kafka--kafka_schema_registry--auth"></a>
### Nested Schema for `output_kafka.kafka_schema_registry.auth`

Optional:

- `credentials_secret` (String) Select or create a secret that references your credentials
- `disabled` (Boolean) Default: true


<a id="nestedatt--output_kafka--kafka_schema_registry--tls"></a>
### Nested Schema for `output_kafka.kafka_schema_registry.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_kafka--pq_controls"></a>
### Nested Schema for `output_kafka.pq_controls`


<a id="nestedatt--output_kafka--sasl"></a>
### Nested Schema for `output_kafka.sasl`

Optional:

- `disabled` (Boolean) Default: true
- `mechanism` (String) Default: "plain"; must be one of ["plain", "scram-sha-256", "scram-sha-512", "kerberos"]


<a id="nestedatt--output_kafka--tls"></a>
### Nested Schema for `output_kafka.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another


                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_kinesis"></a>
### Nested Schema for `output_kinesis`

Required:

- `region` (String) Region where the Kinesis stream is located
- `stream_name` (String) Kinesis stream name to send events to.

Optional:

- `as_ndjson` (Boolean) Batch events into a single record as NDJSON. Default: true
- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `aws_api_key` (String)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String)
- `compression` (String) Compression type to use for records. Default: "gzip"; must be one of ["none", "gzip"]
- `concurrency` (Number) Maximum number of ongoing put requests before blocking. Default: 5
- `description` (String)
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `enable_assume_role` (Boolean) Use Assume Role credentials to access Kinesis stream. Default: false
- `endpoint` (String) Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size. Default: 1
- `id` (String) Unique ID for this output
- `max_record_size_kb` (Number) Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size. Default: 1024
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_kinesis--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `signature_version` (String) Signature version to use for signing Kinesis stream requests. Default: "v4"; must be one of ["v2", "v4"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "kinesis"
- `use_list_shards` (Boolean) Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details. Default: false

<a id="nestedatt--output_kinesis--pq_controls"></a>
### Nested Schema for `output_kinesis.pq_controls`



<a id="nestedatt--output_loki"></a>
### Nested Schema for `output_loki`

Required:

- `type` (String) must be "loki"
- `url` (String) The endpoint to send logs to

Optional:

- `auth_type` (String) Default: "none"; must be one of ["none", "token", "textSecret", "basic", "credentialsSecret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order. Default: 1
- `credentials_secret` (String) Select or create a secret that references your credentials
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_loki--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order. Default: 15
- `id` (String) Unique ID for this output
- `labels` (Attributes List) List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the events __labels field. Example: "__labels: {host: "cribl.io", level: "error"}" (see [below for nested schema](#nestedatt--output_loki--labels))
- `max_payload_events` (Number) Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order. Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order. Default: 4096
- `message` (String) Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
- `message_format` (String) Format to use when sending logs to Loki (Protobuf or JSON). Default: "protobuf"; must be one of ["protobuf", "json"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `password` (String) Password (API key in Grafana Cloud domain) for authentication
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_loki--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_loki--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs. Default: ["cribl_host","cribl_wp"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_loki--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `username` (String) Username for authentication

<a id="nestedatt--output_loki--extra_http_headers"></a>
### Nested Schema for `output_loki.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_loki--labels"></a>
### Nested Schema for `output_loki.labels`

Required:

- `value` (String)

Optional:

- `name` (String) Default: ""


<a id="nestedatt--output_loki--pq_controls"></a>
### Nested Schema for `output_loki.pq_controls`


<a id="nestedatt--output_loki--response_retry_settings"></a>
### Nested Schema for `output_loki.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_loki--timeout_retry_settings"></a>
### Nested Schema for `output_loki.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_minio"></a>
### Nested Schema for `output_minio`

Required:

- `bucket` (String) Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
- `endpoint` (String) MinIO service url (e.g. http://minioHost:9000)

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `aws_api_key` (String) This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String) Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `compression_level` (String) Compression level to apply before moving files to final destination. Default: "best_speed"; must be one of ["best_speed", "normal", "best_compression"]
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `dest_path` (String) Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_minio--key_value_metadata))
- `max_concurrent_file_parts` (Number) Maximum number of parts to upload in parallel per file. Minimum part size is 5MB. Default: 4
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `object_acl` (String) Object ACL to assign to uploaded objects. Default: "private"; must be one of ["private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `partition_expr` (String) JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory. Default: "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `region` (String) Region where the MinIO service/cluster is located
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates). Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `server_side_encryption` (String) Server-side encryption for uploaded objects. must be "AES256"
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `signature_version` (String) Signature version to use for signing MinIO requests. Default: "v4"; must be one of ["v2", "v4"]
- `stage_path` (String) Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_class` (String) Storage class to select for uploaded objects. must be one of ["STANDARD", "REDUCED_REDUNDANCY"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "minio"
- `verify_permissions` (Boolean) Disable if you can access files within the bucket but not the bucket itself. Default: true
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_minio--key_value_metadata"></a>
### Nested Schema for `output_minio.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_msk"></a>
### Nested Schema for `output_msk`

Required:

- `brokers` (List of String) Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
- `region` (String) Region where the MSK cluster is located
- `topic` (String) The topic to publish events to. Can be overridden using the __topicOut field.

Optional:

- `ack` (Number) Control the number of required acknowledgments. Default: 1; must be one of ["1", "0", "-1"]
- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `authentication_timeout` (Number) Maximum time to wait for Kafka to respond to an authentication request. Default: 10000
- `aws_api_key` (String)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String)
- `backoff_rate` (Number) Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details. Default: 2
- `compression` (String) Codec to use to compress the data before sending to Kafka. Default: "gzip"; must be one of ["none", "gzip", "snappy", "lz4"]
- `connection_timeout` (Number) Maximum time to wait for a connection to complete successfully. Default: 10000
- `description` (String)
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `enable_assume_role` (Boolean) Use Assume Role credentials to access MSK. Default: false
- `endpoint` (String) MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_event_count` (Number) The maximum number of events you want the Destination to allow in a batch before forcing a flush. Default: 1000
- `flush_period_sec` (Number) The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent. Default: 1
- `format` (String) Format to use to serialize events before writing to Kafka. Default: "json"; must be one of ["json", "raw", "protobuf"]
- `id` (String) Unique ID for this output
- `initial_backoff` (Number) Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes). Default: 300
- `kafka_schema_registry` (Attributes) (see [below for nested schema](#nestedatt--output_msk--kafka_schema_registry))
- `max_back_off` (Number) The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds). Default: 30000
- `max_record_size_kb` (Number) Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting. Default: 768
- `max_retries` (Number) If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data. Default: 5
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_msk--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protobuf_library_id` (String) Select a set of Protobuf definitions for the events you want to send
- `reauthentication_threshold` (Number) Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire. Default: 10000
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `request_timeout` (Number) Maximum time to wait for Kafka to respond to a request. Default: 60000
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `signature_version` (String) Signature version to use for signing MSK cluster requests. Default: "v4"; must be one of ["v2", "v4"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_msk--tls))
- `type` (String) must be "msk"

<a id="nestedatt--output_msk--kafka_schema_registry"></a>
### Nested Schema for `output_msk.kafka_schema_registry`

Optional:

- `auth` (Attributes) Credentials to use when authenticating with the schema registry using basic HTTP authentication (see [below for nested schema](#nestedatt--output_msk--kafka_schema_registry--auth))
- `connection_timeout` (Number) Maximum time to wait for a Schema Registry connection to complete successfully. Default: 30000
- `default_key_schema_id` (Number) Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
- `default_value_schema_id` (Number) Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
- `disabled` (Boolean) Default: true
- `max_retries` (Number) Maximum number of times to try fetching schemas from the Schema Registry. Default: 1
- `request_timeout` (Number) Maximum time to wait for the Schema Registry to respond to a request. Default: 30000
- `schema_registry_url` (String) URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http. Default: "http://localhost:8081"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_msk--kafka_schema_registry--tls))

<a id="nestedatt--output_msk--kafka_schema_registry--auth"></a>
### Nested Schema for `output_msk.kafka_schema_registry.auth`

Optional:

- `credentials_secret` (String) Select or create a secret that references your credentials
- `disabled` (Boolean) Default: true


<a id="nestedatt--output_msk--kafka_schema_registry--tls"></a>
### Nested Schema for `output_msk.kafka_schema_registry.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_msk--pq_controls"></a>
### Nested Schema for `output_msk.pq_controls`


<a id="nestedatt--output_msk--tls"></a>
### Nested Schema for `output_msk.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: false
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another


                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_netflow"></a>
### Nested Schema for `output_netflow`

Required:

- `hosts` (Attributes List) One or more NetFlow destinations to forward events to (see [below for nested schema](#nestedatt--output_netflow--hosts))
- `type` (String) must be "netflow"

Optional:

- `description` (String)
- `dns_resolve_period_sec` (Number) How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup. Default: 0
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]

<a id="nestedatt--output_netflow--hosts"></a>
### Nested Schema for `output_netflow.hosts`

Required:

- `host` (String) Destination host

Optional:

- `port` (Number) Destination port, default is 2055. Default: 2055



<a id="nestedatt--output_newrelic"></a>
### Nested Schema for `output_newrelic`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "newrelic"

Optional:

- `api_key` (String) New Relic API key. Can be overridden using __newRelic_apiKey field.
- `auth_type` (String) Enter API key directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `custom_url` (String)
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_newrelic--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `log_type` (String) Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value. Default: ""
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 1024
- `message_field` (String) Name of field to send as log message value. If not present, event will be serialized and sent as JSON. Default: ""
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--output_newrelic--metadata))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_newrelic--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `region` (String) Which New Relic region endpoint to use. Default: "US"; must be one of ["US", "EU", "Custom"]
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_newrelic--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_newrelic--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_newrelic--extra_http_headers"></a>
### Nested Schema for `output_newrelic.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_newrelic--metadata"></a>
### Nested Schema for `output_newrelic.metadata`

Required:

- `name` (String) must be one of ["service", "hostname", "timestamp", "auditId"]
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--output_newrelic--pq_controls"></a>
### Nested Schema for `output_newrelic.pq_controls`


<a id="nestedatt--output_newrelic--response_retry_settings"></a>
### Nested Schema for `output_newrelic.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_newrelic--timeout_retry_settings"></a>
### Nested Schema for `output_newrelic.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_newrelic_events"></a>
### Nested Schema for `output_newrelic_events`

Required:

- `account_id` (String) New Relic account ID
- `event_type` (String) Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).

Optional:

- `api_key` (String) New Relic API key. Can be overridden using __newRelic_apiKey field.
- `auth_type` (String) Enter API key directly, or select a stored secret. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `custom_url` (String)
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_newrelic_events--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 1024
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_newrelic_events--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `region` (String) Which New Relic region endpoint to use. Default: "US"; must be one of ["US", "EU", "Custom"]
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_newrelic_events--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_newrelic_events--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `type` (String) must be "newrelic_events"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_newrelic_events--extra_http_headers"></a>
### Nested Schema for `output_newrelic_events.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_newrelic_events--pq_controls"></a>
### Nested Schema for `output_newrelic_events.pq_controls`


<a id="nestedatt--output_newrelic_events--response_retry_settings"></a>
### Nested Schema for `output_newrelic_events.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_newrelic_events--timeout_retry_settings"></a>
### Nested Schema for `output_newrelic_events.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_open_telemetry"></a>
### Nested Schema for `output_open_telemetry`

Required:

- `endpoint` (String) The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.
- `type` (String) must be "open_telemetry"

Optional:

- `auth_header_expr` (String) JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`. Default: "`Bearer ${token}`"
- `auth_type` (String) OpenTelemetry authentication type. Default: "none"; must be one of ["none", "basic", "credentialsSecret", "token", "textSecret", "oauth"]
- `compress` (String) Type of compression to apply to messages sent to the OpenTelemetry endpoint. Default: "gzip"; must be one of ["none", "deflate", "gzip"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `credentials_secret` (String) Select or create a secret that references your credentials
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_open_telemetry--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `http_compress` (String) Type of compression to apply to messages sent to the OpenTelemetry endpoint. Default: "gzip"; must be one of ["none", "gzip"]
- `http_logs_endpoint_override` (String) If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `http_metrics_endpoint_override` (String) If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `http_traces_endpoint_override` (String) If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `id` (String) Unique ID for this output
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `keep_alive_time` (Number) How often the sender should ping the peer to keep the connection open. Default: 30
- `login_url` (String) URL for OAuth
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `metadata` (Attributes List) List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'. (see [below for nested schema](#nestedatt--output_open_telemetry--metadata))
- `oauth_headers` (Attributes List) Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_open_telemetry--oauth_headers))
- `oauth_params` (Attributes List) Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_open_telemetry--oauth_params))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `otlp_version` (String) The version of OTLP Protobuf definitions to use when structuring data to send. Default: "0.10.0"; must be one of ["0.10.0", "1.3.1"]
- `password` (String)
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_open_telemetry--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) Select a transport option for OpenTelemetry. Default: "grpc"; must be one of ["grpc", "http"]
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).


        Enabled by default. When this setting is also present in TLS Settings (Client Side),
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_open_telemetry--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `secret` (String) Secret parameter value to pass in request body
- `secret_param_name` (String) Secret parameter name to pass in request body
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_open_telemetry--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_open_telemetry--tls))
- `token` (String) Bearer token to include in the authorization header
- `token_attribute_name` (String) Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
- `token_timeout_secs` (Number) How often the OAuth token should be refreshed. Default: 3600
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `username` (String)

<a id="nestedatt--output_open_telemetry--extra_http_headers"></a>
### Nested Schema for `output_open_telemetry.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_open_telemetry--metadata"></a>
### Nested Schema for `output_open_telemetry.metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""


<a id="nestedatt--output_open_telemetry--oauth_headers"></a>
### Nested Schema for `output_open_telemetry.oauth_headers`

Required:

- `name` (String) OAuth header name
- `value` (String) OAuth header value


<a id="nestedatt--output_open_telemetry--oauth_params"></a>
### Nested Schema for `output_open_telemetry.oauth_params`

Required:

- `name` (String) OAuth parameter name
- `value` (String) OAuth parameter value


<a id="nestedatt--output_open_telemetry--pq_controls"></a>
### Nested Schema for `output_open_telemetry.pq_controls`


<a id="nestedatt--output_open_telemetry--response_retry_settings"></a>
### Nested Schema for `output_open_telemetry.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_open_telemetry--timeout_retry_settings"></a>
### Nested Schema for `output_open_telemetry.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_open_telemetry--tls"></a>
### Nested Schema for `output_open_telemetry.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another


                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true



<a id="nestedatt--output_prometheus"></a>
### Nested Schema for `output_prometheus`

Required:

- `type` (String) must be "prometheus"
- `url` (String) The endpoint to send metrics to

Optional:

- `auth_header_expr` (String) JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`. Default: "`Bearer ${token}`"
- `auth_type` (String) Remote Write authentication type. Default: "none"; must be one of ["none", "basic", "credentialsSecret", "token", "textSecret", "oauth"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `credentials_secret` (String) Select or create a secret that references your credentials
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_prometheus--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `login_url` (String) URL for OAuth
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `metric_rename_expr` (String) JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>. Default: "name.replace(/[^a-zA-Z0-9_]/g, '_')"
- `metrics_flush_period_sec` (Number) How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above. Default: 60
- `oauth_headers` (Attributes List) Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_prometheus--oauth_headers))
- `oauth_params` (Attributes List) Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_prometheus--oauth_params))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `password` (String)
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_prometheus--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_prometheus--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `secret` (String) Secret parameter value to pass in request body
- `secret_param_name` (String) Secret parameter name to pass in request body
- `send_metadata` (Boolean) Generate and send metadata (`type` and `metricFamilyName`) requests. Default: true
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics. Default: ["cribl_host","cribl_wp"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_prometheus--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) Bearer token to include in the authorization header
- `token_attribute_name` (String) Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
- `token_timeout_secs` (Number) How often the OAuth token should be refreshed. Default: 3600
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `username` (String)

<a id="nestedatt--output_prometheus--extra_http_headers"></a>
### Nested Schema for `output_prometheus.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_prometheus--oauth_headers"></a>
### Nested Schema for `output_prometheus.oauth_headers`

Required:

- `name` (String) OAuth header name
- `value` (String) OAuth header value


<a id="nestedatt--output_prometheus--oauth_params"></a>
### Nested Schema for `output_prometheus.oauth_params`

Required:

- `name` (String) OAuth parameter name
- `value` (String) OAuth parameter value


<a id="nestedatt--output_prometheus--pq_controls"></a>
### Nested Schema for `output_prometheus.pq_controls`


<a id="nestedatt--output_prometheus--response_retry_settings"></a>
### Nested Schema for `output_prometheus.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_prometheus--timeout_retry_settings"></a>
### Nested Schema for `output_prometheus.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_ring"></a>
### Nested Schema for `output_ring`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "ring"

Optional:

- `compress` (String) Default: "gzip"; must be one of ["none", "gzip"]
- `description` (String)
- `dest_path` (String) Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw"]
- `max_data_size` (String) Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted. Default: "1GB"
- `max_data_time` (String) Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted. Default: "24h"
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `partition_expr` (String) JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]


<a id="nestedatt--output_router"></a>
### Nested Schema for `output_router`

Required:

- `rules` (Attributes List) Event routing rules (see [below for nested schema](#nestedatt--output_router--rules))
- `type` (String) must be "router"

Optional:

- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]

<a id="nestedatt--output_router--rules"></a>
### Nested Schema for `output_router.rules`

Required:

- `filter` (String) JavaScript expression to select events to send to output
- `output` (String) Output to send matching events to

Optional:

- `description` (String) Description of this rule's purpose
- `final` (Boolean) Flag to control whether to stop the event from being checked against other rules. Default: true



<a id="nestedatt--output_s3"></a>
### Nested Schema for `output_s3`

Required:

- `bucket` (String) Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `aws_api_key` (String) This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String) Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `compress` (String) Data compression format to apply to HTTP content before it is delivered. Default: "gzip"; must be one of ["none", "gzip"]
- `compression_level` (String) Compression level to apply before moving files to final destination. Default: "best_speed"; must be one of ["best_speed", "normal", "best_compression"]
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `dest_path` (String) Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`. Default: ""
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_assume_role` (Boolean) Use Assume Role credentials to access S3. Default: false
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `endpoint` (String) S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `file_name_suffix` (String) JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`). Default: "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
- `format` (String) Format of the output data. Default: "json"; must be one of ["json", "raw", "parquet"]
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_s3--key_value_metadata))
- `kms_key_id` (String) ID or ARN of the KMS customer-managed key to use for encryption
- `max_closing_files_to_backpressure` (Number) Maximum number of files that can be waiting for upload before backpressure is applied. Default: 100
- `max_concurrent_file_parts` (Number) Maximum number of parts to upload in parallel per file. Minimum part size is 5MB. Default: 4
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `object_acl` (String) Object ACL to assign to uploaded objects. Default: "private"; must be one of ["private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `partition_expr` (String) JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory. Default: "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `region` (String) Region where the S3 bucket is located
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `server_side_encryption` (String) must be one of ["AES256", "aws:kms"]
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `signature_version` (String) Signature version to use for signing S3 requests. Default: "v4"; must be one of ["v2", "v4"]
- `stage_path` (String) Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_class` (String) Storage class to select for uploaded objects. must be one of ["STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "s3"
- `verify_permissions` (Boolean) Disable if you can access files within the bucket but not the bucket itself. Default: true
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_s3--key_value_metadata"></a>
### Nested Schema for `output_s3.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_security_lake"></a>
### Nested Schema for `output_security_lake`

Required:

- `account_id` (String) ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.
- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `bucket` (String) Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
- `custom_source` (String) Name of the custom source configured in Amazon Security Lake
- `region` (String) Region where the Amazon Security Lake is located.

Optional:

- `add_id_to_stage_path` (Boolean) Add the Output ID value to staging location. Default: true
- `assume_role_external_id` (String) External ID to use when assuming role
- `automatic_schema` (Boolean) Automatically calculate the schema based on the events of each Parquet file generated. Default: false
- `aws_api_key` (String) This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String)
- `base_file_name` (String) JavaScript expression to define the output filename prefix (can be constant). Default: "`CriblOut`"
- `deadletter_enabled` (Boolean) If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors. Default: false
- `deadletter_path` (String) Storage location for files that fail to reach their final destination after maximum retries are exceeded. Default: "$CRIBL_HOME/state/outputs/dead-letter"
- `description` (String)
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `empty_dir_cleanup_sec` (Number) How frequently, in seconds, to clean up empty directories. Default: 300
- `enable_assume_role` (Boolean) Use Assume Role credentials to access S3. Default: false
- `enable_page_checksum` (Boolean) Parquet tools can use the checksum of a Parquet page to verify data integrity. Default: false
- `enable_statistics` (Boolean) Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics. Default: true
- `enable_write_page_index` (Boolean) One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping. Default: true
- `endpoint` (String) Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `header_line` (String) If set, this line will be written to the beginning of each output file. Default: ""
- `id` (String) Unique ID for this output
- `key_value_metadata` (Attributes List) The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001" (see [below for nested schema](#nestedatt--output_security_lake--key_value_metadata))
- `kms_key_id` (String) ID or ARN of the KMS customer-managed key to use for encryption
- `max_closing_files_to_backpressure` (Number) Maximum number of files that can be waiting for upload before backpressure is applied. Default: 100
- `max_concurrent_file_parts` (Number) Maximum number of parts to upload in parallel per file. Minimum part size is 5MB. Default: 4
- `max_file_idle_time_sec` (Number) Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location. Default: 30
- `max_file_open_time_sec` (Number) Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location. Default: 300
- `max_file_size_mb` (Number) Maximum uncompressed output file size. Files of this size will be closed and moved to final output location. Default: 32
- `max_open_files` (Number) Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location. Default: 100
- `max_retry_num` (Number) The maximum number of times a file will attempt to move to its final destination before being dead-lettered. Default: 20
- `object_acl` (String) Object ACL to assign to uploaded objects. Default: "private"; must be one of ["private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop"]
- `on_disk_full_backpressure` (String) How to handle events when disk space is below the global 'Min free disk space' limit. Default: "block"; must be one of ["block", "drop"]
- `parquet_data_page_version` (String) Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it. Default: "DATA_PAGE_V2"; must be one of ["DATA_PAGE_V1", "DATA_PAGE_V2"]
- `parquet_page_size` (String) Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Default: "1MB"
- `parquet_row_group_length` (Number) The number of rows that every group will contain. The final group can contain a smaller number of rows. Default: 10000
- `parquet_schema` (String) To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
- `parquet_version` (String) Determines which data types are supported and how they are represented. Default: "PARQUET_2_6"; must be one of ["PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `remove_empty_dirs` (Boolean) Remove empty staging directories after moving files. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `server_side_encryption` (String) must be one of ["AES256", "aws:kms"]
- `should_log_invalid_rows` (Boolean) Log up to 3 rows that @{product} skips due to data mismatch
- `signature_version` (String) Signature version to use for signing Amazon Security Lake requests. Default: "v4"; must be one of ["v2", "v4"]
- `stage_path` (String) Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage. Default: "$CRIBL_HOME/state/outputs/staging"
- `storage_class` (String) Storage class to select for uploaded objects. must be one of ["STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively. Default: ["cribl_pipe"]
- `type` (String) must be "security_lake"
- `verify_permissions` (Boolean) Disable if you can access files within the bucket but not the bucket itself. Default: true
- `write_high_water_mark` (Number) Buffer size used to write to a file. Default: 64

<a id="nestedatt--output_security_lake--key_value_metadata"></a>
### Nested Schema for `output_security_lake.key_value_metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""



<a id="nestedatt--output_sentinel"></a>
### Nested Schema for `output_sentinel`

Required:

- `client_id` (String) JavaScript expression to compute the Client ID for the Azure application. Can be a constant.
- `login_url` (String) URL for OAuth
- `secret` (String) Secret parameter value to pass in request body

Optional:

- `advanced_content_type` (String) HTTP content-type header value. Default: "application/json"
- `auth_type` (String) must be "oauth"
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `custom_content_type` (String) Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry. Default: "application/x-ndjson"
- `custom_drop_when_null` (Boolean) Whether to drop events when the source expression evaluates to null. Default: false
- `custom_event_delimiter` (String) Delimiter string to insert between individual events. Defaults to newline character. Default: "\n"
- `custom_payload_expression` (String) Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object. Default: "`${events}`"
- `custom_source_expression` (String) Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON. Default: "__httpOut"
- `dce_endpoint` (String) Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`
- `dcr_id` (String) Immutable ID for the Data Collection Rule (DCR)
- `description` (String)
- `endpoint_url_configuration` (String) Enter the data collection endpoint URL or the individual ID. Default: "url"; must be one of ["url", "ID"]
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field. (see [below for nested schema](#nestedatt--output_sentinel--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) must be one of ["ndjson", "json_array", "custom", "advanced"]
- `format_event_code` (String) Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
- `format_payload_code` (String) Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
- `id` (String) Unique ID for this output
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB). Default: 1000
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_sentinel--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_sentinel--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `scope` (String) Scope to pass in the OAuth request. Default: "https://monitor.azure.com/.default"
- `stream_name` (String) The name of the stream (Sentinel table) in which to store the events
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_sentinel--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `type` (String) must be "sentinel"
- `url` (String) URL to send events to. Can be overwritten by an event's __url field.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_sentinel--extra_http_headers"></a>
### Nested Schema for `output_sentinel.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_sentinel--pq_controls"></a>
### Nested Schema for `output_sentinel.pq_controls`


<a id="nestedatt--output_sentinel--response_retry_settings"></a>
### Nested Schema for `output_sentinel.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_sentinel--timeout_retry_settings"></a>
### Nested Schema for `output_sentinel.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_service_now"></a>
### Nested Schema for `output_service_now`

Required:

- `token_secret` (String) Select or create a stored text secret

Optional:

- `auth_token_name` (String) Default: "lightstep-access-token"
- `compress` (String) Type of compression to apply to messages sent to the OpenTelemetry endpoint. Default: "gzip"; must be one of ["none", "deflate", "gzip"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `endpoint` (String) The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Default: "ingest.lightstep.com:443"
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_service_now--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `http_compress` (String) Type of compression to apply to messages sent to the OpenTelemetry endpoint. Default: "gzip"; must be one of ["none", "gzip"]
- `http_logs_endpoint_override` (String) If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `http_metrics_endpoint_override` (String) If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `http_traces_endpoint_override` (String) If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
- `id` (String) Unique ID for this output
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `keep_alive_time` (Number) How often the sender should ping the peer to keep the connection open. Default: 30
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 2048
- `metadata` (Attributes List) List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'. (see [below for nested schema](#nestedatt--output_service_now--metadata))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `otlp_version` (String) The version of OTLP Protobuf definitions to use when structuring data to send. Default: "1.3.1"; must be "1.3.1"
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_service_now--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) Select a transport option for OpenTelemetry. Default: "grpc"; must be one of ["grpc", "http"]
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_service_now--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_service_now--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_service_now--tls))
- `type` (String) must be "service_now"
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_service_now--extra_http_headers"></a>
### Nested Schema for `output_service_now.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_service_now--metadata"></a>
### Nested Schema for `output_service_now.metadata`

Required:

- `value` (String)

Optional:

- `key` (String) Default: ""


<a id="nestedatt--output_service_now--pq_controls"></a>
### Nested Schema for `output_service_now.pq_controls`


<a id="nestedatt--output_service_now--response_retry_settings"></a>
### Nested Schema for `output_service_now.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_service_now--timeout_retry_settings"></a>
### Nested Schema for `output_service_now.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_service_now--tls"></a>
### Nested Schema for `output_service_now.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another


                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true



<a id="nestedatt--output_signalfx"></a>
### Nested Schema for `output_signalfx`

Required:

- `type` (String) must be "signalfx"

Optional:

- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_signalfx--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_signalfx--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `realm` (String) SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions). Default: "us0"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_signalfx--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_signalfx--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_signalfx--extra_http_headers"></a>
### Nested Schema for `output_signalfx.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_signalfx--pq_controls"></a>
### Nested Schema for `output_signalfx.pq_controls`


<a id="nestedatt--output_signalfx--response_retry_settings"></a>
### Nested Schema for `output_signalfx.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_signalfx--timeout_retry_settings"></a>
### Nested Schema for `output_signalfx.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_snmp"></a>
### Nested Schema for `output_snmp`

Required:

- `hosts` (Attributes List) One or more SNMP destinations to forward traps to (see [below for nested schema](#nestedatt--output_snmp--hosts))
- `type` (String) must be "snmp"

Optional:

- `description` (String)
- `dns_resolve_period_sec` (Number) How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup. Default: 0
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `pipeline` (String) Pipeline to process data before sending out to this output
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]

<a id="nestedatt--output_snmp--hosts"></a>
### Nested Schema for `output_snmp.hosts`

Required:

- `host` (String) Destination host

Optional:

- `port` (Number) Destination port, default is 162. Default: 162



<a id="nestedatt--output_sns"></a>
### Nested Schema for `output_sns`

Required:

- `message_group_id` (String) Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
- `topic_arn` (String) The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`

Optional:

- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `aws_api_key` (String)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String)
- `description` (String)
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `enable_assume_role` (Boolean) Use Assume Role credentials to access SNS. Default: false
- `endpoint` (String) SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `max_retries` (Number) Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_sns--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `region` (String) Region where the SNS is located
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `signature_version` (String) Signature version to use for signing SNS requests. Default: "v4"; must be one of ["v2", "v4"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "sns"

<a id="nestedatt--output_sns--pq_controls"></a>
### Nested Schema for `output_sns.pq_controls`



<a id="nestedatt--output_splunk"></a>
### Nested Schema for `output_splunk`

Required:

- `host` (String) The hostname of the receiver

Optional:

- `auth_token` (String) Shared secret token to use when establishing a connection to a Splunk indexer. Default: ""
- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (String) Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data. Default: "disabled"; must be one of ["disabled", "auto", "always"]
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `enable_ack` (Boolean) Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown. Default: true
- `enable_multi_metrics` (Boolean) Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above. Default: false
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `id` (String) Unique ID for this output
- `log_failed_requests` (Boolean) Use to troubleshoot issues with sending data. Default: false
- `max_failed_health_checks` (Number) Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur. Default: 1
- `max_s2_sversion` (String) The highest S2S protocol version to advertise during handshake. Default: "v3"; must be one of ["v3", "v4"]
- `nested_fields` (String) How to serialize nested fields into index-time fields. Default: "none"; must be one of ["json", "none"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) The port to connect to on the provided host. Default: 9997
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_splunk--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_splunk--tls))
- `type` (String) must be "splunk"
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_splunk--pq_controls"></a>
### Nested Schema for `output_splunk.pq_controls`


<a id="nestedatt--output_splunk--tls"></a>
### Nested Schema for `output_splunk.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_splunk_hec"></a>
### Nested Schema for `output_splunk_hec`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "splunk_hec"

Optional:

- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `enable_multi_metrics` (Boolean) Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event. Default: false
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_splunk_hec--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS. Default: true
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `next_queue` (String) In the Splunk app, define which Splunk processing queue to send the events after HEC processing. Default: "indexQueue"
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_splunk_hec--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_splunk_hec--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `tcp_routing` (String) In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set. Default: "nowhere"
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_splunk_hec--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) Splunk HEC authentication token
- `url` (String) URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event. Default: "http://localhost:8088/services/collector/event"
- `urls` (Attributes List) (see [below for nested schema](#nestedatt--output_splunk_hec--urls))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_splunk_hec--extra_http_headers"></a>
### Nested Schema for `output_splunk_hec.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_splunk_hec--pq_controls"></a>
### Nested Schema for `output_splunk_hec.pq_controls`


<a id="nestedatt--output_splunk_hec--response_retry_settings"></a>
### Nested Schema for `output_splunk_hec.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_splunk_hec--timeout_retry_settings"></a>
### Nested Schema for `output_splunk_hec.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_splunk_hec--urls"></a>
### Nested Schema for `output_splunk_hec.urls`

Optional:

- `url` (String) URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event. Default: "http://localhost:8088/services/collector/event"
- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1



<a id="nestedatt--output_splunk_lb"></a>
### Nested Schema for `output_splunk_lb`

Required:

- `hosts` (Attributes List) Set of Splunk indexers to load-balance data to. (see [below for nested schema](#nestedatt--output_splunk_lb--hosts))
- `type` (String) must be "splunk_lb"

Optional:

- `auth_token` (String) Shared secret token to use when establishing a connection to a Splunk indexer. Default: ""
- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (String) Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data. Default: "disabled"; must be one of ["disabled", "auto", "always"]
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `enable_ack` (Boolean) Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown. Default: true
- `enable_multi_metrics` (Boolean) Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above. Default: false
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `id` (String) Unique ID for this output
- `indexer_discovery` (Boolean) Automatically discover indexers in indexer clustering environment. Default: false
- `indexer_discovery_configs` (Attributes) List of configurations to set up indexer discovery in Splunk Indexer clustering environment. (see [below for nested schema](#nestedatt--output_splunk_lb--indexer_discovery_configs))
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `log_failed_requests` (Boolean) Use to troubleshoot issues with sending data. Default: false
- `max_concurrent_senders` (Number) Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited. Default: 0
- `max_failed_health_checks` (Number) Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur. Default: 1
- `max_s2_sversion` (String) The highest S2S protocol version to advertise during handshake. Default: "v3"; must be one of ["v3", "v4"]
- `nested_fields` (String) How to serialize nested fields into index-time fields. Default: "none"; must be one of ["json", "none"]
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_splunk_lb--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `sender_unhealthy_time_allowance` (Number) How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute. Default: 100
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_splunk_lb--tls))
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_splunk_lb--hosts"></a>
### Nested Schema for `output_splunk_lb.hosts`

Required:

- `host` (String) The hostname of the receiver

Optional:

- `port` (Number) The port to connect to on the provided host. Default: 9997
- `servername` (String) Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
- `tls` (String) Whether to inherit TLS configs from group setting or disable TLS. Default: "inherit"; must be one of ["inherit", "off"]
- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1


<a id="nestedatt--output_splunk_lb--indexer_discovery_configs"></a>
### Nested Schema for `output_splunk_lb.indexer_discovery_configs`

Required:

- `master_uri` (String) Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089

Optional:

- `auth_token` (String) Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted. Default: ""
- `auth_tokens` (Attributes List) Tokens required to authenticate to cluster manager for indexer discovery (see [below for nested schema](#nestedatt--output_splunk_lb--indexer_discovery_configs--auth_tokens))
- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `refresh_interval_sec` (Number) Time interval, in seconds, between two consecutive indexer list fetches from cluster manager. Default: 300
- `reject_unauthorized` (Boolean) During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates. Default: false
- `site` (String) Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site. Default: "default"
- `text_secret` (String) Select or create a stored text secret

<a id="nestedatt--output_splunk_lb--indexer_discovery_configs--auth_tokens"></a>
### Nested Schema for `output_splunk_lb.indexer_discovery_configs.auth_tokens`

Optional:

- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]



<a id="nestedatt--output_splunk_lb--pq_controls"></a>
### Nested Schema for `output_splunk_lb.pq_controls`


<a id="nestedatt--output_splunk_lb--tls"></a>
### Nested Schema for `output_splunk_lb.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another


                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_sqs"></a>
### Nested Schema for `output_sqs`

Required:

- `queue_name` (String) The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.

Optional:

- `assume_role_arn` (String) Amazon Resource Name (ARN) of the role to assume
- `assume_role_external_id` (String) External ID to use when assuming role
- `aws_account_id` (String) SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
- `aws_api_key` (String)
- `aws_authentication_method` (String) AWS authentication method. Choose Auto to use IAM roles. Default: "auto"; must be one of ["auto", "manual", "secret"]
- `aws_secret` (String) Select or create a stored secret that references your access key and secret key
- `aws_secret_key` (String)
- `create_queue` (Boolean) Create queue if it does not exist. Default: true
- `description` (String)
- `duration_seconds` (Number) Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours). Default: 3600
- `enable_assume_role` (Boolean) Use Assume Role credentials to access SQS. Default: false
- `endpoint` (String) SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size. Default: 1
- `id` (String) Unique ID for this output
- `max_in_progress` (Number) The maximum number of in-progress API requests before backpressure is applied. Default: 10
- `max_queue_size` (Number) Maximum number of queued batches before blocking. Default: 100
- `max_record_size_kb` (Number) Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB. Default: 256
- `message_group_id` (String) This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value. Default: "cribl"
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_sqs--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `queue_type` (String) The queue type used (or created). Defaults to Standard. Default: "standard"; must be one of ["standard", "fifo"]
- `region` (String) AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
- `reject_unauthorized` (Boolean) Reject certificates that cannot be verified against a valid CA, such as self-signed certificates. Default: true
- `reuse_connections` (Boolean) Reuse connections between requests, which can improve performance. Default: true
- `signature_version` (String) Signature version to use for signing SQS requests. Default: "v4"; must be one of ["v2", "v4"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `type` (String) must be "sqs"

<a id="nestedatt--output_sqs--pq_controls"></a>
### Nested Schema for `output_sqs.pq_controls`



<a id="nestedatt--output_statsd"></a>
### Nested Schema for `output_statsd`

Required:

- `host` (String) The hostname of the destination.

Optional:

- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `dns_resolve_period_sec` (Number) How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup. Default: 0
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination. Default: 1
- `id` (String) Unique ID for this output
- `mtu` (Number) When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system. Default: 512
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) Destination port. Default: 8125
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_statsd--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) Protocol to use when communicating with the destination. Default: "udp"; must be one of ["udp", "tcp"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) must be "statsd"
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_statsd--pq_controls"></a>
### Nested Schema for `output_statsd.pq_controls`



<a id="nestedatt--output_statsd_ext"></a>
### Nested Schema for `output_statsd_ext`

Required:

- `host` (String) The hostname of the destination.

Optional:

- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `dns_resolve_period_sec` (Number) How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup. Default: 0
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `flush_period_sec` (Number) When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination. Default: 1
- `id` (String) Unique ID for this output
- `mtu` (Number) When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system. Default: 512
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) Destination port. Default: 8125
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_statsd_ext--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) Protocol to use when communicating with the destination. Default: "udp"; must be one of ["udp", "tcp"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `type` (String) must be "statsd_ext"
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_statsd_ext--pq_controls"></a>
### Nested Schema for `output_statsd_ext.pq_controls`



<a id="nestedatt--output_sumo_logic"></a>
### Nested Schema for `output_sumo_logic`

Required:

- `type` (String) must be "sumo_logic"
- `url` (String) Sumo Logic HTTP collector URL to which events should be sent

Optional:

- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `custom_category` (String) Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.
- `custom_source` (String) Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_sumo_logic--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) Preserve the raw event format instead of JSONifying it. Default: "json"; must be one of ["json", "raw"]
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 1024
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_sumo_logic--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_sumo_logic--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_sumo_logic--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_sumo_logic--extra_http_headers"></a>
### Nested Schema for `output_sumo_logic.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_sumo_logic--pq_controls"></a>
### Nested Schema for `output_sumo_logic.pq_controls`


<a id="nestedatt--output_sumo_logic--response_retry_settings"></a>
### Nested Schema for `output_sumo_logic.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_sumo_logic--timeout_retry_settings"></a>
### Nested Schema for `output_sumo_logic.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_syslog"></a>
### Nested Schema for `output_syslog`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "syslog"

Optional:

- `app_name` (String) Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set. Default: "Cribl"
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `facility` (Number) Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user. Default: 1; must be one of ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21"]
- `host` (String) The hostname of the receiver
- `load_balanced` (Boolean) For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS. Default: true
- `log_failed_requests` (Boolean) Use to troubleshoot issues with sending data. Default: false
- `max_record_size` (Number) Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation. Default: 1500
- `message_format` (String) The syslog message format depending on the receiver's support. Default: "rfc3164"; must be one of ["rfc3164", "rfc5424"]
- `octet_count_framing` (Boolean) Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) The port to connect to on the provided host
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_syslog--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `protocol` (String) The network protocol to use for sending out syslog messages. Default: "tcp"; must be one of ["tcp", "udp"]
- `severity` (Number) Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice. Default: 5; must be one of ["0", "1", "2", "3", "4", "5", "6", "7"]
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `timestamp_format` (String) Timestamp format to use when serializing event's time field. Default: "syslog"; must be one of ["syslog", "iso8601"]
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_syslog--tls))
- `udp_dns_resolve_period_sec` (Number) How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup. Default: 0
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_syslog--pq_controls"></a>
### Nested Schema for `output_syslog.pq_controls`


<a id="nestedatt--output_syslog--tls"></a>
### Nested Schema for `output_syslog.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_tcpjson"></a>
### Nested Schema for `output_tcpjson`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "tcpjson"

Optional:

- `auth_token` (String) Optional authentication token to include as part of the connection header. Default: ""
- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compression` (String) Codec to use to compress the data before sending. Default: "gzip"; must be one of ["none", "gzip"]
- `connection_timeout` (Number) Amount of time (milliseconds) to wait for the connection to establish before retrying. Default: 10000
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `host` (String) The hostname of the receiver
- `hosts` (Attributes List) Set of hosts to load-balance data to (see [below for nested schema](#nestedatt--output_tcpjson--hosts))
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) Use load-balanced destinations. Default: true
- `log_failed_requests` (Boolean) Use to troubleshoot issues with sending data. Default: false
- `max_concurrent_senders` (Number) Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited. Default: 0
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `port` (Number) The port to connect to on the provided host
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_tcpjson--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `send_header` (Boolean) Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will. Default: true
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_tcpjson--tls))
- `token_ttl_minutes` (Number) The number of minutes before the internally generated authentication token expires, valid values between 1 and 60. Default: 60
- `write_timeout` (Number) Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead. Default: 60000

<a id="nestedatt--output_tcpjson--hosts"></a>
### Nested Schema for `output_tcpjson.hosts`

Required:

- `host` (String) The hostname of the receiver
- `port` (Number) The port to connect to on the provided host

Optional:

- `servername` (String) Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
- `tls` (String) Whether to inherit TLS configs from group setting or disable TLS. Default: "inherit"; must be one of ["inherit", "off"]
- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1


<a id="nestedatt--output_tcpjson--pq_controls"></a>
### Nested Schema for `output_tcpjson.pq_controls`


<a id="nestedatt--output_tcpjson--tls"></a>
### Nested Schema for `output_tcpjson.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `reject_unauthorized` (Boolean) Reject certificates that are not authorized by a CA in the CA certificate path, or by another 
                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
Default: true
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.



<a id="nestedatt--output_wavefront"></a>
### Nested Schema for `output_wavefront`

Required:

- `type` (String) must be "wavefront"

Optional:

- `auth_type` (String) Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate. Default: "manual"; must be one of ["manual", "secret"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `domain` (String) WaveFront domain name, e.g. "longboard". Default: "longboard"
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_wavefront--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `id` (String) Unique ID for this output
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_wavefront--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_wavefront--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_wavefront--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_wavefront--extra_http_headers"></a>
### Nested Schema for `output_wavefront.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_wavefront--pq_controls"></a>
### Nested Schema for `output_wavefront.pq_controls`


<a id="nestedatt--output_wavefront--response_retry_settings"></a>
### Nested Schema for `output_wavefront.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_wavefront--timeout_retry_settings"></a>
### Nested Schema for `output_wavefront.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false



<a id="nestedatt--output_webhook"></a>
### Nested Schema for `output_webhook`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "webhook"

Optional:

- `advanced_content_type` (String) HTTP content-type header value. Default: "application/json"
- `auth_header_expr` (String) JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`. Default: "`Bearer ${token}`"
- `auth_type` (String) Authentication method to use for the HTTP request. Default: "none"; must be one of ["none", "basic", "credentialsSecret", "token", "textSecret", "oauth"]
- `compress` (Boolean) Compress the payload body before sending. Default: true
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `credentials_secret` (String) Select or create a secret that references your credentials
- `custom_content_type` (String) Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry. Default: "application/x-ndjson"
- `custom_drop_when_null` (Boolean) Whether to drop events when the source expression evaluates to null. Default: false
- `custom_event_delimiter` (String) Delimiter string to insert between individual events. Defaults to newline character. Default: "\n"
- `custom_payload_expression` (String) Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object. Default: "`${events}`"
- `custom_source_expression` (String) Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON. Default: "__httpOut"
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `extra_http_headers` (Attributes List) Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields). (see [below for nested schema](#nestedatt--output_webhook--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `format` (String) How to format events before sending out. Default: "ndjson"; must be one of ["ndjson", "json_array", "custom", "advanced"]
- `format_event_code` (String) Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
- `format_payload_code` (String) Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
- `keep_alive` (Boolean) Disable to close the connection immediately after sending the outgoing request. Default: true
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS. Default: false
- `login_url` (String) URL for OAuth
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 4096
- `method` (String) The method to use when sending events. Default: "POST"; must be one of ["POST", "PUT", "PATCH"]
- `oauth_headers` (Attributes List) Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_webhook--oauth_headers))
- `oauth_params` (Attributes List) Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request. (see [below for nested schema](#nestedatt--output_webhook--oauth_params))
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `password` (String)
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_webhook--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_webhook--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `secret` (String) Secret parameter value to pass in request body
- `secret_param_name` (String) Secret parameter name to pass in request body
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_webhook--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `tls` (Attributes) (see [below for nested schema](#nestedatt--output_webhook--tls))
- `token` (String) Bearer token to include in the authorization header
- `token_attribute_name` (String) Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
- `token_timeout_secs` (Number) How often the OAuth token should be refreshed. Default: 3600
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `url` (String) URL of a webhook endpoint to send events to, such as http://localhost:10200
- `urls` (Attributes List) (see [below for nested schema](#nestedatt--output_webhook--urls))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false
- `username` (String)

<a id="nestedatt--output_webhook--extra_http_headers"></a>
### Nested Schema for `output_webhook.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_webhook--oauth_headers"></a>
### Nested Schema for `output_webhook.oauth_headers`

Required:

- `name` (String) OAuth header name
- `value` (String) OAuth header value


<a id="nestedatt--output_webhook--oauth_params"></a>
### Nested Schema for `output_webhook.oauth_params`

Required:

- `name` (String) OAuth parameter name
- `value` (String) OAuth parameter value


<a id="nestedatt--output_webhook--pq_controls"></a>
### Nested Schema for `output_webhook.pq_controls`


<a id="nestedatt--output_webhook--response_retry_settings"></a>
### Nested Schema for `output_webhook.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_webhook--timeout_retry_settings"></a>
### Nested Schema for `output_webhook.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_webhook--tls"></a>
### Nested Schema for `output_webhook.tls`

Optional:

- `ca_path` (String) Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
- `cert_path` (String) Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
- `certificate_name` (String) The name of the predefined certificate
- `disabled` (Boolean) Default: true
- `max_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `min_version` (String) must be one of ["TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3"]
- `passphrase` (String) Passphrase to use to decrypt private key
- `priv_key_path` (String) Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
- `servername` (String) Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.


<a id="nestedatt--output_webhook--urls"></a>
### Nested Schema for `output_webhook.urls`

Required:

- `url` (String) URL of a webhook endpoint to send events to, such as http://localhost:10200

Optional:

- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1



<a id="nestedatt--output_xsiam"></a>
### Nested Schema for `output_xsiam`

Required:

- `id` (String) Unique ID for this output
- `type` (String) must be "xsiam"

Optional:

- `auth_type` (String) Enter a token directly, or provide a secret referencing a token. Default: "token"; must be one of ["token", "secret"]
- `concurrency` (Number) Maximum number of ongoing requests before blocking. Default: 5
- `description` (String)
- `dns_resolve_period_sec` (Number) The interval in which to re-resolve any hostnames and pick up destinations from A records. Default: 600
- `environment` (String) Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
- `exclude_self` (Boolean) Exclude all IPs of the current host from the list of any resolved hostnames. Default: false
- `extra_http_headers` (Attributes List) Headers to add to all events (see [below for nested schema](#nestedatt--output_xsiam--extra_http_headers))
- `failed_request_logging_mode` (String) Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below. Default: "none"; must be one of ["payload", "payloadAndHeaders", "none"]
- `flush_period_sec` (Number) Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit. Default: 1
- `load_balance_stats_period_sec` (Number) How far back in time to keep traffic stats for load balancing purposes. Default: 300
- `load_balanced` (Boolean) Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS. Default: false
- `max_payload_events` (Number) Maximum number of events to include in the request body. Default is 0 (unlimited). Default: 0
- `max_payload_size_kb` (Number) Maximum size, in KB, of the request body. Default: 10000
- `on_backpressure` (String) How to handle events when all receivers are exerting backpressure. Default: "block"; must be one of ["block", "drop", "queue"]
- `pipeline` (String) Pipeline to process data before sending out to this output
- `pq_compress` (String) Codec to use to compress the persisted data. Default: "none"; must be one of ["none", "gzip"]
- `pq_controls` (Attributes) (see [below for nested schema](#nestedatt--output_xsiam--pq_controls))
- `pq_max_file_size` (String) The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.). Default: "1 MB"
- `pq_max_size` (String) The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc. Default: "5GB"
- `pq_mode` (String) In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem. Default: "error"; must be one of ["error", "backpressure", "always"]
- `pq_on_backpressure` (String) How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged. Default: "block"; must be one of ["block", "drop"]
- `pq_path` (String) The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>. Default: "$CRIBL_HOME/state/queues"
- `reject_unauthorized` (Boolean) Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). 
        Enabled by default. When this setting is also present in TLS Settings (Client Side), 
        that value will take precedence.
Default: true
- `response_honor_retry_after_header` (Boolean) Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored. Default: false
- `response_retry_settings` (Attributes List) Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable) (see [below for nested schema](#nestedatt--output_xsiam--response_retry_settings))
- `safe_headers` (List of String) List of headers that are safe to log in plain text. Default: []
- `streamtags` (List of String) Tags for filtering and grouping in @{product}. Default: []
- `system_fields` (List of String) Fields to automatically add to events, such as cribl_pipe. Supports wildcards. Default: ["cribl_pipe"]
- `text_secret` (String) Select or create a stored text secret
- `throttle_rate_req_per_sec` (Number) Maximum number of requests to limit to per second. Default: 400
- `timeout_retry_settings` (Attributes) (see [below for nested schema](#nestedatt--output_xsiam--timeout_retry_settings))
- `timeout_sec` (Number) Amount of time, in seconds, to wait for a request to complete before canceling it. Default: 30
- `token` (String) XSIAM authentication token
- `total_memory_limit_kb` (Number) Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
- `url` (String) XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event. Default: "http://localhost:8088/logs/v1/event"
- `urls` (Attributes List) (see [below for nested schema](#nestedatt--output_xsiam--urls))
- `use_round_robin_dns` (Boolean) Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations. Default: false

<a id="nestedatt--output_xsiam--extra_http_headers"></a>
### Nested Schema for `output_xsiam.extra_http_headers`

Required:

- `value` (String)

Optional:

- `name` (String)


<a id="nestedatt--output_xsiam--pq_controls"></a>
### Nested Schema for `output_xsiam.pq_controls`


<a id="nestedatt--output_xsiam--response_retry_settings"></a>
### Nested Schema for `output_xsiam.response_retry_settings`

Required:

- `http_status` (Number) The HTTP response status code that will trigger retries

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000


<a id="nestedatt--output_xsiam--timeout_retry_settings"></a>
### Nested Schema for `output_xsiam.timeout_retry_settings`

Optional:

- `backoff_rate` (Number) Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc. Default: 2
- `initial_backoff` (Number) How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes). Default: 1000
- `max_backoff` (Number) The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds). Default: 10000
- `timeout_retry` (Boolean) Default: false


<a id="nestedatt--output_xsiam--urls"></a>
### Nested Schema for `output_xsiam.urls`

Required:

- `url` (String) Parsed as JSON.

Optional:

- `weight` (Number) Assign a weight (>0) to each endpoint to indicate its traffic-handling capability. Default: 1

## Import

Import is supported using the following syntax:

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = criblio_destination.my_criblio_destination
  id = jsonencode({
    group_id = "default"
    id = "out-s3-main"
  })
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
terraform import criblio_destination.my_criblio_destination '{"group_id": "default", "id": "out-s3-main"}'
```
