---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "criblio_collector Data Source - terraform-provider-criblio"
subcategory: ""
description: |-
  Collector DataSource
---

# criblio_collector (Data Source)

Collector DataSource

## Example Usage

```terraform
data "criblio_collector" "my_collector" {
  group_id = "default"
  id       = "...my_id..."
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `group_id` (String) The consumer group to which this instance belongs. Defaults to 'default'.
- `id` (String) Unique ID to GET

### Read-Only

- `collector` (Attributes) (see [below for nested schema](#nestedatt--collector))
- `environment` (String)
- `ignore_group_jobs_limit` (Boolean)
- `input` (Attributes) (see [below for nested schema](#nestedatt--input))
- `remove_fields` (List of String)
- `resume_on_boot` (Boolean)
- `schedule` (Attributes) Configuration for a scheduled job (see [below for nested schema](#nestedatt--schedule))
- `streamtags` (List of String) Tags for filtering and grouping
- `ttl` (String)
- `worker_affinity` (Boolean) If enabled, tasks are created and run by the same Worker Node

<a id="nestedatt--collector"></a>
### Nested Schema for `collector`

Read-Only:

- `conf` (Attributes) (see [below for nested schema](#nestedatt--collector--conf))
- `destructive` (Boolean) Delete any files collected (where applicable)
- `encoding` (String) Character encoding to use when parsing ingested data
- `type` (String)

<a id="nestedatt--collector--conf"></a>
### Nested Schema for `collector.conf`

Read-Only:

- `auth_type` (String)
- `authentication` (String)
- `aws_api_key` (String)
- `aws_authentication_method` (String)
- `aws_secret` (String)
- `aws_secret_key` (String)
- `bucket` (String) S3 Bucket from which to collect data
- `collect_method` (String)
- `collect_url` (String) URL to use for the Collect operation
- `connection_id` (String) Select an existing Database Connection
- `connection_string` (String) Azure storage account Connection String
- `container_name` (String) Azure container to collect from
- `credentials_secret` (String)
- `dataset` (String) Lake dataset to collect data from
- `disable_time_filter` (Boolean)
- `earliest` (String) Earliest time boundary for the search
- `endpoint` (String) REST API endpoint used to create a search
- `extractors` (Attributes List) (see [below for nested schema](#nestedatt--collector--conf--extractors))
- `handle_escaped_chars` (Boolean)
- `latest` (String) Latest time boundary for the search
- `max_batch_size` (Number)
- `output_mode` (String)
- `password` (String)
- `path` (String) Directory where data will be collected
- `query` (String) Query string for selecting data from the database
- `query_validation_enabled` (Boolean)
- `recurse` (Boolean)
- `region` (String) AWS region from which to retrieve data
- `reject_unauthorized` (Boolean)
- `search` (String) Splunk search query
- `search_head` (String) Search head base URL
- `service_account_credentials` (String)
- `storage_account_name` (String)
- `timeout` (Number)
- `token` (String)
- `token_secret` (String)
- `use_round_robin_dns` (Boolean)
- `username` (String)

<a id="nestedatt--collector--conf--extractors"></a>
### Nested Schema for `collector.conf.extractors`




<a id="nestedatt--input"></a>
### Nested Schema for `input`

Read-Only:

- `breaker_rulesets` (List of String) A list of event-breaking rulesets that will be applied, in order, to the input data stream
- `metadata` (Attributes List) Fields to add to events from this input (see [below for nested schema](#nestedatt--input--metadata))
- `output` (String) Destination to send results to
- `pipeline` (String) Pipeline to process results
- `preprocess` (Attributes) (see [below for nested schema](#nestedatt--input--preprocess))
- `send_to_routes` (Boolean) Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.
- `stale_channel_flush_ms` (Number) How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
- `throttle_rate_per_sec` (String) Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
- `type` (String)

<a id="nestedatt--input--metadata"></a>
### Nested Schema for `input.metadata`

Read-Only:

- `name` (String)
- `value` (String) JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)


<a id="nestedatt--input--preprocess"></a>
### Nested Schema for `input.preprocess`

Read-Only:

- `args` (List of String) Arguments to be added to the custom command
- `command` (String) Command to feed the data through (via stdin) and process its output (stdout)
- `disabled` (Boolean)



<a id="nestedatt--schedule"></a>
### Nested Schema for `schedule`

Read-Only:

- `cron_schedule` (String) A cron schedule on which to run this job
- `enabled` (Boolean) Enable to configure scheduling for this Collector
- `max_concurrent_runs` (Number) The maximum number of instances of this scheduled job that may be running at any time
- `run` (Attributes) (see [below for nested schema](#nestedatt--schedule--run))
- `skippable` (Boolean) Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits

<a id="nestedatt--schedule--run"></a>
### Nested Schema for `schedule.run`

Read-Only:

- `earliest` (Number) Earliest time to collect data for the selected timezone
- `expression` (String) A filter for tokens in the provided collect path and/or the events being collected
- `job_timeout` (String) Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time.
- `latest` (Number) Latest time to collect data for the selected timezone
- `log_level` (String) Level at which to set task logging
- `max_task_reschedule` (Number) Maximum number of times a task can be rescheduled
- `max_task_size` (String) Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks.
- `min_task_size` (String) Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task.
- `mode` (String) Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job.
- `reschedule_dropped_tasks` (Boolean) Reschedule tasks that failed with non-fatal errors
- `time_range_type` (String)
