resource "criblio_destination" "my_destination" {
group_id = "Cribl"
id = "out-s3-main"
output_azure_blob = {
add_id_to_stage_path = true
auth_type = "manual"
automatic_schema = true
azure_cloud = "AzurePublicCloud"
base_file_name = "`CriblOut`"
certificate = {
certificate_name = "azure-app-cert"
}
client_id = "11111111-1111-1111-1111-111111111111"
client_text_secret = "azure-sp-client-secret"
compress = "gzip"
compression_level = "normal"
connection_string = "DefaultEndpointsProtocol=https;AccountName=criblstore;AccountKey=***REDACTED***;EndpointSuffix=core.windows.net"
container_name = "cribl-data"
create_container = true
deadletter_enabled = true
deadletter_path = "/var/lib/cribl/state/outputs/dead-letter"
description = "Write objects to Azure Blob Storage with date-based partitioning"
dest_path = "logs/ingest"
empty_dir_cleanup_sec = 600
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
endpoint_suffix = "core.windows.net"
environment = "main"
file_name_suffix = "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
format = "json"
header_line = "timestamp,host,message"
id = "azure-blob-out"
key_value_metadata = [
{
key = "team"
value = "platform"
}
]
max_concurrent_file_parts = 4
max_file_idle_time_sec = 30
max_file_open_time_sec = 300
max_file_size_mb = 64
max_open_files = 200
max_retry_num = 20
on_backpressure = "block"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "4MB"
parquet_row_group_length = 10000
parquet_version = "PARQUET_2_6"
partition_expr = "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d') + '/host=' + host"
pipeline = "default"
remove_empty_dirs = true
should_log_invalid_rows = true
stage_path = "/var/lib/cribl/state/outputs/staging"
storage_account_name = "criblstore"
storage_class = "Hot"
streamtags = [
"azure",
"blob",
]
system_fields = [
"cribl_pipe",
]
tenant_id = "00000000-0000-0000-0000-000000000000"
text_secret = "azure-connstr-secret"
type = "azure_blob"
write_high_water_mark = 256
}
output_azure_data_explorer = {
add_id_to_stage_path = true
additional_properties = [
{
key = "format"
value = "json"
}
]
certificate = {
certificate_name = "adx-app-cert"
}
client_id = "11111111-1111-1111-1111-111111111111"
client_secret = "***REDACTED***"
cluster_url = "https://mycluster.eastus.kusto.windows.net"
compress = "gzip"
concurrency = 8
database = "telemetry"
deadletter_enabled = true
description = "Ingest data into Azure Data Explorer (Kusto)"
environment = "main"
extent_tags = [
{
prefix = "ingestBy"
value = "source:app1"
}
]
file_name_suffix = "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
flush_immediately = true
flush_period_sec = 1
format = "json"
id = "adx-out"
ingest_if_not_exists = [
{
value = "batchId:2025-10-02T00:00Z"
}
]
ingest_mode = "batching"
ingest_url = "https://ingest-mycluster.eastus.kusto.windows.net"
is_mapping_obj = false
keep_alive = true
mapping_ref = "my_table_mapping"
max_concurrent_file_parts = 4
max_file_idle_time_sec = 30
max_file_open_time_sec = 300
max_file_size_mb = 64
max_open_files = 200
max_payload_events = 0
max_payload_size_kb = 4096
oauth_endpoint = "https://login.microsoftonline.com"
oauth_type = "clientSecret"
on_backpressure = "block"
on_disk_full_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
remove_empty_dirs = true
report_level = "failuresOnly"
report_method = "queue"
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
retain_blob_on_success = true
scope = "https://kusto.kusto.windows.net/.default"
stage_path = "/var/lib/cribl/state/outputs/staging"
streamtags = [
"azure",
"adx",
]
system_fields = [
"cribl_pipe",
]
table = "logs_raw"
tenant_id = "00000000-0000-0000-0000-000000000000"
text_secret = "adx-client-secret"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "azure_data_explorer"
use_round_robin_dns = true
validate_database_settings = true
}
output_azure_eventhub = {
ack = 1
authentication_timeout = 10000
backoff_rate = 2
brokers = [
"myns.servicebus.windows.net:9093",
]
connection_timeout = 10000
description = "Deliver events to Azure Event Hubs via Kafka protocol"
environment = "main"
flush_event_count = 1000
flush_period_sec = 1
format = "json"
id = "eventhub-out"
initial_backoff = 1000
max_back_off = 60000
max_record_size_kb = 768
max_retries = 5
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reauthentication_threshold = 60000
request_timeout = 60000
sasl = {
disabled = false
mechanism = "plain"
}
streamtags = [
"azure",
"eventhub",
]
system_fields = [
"cribl_pipe",
]
tls = {
disabled = false
reject_unauthorized = true
}
topic = "app-events"
type = "azure_eventhub"
}
output_azure_logs = {
api_url = ".ods.opinsights.azure.com"
auth_type = "manual"
compress = true
concurrency = 8
description = "Send logs to Azure Log Analytics workspace"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "azure-logs-out"
keypair_secret = "azure-log-analytics-keys"
log_type = "Cribl"
max_payload_events = 0
max_payload_size_kb = 1024
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
resource_id = "/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg1/providers/Microsoft.Compute/virtualMachines/vm1"
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
safe_headers = [
"X-Request-ID",
]
streamtags = [
"azure",
"loganalytics",
]
system_fields = [
"cribl_pipe",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "azure_logs"
use_round_robin_dns = true
workspace_id = "22222222-2222-2222-2222-222222222222"
workspace_key = "***REDACTED***"
}
output_click_house = {
async_inserts = true
auth_header_expr = "`Bearer ${token}`"
auth_type = "basic"
column_mappings = [
{
column_name = "timestamp"
column_type = "DateTime64(3)"
column_value_expression = "toDateTime64(ts, 3)"
}
]
compress = true
concurrency = 8
credentials_secret = "clickhouse_basic_auth"
database = "logs"
describe_table = "DESCRIBE TABLE app_events"
description = "Ingest logs to ClickHouse with async inserts and TLS"
dump_format_errors_to_disk = true
environment = "main"
exclude_mapping_fields = [
"_raw",
"ts",
]
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
format = "json-each-row"
id = "clickhouse_ingest_prod"
login_url = "https://auth.example.com/oauth/token"
mapping_type = "automatic"
max_payload_events = 1000
max_payload_size_kb = 2048
oauth_headers = [
{
name = "Accept"
value = "application/json"
}
]
oauth_params = [
{
name = "grant_type"
value = "client_credentials"
}
]
on_backpressure = "block"
password = "s3cr3tPass!"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
secret = "s3cr3tClientSecret"
secret_param_name = "client_secret"
sql_username = "clickuser"
streamtags = [
"prod",
"clickhouse",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
table_name = "app_events"
text_secret = "clickhouse_bearer_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/opt/cribl/certs/client.crt"
certificate_name = "clickhouse-client"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "s3cr3t"
priv_key_path = "/opt/cribl/certs/client.key"
servername = "clickhouse.example.com"
}
token = "chBearerToken_abc123xyz"
token_attribute_name = "access_token"
token_timeout_secs = 3600
type = "click_house"
url = "https://clickhouse.example.com:8443"
use_round_robin_dns = true
username = "clickuser"
wait_for_async_inserts = true
}
output_cloudwatch = {
assume_role_arn = "arn:aws:iam::123456789012:role/CloudWatchLogsWriter"
assume_role_external_id = "external-id-abc123"
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws_cloudwatch_credentials"
aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
description = "Send application logs to Amazon CloudWatch Logs"
duration_seconds = 3600
enable_assume_role = true
endpoint = "https://logs.us-east-1.amazonaws.com"
environment = "main"
flush_period_sec = 2
id = "cloudwatch_logs_prod"
log_group_name = "/aws/eks/cluster-1/app-logs"
log_stream_name = "app-logs"
max_queue_size = 10
max_record_size_kb = 1024
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "us-east-1"
reject_unauthorized = true
reuse_connections = true
streamtags = [
"prod",
"cloudwatch",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "cloudwatch"
}
output_confluent_cloud = {
ack = 1
authentication_timeout = 10000
backoff_rate = 2
brokers = [
"mycluster.us-central1.gcp.confluent.cloud:9092",
]
compression = "gzip"
connection_timeout = 10000
description = "Produce events to Confluent Cloud Kafka with Schema Registry"
environment = "main"
flush_event_count = 1000
flush_period_sec = 1
format = "json"
id = "ccloud-out"
initial_backoff = 1000
kafka_schema_registry = {
auth = {
credentials_secret = "ccloud-schema-registry-basic"
disabled = false
}
connection_timeout = 30000
default_key_schema_id = 1
default_value_schema_id = 100
disabled = false
max_retries = 3
request_timeout = 30000
schema_registry_url = "https://schema-registry.confluent.cloud"
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "ccloud-sr-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "schema-registry.confluent.cloud"
}
}
max_back_off = 60000
max_record_size_kb = 768
max_retries = 5
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protobuf_library_id = "user-events-protos"
reauthentication_threshold = 60000
request_timeout = 60000
sasl = {
disabled = false
mechanism = "plain"
}
streamtags = [
"confluent",
"kafka",
]
system_fields = [
"cribl_pipe",
]
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "ccloud-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "mycluster.us-central1.gcp.confluent.cloud"
}
topic = "app-events"
type = "confluent_cloud"
}
output_cribl_http = {
compression = "gzip"
concurrency = 8
description = "Send events to Cribl Worker HTTP endpoint with retries"
dns_resolve_period_sec = 300
environment = "main"
exclude_fields = [
"__kube_*",
"__metadata",
]
exclude_self = false
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "cribl_http_prod"
load_balance_stats_period_sec = 300
load_balanced = true
max_payload_events = 1000
max_payload_size_kb = 2048
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"cribl",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/opt/cribl/certs/client.crt"
certificate_name = "cribl-client"
disabled = true
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "s3cr3t"
priv_key_path = "/opt/cribl/certs/client.key"
reject_unauthorized = true
servername = "collector.cribl.example.com"
}
token_ttl_minutes = 60
type = "cribl_http"
url = "https://edge.example.com:10200"
urls = [
{
url = "https://edge01.example.com:10200"
weight = 2
}
]
use_round_robin_dns = true
}
output_cribl_lake = {
description = "Cribl Lake destination"
dest_path = "security_logs"
id = "lake_ingest_prod"
type = "cribl_lake"
}
output_cribl_tcp = {
compression = "gzip"
connection_timeout = 10000
description = "Send events to Cribl Edge over TCP with TLS"
dns_resolve_period_sec = 300
environment = "main"
exclude_fields = [
"__kube_*",
"__metadata",
]
exclude_self = false
host = "edge01.example.com"
hosts = [
{
host = "edge02.example.com"
port = 10300
servername = "edge02.example.com"
tls = "inherit"
weight = 2
}
]
id = "cribl_tcp_prod"
load_balance_stats_period_sec = 300
load_balanced = true
log_failed_requests = true
max_concurrent_senders = 4
on_backpressure = "block"
pipeline = "main"
port = 10300
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
streamtags = [
"prod",
"cribl",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
throttle_rate_per_sec = "10 MB"
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/opt/cribl/certs/client.crt"
certificate_name = "cribl-client"
disabled = true
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "s3cr3t"
priv_key_path = "/opt/cribl/certs/client.key"
reject_unauthorized = true
servername = "tcp.cribl.example.com"
}
token_ttl_minutes = 60
type = "cribl_tcp"
write_timeout = 30000
}
output_crowdstrike_next_gen_siem = {
auth_type = "secret"
compress = true
concurrency = 8
description = "Send events to CrowdStrike Next-Gen SIEM with token auth"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
format = "raw"
id = "cs_nextgen_siem_prod"
max_payload_events = 1000
max_payload_size_kb = 8192
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"crowdstrike",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "crowdstrike_nextgen_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "csngs-0123456789abcdef0123456789abcdef"
type = "crowdstrike_next_gen_siem"
url = "https://ingest.us-1.crowdstrike.com/api/ingest/hec/abcd1234/v1/services/collector"
use_round_robin_dns = true
}
output_datadog = {
allow_api_key_from_events = false
api_key = "0123456789abcdef0123456789abcdef"
auth_type = "secret"
batch_by_tags = true
compress = true
concurrency = 8
content_type = "json"
custom_url = "https://http-intake.logs.datadoghq.com/api/v2/logs"
description = "Send logs to Datadog Logs API with token auth"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
host = "web-01.example.com"
id = "datadog_logs_prod"
max_payload_events = 1000
max_payload_size_kb = 2048
message = "_raw"
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
send_counters_as_count = true
service = "web-app"
severity = "info"
site = "us"
source = "nginx"
streamtags = [
"prod",
"datadog",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
tags = [
"env:prod",
"team:platform",
]
text_secret = "datadog_api_key"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
total_memory_limit_kb = 51200
type = "datadog"
use_round_robin_dns = true
}
output_dataset = {
api_key = "ds-0123456789abcdef0123456789abcdef"
auth_type = "secret"
compress = true
concurrency = 8
custom_url = "https://api.dataset.com/v1/logs"
default_severity = "info"
description = "Send events to DataSet with API key authentication"
environment = "main"
exclude_fields = [
"sev",
"_time",
]
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "dataset_logs_prod"
max_payload_events = 1000
max_payload_size_kb = 2048
message_field = "_raw"
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
server_host_field = "host"
site = "us"
streamtags = [
"prod",
"dataset",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "dataset_api_key"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
timestamp_field = "ts"
total_memory_limit_kb = 51200
type = "dataset"
use_round_robin_dns = true
}
output_default = {
default_id = "http-default"
environment = "main"
id = "default-output"
pipeline = "default"
streamtags = [
"default",
]
system_fields = [
"cribl_pipe",
]
type = "default"
}
output_devnull = {
environment = "main"
id = "devnull-out"
pipeline = "default"
streamtags = [
"discard",
]
system_fields = [
"cribl_pipe",
]
type = "devnull"
}
output_disk_spool = {
compress = "gzip"
description = "Local disk spool for short-term buffering and replay"
environment = "main"
id = "disk_spool_buffer"
max_data_size = "100GB"
max_data_time = "7d"
partition_expr = "C.Time.strftime(_time || Date.now()/1000, '%Y/%m/%d/%H')"
pipeline = "main"
streamtags = [
"prod",
"spool",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
time_window = "10m"
type = "disk_spool"
}
output_dl_s3 = {
add_id_to_stage_path = true
assume_role_arn = "arn:aws:iam::123456789012:role/S3Writer"
assume_role_external_id = "external-id-abc123"
automatic_schema = true
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws_s3_credentials"
aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
base_file_name = "app-logs"
bucket = "logs-archive-prod"
compress = "gzip"
compression_level = "normal"
deadletter_enabled = true
deadletter_path = "/opt/cribl/state/outputs/dead-letter"
description = "Archive logs to S3 in Parquet with field-based partitions"
dest_path = "year=%Y/month=%m/day=%d/app=orders"
duration_seconds = 3600
empty_dir_cleanup_sec = 600
enable_assume_role = true
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
endpoint = "https://s3.us-east-1.amazonaws.com"
environment = "main"
file_name_suffix = ".parquet.gz"
format = "parquet"
header_line = "timestamp,host,level,message"
id = "dls3_archive_prod"
key_value_metadata = [
{
key = "OCSF Event Class"
value = "9001"
}
]
kms_key_id = "arn:aws:kms:us-east-1:123456789012:key/abcd-1234-efgh-5678"
max_closing_files_to_backpressure = 500
max_concurrent_file_parts = 5
max_file_idle_time_sec = 120
max_file_open_time_sec = 600
max_file_size_mb = 256
max_open_files = 200
max_retry_num = 20
object_acl = "private"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "128MB"
parquet_row_group_length = 100000
parquet_version = "PARQUET_2_6"
partitioning_fields = [
"app",
"env",
]
pipeline = "main"
region = "us-east-1"
reject_unauthorized = true
remove_empty_dirs = true
reuse_connections = true
server_side_encryption = "aws:kms"
should_log_invalid_rows = true
signature_version = "v4"
stage_path = "/opt/cribl/state/outputs/staging"
storage_class = "INTELLIGENT_TIERING"
streamtags = [
"prod",
"archive",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "dl_s3"
verify_permissions = true
write_high_water_mark = 256
}
output_dynatrace_http = {
active_gate_domain = "https://activegate.example.com:9999/e/abc12345/api/v2/logs/ingest"
auth_type = "token"
compress = true
concurrency = 8
description = "Send logs to Dynatrace Logs Ingest API"
endpoint = "cloud"
environment = "main"
environment_id = "abc12345"
extra_http_headers = [
{
name = "Api-Token"
value = "dt0c01.XXXX.YYYYZZZZ"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
format = "json_array"
id = "dynatrace_http_metrics"
keep_alive = true
max_payload_events = 10000
max_payload_size_kb = 4096
method = "POST"
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"api-token",
]
streamtags = [
"prod",
"dynatrace",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
telemetry_type = "logs"
text_secret = "dynatrace_api_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "dt0c01.XXXX.YYYYZZZZ"
total_memory_limit_kb = 51200
type = "dynatrace_http"
url = "https://abc.live.dynatrace.com/e/abc12345/api/v2/logs/ingest"
use_round_robin_dns = true
}
output_dynatrace_otlp = {
auth_token_name = "Authorization"
compress = "gzip"
concurrency = 5
connection_timeout = 10000
description = "Send OTLP logs and metrics to Dynatrace SaaS"
endpoint = "https://abc123.live.dynatrace.com/api/v2/otlp"
endpoint_type = "saas"
environment = "main"
extra_http_headers = [
{
name = "Api-Token"
value = "dt0c01.XXXX.YYYYZZZZ"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
http_compress = "gzip"
http_logs_endpoint_override = "https://abc123.live.dynatrace.com/api/v2/otlp/v1/logs"
http_metrics_endpoint_override = "https://abc123.live.dynatrace.com/api/v2/otlp/v1/metrics"
http_traces_endpoint_override = "https://abc123.live.dynatrace.com/api/v2/otlp/v1/traces"
id = "dynatrace_otlp_export"
keep_alive = true
keep_alive_time = 30
max_payload_size_kb = 2048
metadata = [
{
key = "x-tenant-id"
value = "acme-prod"
}
]
on_backpressure = "block"
otlp_version = "1.3.1"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "http"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"api-token",
]
streamtags = [
"prod",
"dynatrace",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token_secret = "dynatrace_otlp_token"
type = "dynatrace_otlp"
use_round_robin_dns = true
}
output_elastic = {
auth = {
auth_type = "manualAPIKey"
disabled = false
}
compress = true
concurrency = 8
description = "Send documents to Elasticsearch bulk API with retries and custom params"
dns_resolve_period_sec = 300
doc_type = "_doc"
elastic_pipeline = "ingest-grok-pipeline"
elastic_version = "7"
environment = "main"
exclude_self = false
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
extra_params = [
{
name = "filter_path"
value = "errors,items.*.error,items.*._index,items.*.status"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "elastic-out"
include_doc_id = true
index = "\"logs-%{C.Time.strftime(_time, \"%Y.%m.%d\")}\""
load_balance_stats_period_sec = 300
load_balanced = true
max_payload_events = 0
max_payload_size_kb = 4096
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
retry_partial_errors = true
safe_headers = [
"X-Request-ID",
]
streamtags = [
"elastic",
"es",
]
system_fields = [
"cribl_pipe",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "elastic"
url = "https://es.example.com:9200/_bulk"
urls = [
{
url = "https://es-node-1.example.com:9200/_bulk"
weight = 2
}
]
use_round_robin_dns = true
write_action = "create"
}
output_elastic_cloud = {
auth = {
auth_type = "manualAPIKey"
disabled = false
}
compress = true
concurrency = 8
description = "Send documents to Elastic Cloud with retries and pipeline support"
elastic_pipeline = "ingest-grok-pipeline"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
extra_params = [
{
name = "filter_path"
value = "errors,items.*.error,items.*._index,items.*.status"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "es-cloud-out"
include_doc_id = true
index = "\"logs-%{C.Time.strftime(_time, \"%Y.%m.%d\")}\""
max_payload_events = 0
max_payload_size_kb = 4096
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
safe_headers = [
"X-Request-ID",
]
streamtags = [
"elastic",
"cloud",
]
system_fields = [
"cribl_pipe",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "elastic_cloud"
url = "my-deployment:ZXM0LmNsb3VkLmV... (truncated)"
}
output_exabeam = {
add_id_to_stage_path = true
aws_api_key = "***REDACTED***"
aws_secret_key = "***REDACTED***"
bucket = "exabeam-data"
collector_instance_id = "11112222-3333-4444-5555-666677778888"
deadletter_enabled = true
deadletter_path = "/var/lib/cribl/state/outputs/dead-letter"
description = "Deliver logs to Exabeam Collector via GCS staging"
empty_dir_cleanup_sec = 600
encoded_configuration = "***REDACTED***"
endpoint = "https://storage.googleapis.com"
environment = "main"
id = "exabeam-out"
max_file_idle_time_sec = 30
max_file_open_time_sec = 300
max_file_size_mb = 64
max_open_files = 200
max_retry_num = 20
object_acl = "private"
on_backpressure = "block"
on_disk_full_backpressure = "block"
pipeline = "default"
region = "us-central1"
reject_unauthorized = true
remove_empty_dirs = true
reuse_connections = true
signature_version = "v4"
site_id = "site-123"
site_name = "\"corp-east\""
stage_path = "/var/lib/cribl/state/outputs/staging"
storage_class = "NEARLINE"
streamtags = [
"exabeam",
"gcs",
]
system_fields = [
"cribl_pipe",
]
timezone_offset = "-07:00"
type = "exabeam"
}
output_filesystem = {
add_id_to_stage_path = true
automatic_schema = true
base_file_name = "`CriblOut`"
compress = "gzip"
compression_level = "normal"
deadletter_enabled = true
deadletter_path = "/var/lib/cribl/state/outputs/dead-letter"
description = "Write events to local filesystem with daily partitioning"
dest_path = "/var/log/cribl/out"
empty_dir_cleanup_sec = 600
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
environment = "main"
file_name_suffix = "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
format = "json"
header_line = "timestamp,host,message"
id = "filesystem-out"
key_value_metadata = [
{
key = "team"
value = "platform"
}
]
max_file_idle_time_sec = 30
max_file_open_time_sec = 300
max_file_size_mb = 64
max_open_files = 200
max_retry_num = 20
on_backpressure = "block"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "4MB"
parquet_row_group_length = 10000
parquet_version = "PARQUET_2_6"
partition_expr = "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d') + '/host=' + host"
pipeline = "default"
remove_empty_dirs = true
should_log_invalid_rows = true
stage_path = "/var/log/cribl/stage"
streamtags = [
"filesystem",
"prod",
]
system_fields = [
"cribl_pipe",
]
type = "filesystem"
write_high_water_mark = 256
}
output_google_chronicle = {
api_key = "***REDACTED***"
api_key_secret = "chronicle-api-key"
api_version = "v1"
authentication_method = "serviceAccount"
compress = true
concurrency = 8
custom_labels = [
{
key = "...my_key..."
value = "...my_value..."
}
]
customer_id = "123e4567-e89b-12d3-a456-426614174000"
description = "Send events to Google SecOps Chronicle"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
extra_log_types = [
{
description = "...my_description..."
log_type = "...my_log_type..."
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "chronicle-out"
log_format_type = "unstructured"
log_text_field = "message"
log_type = "CUSTOM_WEBLOG"
max_payload_events = 0
max_payload_size_kb = 1024
namespace = "prod-us"
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "us"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 30000
max_backoff = 30000
}
]
safe_headers = [
"X-Request-ID",
]
service_account_credentials = "***REDACTED***"
service_account_credentials_secret = "chronicle-sa-credentials"
streamtags = [
"google",
"chronicle",
]
system_fields = [
"cribl_pipe",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 90
total_memory_limit_kb = 5120
type = "google_chronicle"
use_round_robin_dns = true
}
output_google_cloud_logging = {
cache_fill_bytes_expression = "String(_raw.cache_fill_bytes)"
cache_hit_expression = "Boolean(_raw.cache_hit)"
cache_lookup_expression = "Boolean(_raw.cache_lookup)"
cache_validated_expression = "Boolean(_raw.cache_validated)"
concurrency = 8
connection_timeout = 10000
description = "Send logs to Google Cloud Logging with custom resource labels"
environment = "main"
file_expression = "String(_raw.file)"
first_expression = "Boolean(_raw.operation_first)"
flush_period_sec = 1
function_expression = "String(_raw.function)"
google_auth_method = "secret"
id = "gcl-out"
id_expression = "String(_raw.operation_id)"
index_expression = "Number(_raw.split_index)"
insert_id_expression = "Crypto.uuid()"
last_expression = "Boolean(_raw.operation_last)"
latency_expression = "(_raw.latency_ms/1000).toFixed(3) + \"s\""
line_expression = "String(_raw.line)"
log_labels = [
{
label = "environment"
value_expression = "\"prod\""
}
]
log_location_expression = "\"projects/my-project\""
log_location_type = "project"
log_name_expression = "\"cribl_logs\""
max_payload_events = 0
max_payload_size_kb = 4096
on_backpressure = "block"
payload_expression = "{ message: _raw.message, severity: _raw.severity || \"DEFAULT\" }"
payload_format = "json"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
producer_expression = "String(_raw.operation_producer)"
protocol_expression = "String(_raw.protocol)"
referer_expression = "String(_raw.referer)"
remote_ip_expression = "String(_raw.client_ip)"
request_method_expression = "\"POST\""
request_size_expression = "String(length(_raw.request_body))"
request_url_expression = "\"https://example.com/api\""
resource_type_expression = "\"gce_instance\""
resource_type_labels = [
{
label = "instance_id"
value_expression = "String(_raw.instance_id)"
}
]
response_size_expression = "String(length(_raw.response_body))"
secret = "gcl-service-account"
server_ip_expression = "String(_raw.server_ip)"
service_account_credentials = "***REDACTED***"
severity_expression = "\"INFO\""
span_id_expression = "String(_raw.span_id)"
status_expression = "Number(_raw.status)"
streamtags = [
"gcp",
"logging",
]
system_fields = [
"cribl_pipe",
]
throttle_rate_req_per_sec = 500
timeout_sec = 30
total_memory_limit_kb = 20480
total_splits_expression = "Number(_raw.split_total)"
trace_expression = "String(_raw.trace)"
trace_sampled_expression = "Boolean(_raw.trace_sampled)"
type = "google_cloud_logging"
uid_expression = "String(_raw.split_uid)"
user_agent_expression = "String(_raw.user_agent)"
}
output_google_cloud_storage = {
add_id_to_stage_path = true
automatic_schema = true
aws_api_key = "***REDACTED***"
aws_authentication_method = "manual"
aws_secret = "gcs-hmac-credentials"
aws_secret_key = "***REDACTED***"
base_file_name = "`CriblOut`"
bucket = "cribl-data-bucket"
compress = "gzip"
compression_level = "normal"
deadletter_enabled = true
deadletter_path = "/var/lib/cribl/state/outputs/dead-letter"
description = "Write objects to Google Cloud Storage with date-based partitioning"
dest_path = "logs/ingest"
empty_dir_cleanup_sec = 600
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
endpoint = "https://storage.googleapis.com"
environment = "main"
file_name_suffix = "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
format = "json"
header_line = "timestamp,host,message"
id = "gcs-out"
key_value_metadata = [
{
key = "team"
value = "platform"
}
]
max_file_idle_time_sec = 30
max_file_open_time_sec = 300
max_file_size_mb = 64
max_open_files = 200
max_retry_num = 20
object_acl = "private"
on_backpressure = "block"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "4MB"
parquet_row_group_length = 10000
parquet_version = "PARQUET_2_6"
partition_expr = "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d') + '/host=' + host"
pipeline = "default"
region = "us-central1"
reject_unauthorized = true
remove_empty_dirs = true
reuse_connections = true
should_log_invalid_rows = true
signature_version = "v4"
stage_path = "/var/lib/cribl/state/outputs/staging"
storage_class = "NEARLINE"
streamtags = [
"gcp",
"gcs",
]
system_fields = [
"cribl_pipe",
]
type = "google_cloud_storage"
verify_permissions = true
write_high_water_mark = 256
}
output_google_pubsub = {
batch_size = 1000
batch_timeout = 200
create_topic = true
description = "Publish events to Google Pub/Sub with ordered delivery"
environment = "main"
flush_period_sec = {"type":"number","default":1}
google_auth_method = "secret"
id = "gpubsub-out"
max_in_progress = 20
max_queue_size = 500
max_record_size_kb = 256
on_backpressure = "block"
ordered_delivery = true
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "us-central1"
secret = "gcp-pubsub-sa"
service_account_credentials = "***REDACTED***"
streamtags = [
"gcp",
"pubsub",
]
system_fields = [
"cribl_pipe",
]
topic_name = "app-events"
type = "google_pubsub"
}
output_grafana_cloud = {
output_grafana_cloud_grafana_cloud2 = {
compress = true
concurrency = 2
description = "Send logs and metrics to Grafana Cloud Loki and Prometheus"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 10
id = "grafana_cloud_logs_prod"
labels = [
{
name = "host"
value = "web-01"
}
]
loki_auth = {
auth_type = "basic"
credentials_secret = "grafana_loki_credentials"
password = "glc_efgh5678"
text_secret = "grafana_loki_token"
token = "12345:glc_efgh5678"
username = 12345
}
loki_url = "https://logs-prod-us-central1.grafana.net"
max_payload_events = 1000
max_payload_size_kb = 2048
message = "_raw"
message_format = "protobuf"
metric_rename_expr = "name.replace(/[^a-zA-Z0-9_]/g, '_')"
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
prometheus_auth = {
auth_type = "basic"
credentials_secret = "grafana_prom_credentials"
password = "glc_abcd1234"
text_secret = "grafana_prom_token"
token = "12345:glc_abcd1234"
username = 12345
}
prometheus_url = "https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"grafana",
]
system_fields = [
"cribl_host",
"cribl_wp",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "grafana_cloud"
use_round_robin_dns = true
}
}
output_graphite = {
connection_timeout = 10000
description = "Send metrics to Graphite in plaintext protocol"
dns_resolve_period_sec = 300
environment = "main"
flush_period_sec = 1
host = "graphite.example.com"
id = "graphite_metrics_prod"
mtu = 1400
on_backpressure = "block"
pipeline = "metrics"
port = 2003
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "tcp"
streamtags = [
"prod",
"graphite",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
throttle_rate_per_sec = "10 MB"
type = "graphite"
write_timeout = 30000
}
output_honeycomb = {
auth_type = "manual"
compress = true
concurrency = 8
dataset = "observability"
description = "Send events to Honeycomb dataset"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "honeycomb-out"
max_payload_events = 0
max_payload_size_kb = 4096
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
safe_headers = [
"X-Request-ID",
]
streamtags = [
"honeycomb",
"prod",
]
system_fields = [
"cribl_pipe",
]
team = "***REDACTED***"
text_secret = "honeycomb-api-key"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "honeycomb"
use_round_robin_dns = true
}
output_humio_hec = {
auth_type = "secret"
compress = true
concurrency = 8
description = "Send logs to CrowdStrike Falcon LogScale via HEC endpoint"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
format = "JSON"
id = "humio_hec_prod"
max_payload_events = 1000
max_payload_size_kb = 8192
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"humio",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "humio_hec_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "humio-0123456789abcdef0123456789abcdef"
type = "humio_hec"
url = "https://cloud.us.humio.com/api/v1/ingest/hec"
use_round_robin_dns = true
}
output_influxdb = {
auth_header_expr = "`Bearer ${token}`"
auth_type = "token"
bucket = "metrics_prod"
compress = true
concurrency = 8
credentials_secret = "influxdb_basic_auth"
database = "telegraf"
description = "Send metrics to InfluxDB with v2 API and token auth"
dynamic_value_field_name = true
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "influxdb_metrics_prod"
login_url = "https://influxdb.example.com/oauth/token"
max_payload_events = 5000
max_payload_size_kb = 8192
oauth_headers = [
{
name = "Accept"
value = "application/json"
}
]
oauth_params = [
{
name = "grant_type"
value = "client_credentials"
}
]
on_backpressure = "block"
org = "acme-observability"
password = "s3cr3tPass!"
pipeline = "metrics"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
secret = "s3cr3tClientSecret"
secret_param_name = "client_secret"
streamtags = [
"prod",
"influxdb",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "influxdb_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
timestamp_precision = "ms"
token = "influxV2Token_abc123xyz"
token_attribute_name = "access_token"
token_timeout_secs = 3600
type = "influxdb"
url = "https://influxdb.example.com:8086/write"
use_round_robin_dns = true
use_v2_api = true
username = "influx_writer"
value_field_name = "value"
}
output_kafka = {
ack = 1
authentication_timeout = 10000
backoff_rate = 2
brokers = [
"kafka-1.example.com:9092",
"kafka-2.example.com:9092",
]
compression = "gzip"
connection_timeout = 10000
description = "Produce events to Kafka with retries and TLS"
environment = "main"
flush_event_count = 1000
flush_period_sec = 1
format = "json"
id = "kafka-out"
initial_backoff = 1000
kafka_schema_registry = {
auth = {
credentials_secret = "schema-registry-basic-auth"
disabled = false
}
connection_timeout = 30000
default_key_schema_id = 1
default_value_schema_id = 2
disabled = true
max_retries = 3
request_timeout = 30000
schema_registry_url = "https://schema-registry.example.com:8081"
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "kafka-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "schema-registry.example.com"
}
}
max_back_off = 60000
max_record_size_kb = 768
max_retries = 5
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protobuf_library_id = "user-events-protos"
reauthentication_threshold = 60000
request_timeout = 60000
sasl = {
disabled = false
mechanism = "scram-sha-512"
}
streamtags = [
"kafka",
"prod",
]
system_fields = [
"cribl_pipe",
]
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "kafka-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "kafka-broker.example.com"
}
topic = "app-events"
type = "kafka"
}
output_kinesis = {
as_ndjson = true
assume_role_arn = "arn:aws:iam::123456789012:role/cribl-kinesis-writer"
assume_role_external_id = "cribl-external-123"
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws-credentials-secret"
aws_secret_key = "***REDACTED***"
compression = "gzip"
concurrency = 8
description = "Deliver events to AWS Kinesis Data Streams"
duration_seconds = 3600
enable_assume_role = true
endpoint = "https://kinesis.us-east-1.amazonaws.com"
environment = "main"
flush_period_sec = 1
id = "kinesis-out"
max_record_size_kb = 1024
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "us-east-1"
reject_unauthorized = true
reuse_connections = true
signature_version = "v4"
stream_name = "app-events-stream"
streamtags = [
"aws",
"kinesis",
]
system_fields = [
"cribl_pipe",
]
type = "kinesis"
use_list_shards = true
}
output_loki = {
auth_type = "token"
compress = true
concurrency = 2
credentials_secret = "grafana_loki_credentials"
description = "Send logs to Loki with labels and batching"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 10
id = "loki_logs_prod"
labels = [
{
name = "host"
value = "web-01"
}
]
max_payload_events = 1000
max_payload_size_kb = 2048
message = "_raw"
message_format = "protobuf"
on_backpressure = "block"
password = "glc_abcd1234"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"loki",
]
system_fields = [
"cribl_host",
"cribl_wp",
]
text_secret = "grafana_loki_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "12345:glc_abcd1234"
total_memory_limit_kb = 51200
type = "loki"
url = "https://loki.example.com/loki/api/v1/push"
use_round_robin_dns = true
username = 12345
}
output_minio = {
add_id_to_stage_path = true
automatic_schema = true
aws_api_key = "minio_access_key"
aws_authentication_method = "manual"
aws_secret = "minio_credentials"
aws_secret_key = "minio_secret_key_123"
base_file_name = "app-logs"
bucket = "logs-prod"
compress = "gzip"
compression_level = "normal"
deadletter_enabled = true
deadletter_path = "/opt/cribl/state/outputs/dead-letter"
description = "Archive logs to MinIO in Parquet with date-based partitioning"
dest_path = "year=%Y/month=%m/day=%d/app=orders"
empty_dir_cleanup_sec = 600
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
endpoint = "http://minio.example.com:9000"
environment = "main"
file_name_suffix = ".json.gz"
format = "json"
header_line = "timestamp,host,level,message"
id = "minio_archive_prod"
key_value_metadata = [
{
key = "OCSF Event Class"
value = "9001"
}
]
max_concurrent_file_parts = 5
max_file_idle_time_sec = 120
max_file_open_time_sec = 600
max_file_size_mb = 128
max_open_files = 200
max_retry_num = 20
object_acl = "private"
on_backpressure = "block"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "128MB"
parquet_row_group_length = 100000
parquet_version = "PARQUET_2_6"
partition_expr = "C.Time.strftime(_time || Date.now()/1000, '%Y/%m/%d/%H')"
pipeline = "main"
region = "us-east-1"
reject_unauthorized = true
remove_empty_dirs = true
reuse_connections = true
server_side_encryption = "AES256"
should_log_invalid_rows = true
signature_version = "v4"
stage_path = "/opt/cribl/state/outputs/staging"
storage_class = "STANDARD"
streamtags = [
"prod",
"minio",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "minio"
verify_permissions = true
write_high_water_mark = 256
}
output_msk = {
ack = 1
assume_role_arn = "arn:aws:iam::123456789012:role/cribl-msk-access"
assume_role_external_id = "cribl-external-123"
authentication_timeout = 10000
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws-credentials-secret"
aws_secret_key = "***REDACTED***"
backoff_rate = 2
brokers = [
"b-1.mskcluster.abcde.c2.kafka.us-east-1.amazonaws.com:9092",
"b-2.mskcluster.abcde.c2.kafka.us-east-1.amazonaws.com:9092",
]
compression = "gzip"
connection_timeout = 10000
description = "Produce events to Amazon MSK with retries and TLS"
duration_seconds = 3600
enable_assume_role = true
endpoint = "https://kafka.us-east-1.amazonaws.com"
environment = "main"
flush_event_count = 1000
flush_period_sec = 1
format = "json"
id = "msk-out"
initial_backoff = 1000
kafka_schema_registry = {
auth = {
credentials_secret = "msk-schema-registry-basic"
disabled = false
}
connection_timeout = 30000
default_key_schema_id = 1
default_value_schema_id = 100
disabled = true
max_retries = 3
request_timeout = 30000
schema_registry_url = "https://schema-registry.example.com:8081"
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "msk-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "schema-registry.example.com"
}
}
max_back_off = 60000
max_record_size_kb = 768
max_retries = 5
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protobuf_library_id = "user-events-protos"
reauthentication_threshold = 60000
region = "us-east-1"
reject_unauthorized = true
request_timeout = 60000
reuse_connections = true
signature_version = "v4"
streamtags = [
"aws",
"msk",
]
system_fields = [
"cribl_pipe",
]
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "msk-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "b-1.mskcluster.abcde.c2.kafka.us-east-1.amazonaws.com"
}
topic = "app-events"
type = "msk"
}
output_netflow = {
description = "Forward NetFlow v5/v9/IPFIX to downstream collectors"
dns_resolve_period_sec = 300
environment = "main"
hosts = [
{
host = "netflow-collector.example.com"
port = 2055
}
]
id = "netflow_export_prod"
pipeline = "main"
streamtags = [
"prod",
"netflow",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "netflow"
}
output_newrelic = {
api_key = "NRAK-0123456789abcdef0123456789abcdef"
auth_type = "manual"
compress = true
concurrency = 8
custom_url = "https://log-api.newrelic.com/log/v1"
description = "Send logs to New Relic Logs with custom endpoint"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "newrelic_logs_prod"
log_type = "access_log"
max_payload_events = 500
max_payload_size_kb = 512
message_field = "_raw"
metadata = [
{
name = "service"
value = "`\"orders-service\"`"
}
]
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "US"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"newrelic",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "newrelic_api_key"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
total_memory_limit_kb = 51200
type = "newrelic"
use_round_robin_dns = true
}
output_newrelic_events = {
account_id = "12345678"
api_key = "NRAK-0123456789abcdef0123456789abcdef"
auth_type = "secret"
compress = true
concurrency = 8
custom_url = "https://insights-collector.newrelic.com/v1/accounts/12345678/events"
description = "Send custom events to New Relic Events API"
environment = "main"
event_type = "CriblCustomEvent"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "newrelic_events_prod"
max_payload_events = 500
max_payload_size_kb = 512
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "US"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"newrelic",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "newrelic_api_key"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
type = "newrelic_events"
use_round_robin_dns = true
}
output_open_telemetry = {
auth_header_expr = "`Bearer ${token}`"
auth_type = "token"
compress = "gzip"
concurrency = 5
connection_timeout = 10000
credentials_secret = "otel_basic_auth"
description = "Export telemetry to OTel Collector with OAuth and keepalive"
endpoint = "https://otel-collector.example.com:4317"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
http_compress = "gzip"
http_logs_endpoint_override = "https://otel-collector.example.com/v1/logs"
http_metrics_endpoint_override = "https://otel-collector.example.com/v1/metrics"
http_traces_endpoint_override = "https://otel-collector.example.com/v1/traces"
id = "otel_export_prod"
keep_alive = true
keep_alive_time = 30
login_url = "https://auth.example.com/oauth/token"
max_payload_size_kb = 2048
metadata = [
{
key = "x-tenant-id"
value = "acme-prod"
}
]
oauth_headers = [
{
name = "Accept"
value = "application/json"
}
]
oauth_params = [
{
name = "grant_type"
value = "client_credentials"
}
]
on_backpressure = "block"
otlp_version = "1.3.1"
password = "s3cr3tPass!"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "grpc"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
secret = "s3cr3tClientSecret"
secret_param_name = "client_secret"
streamtags = [
"prod",
"otel",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "otel_bearer_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/opt/cribl/certs/client.crt"
certificate_name = "otel-client"
disabled = true
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "s3cr3t"
priv_key_path = "/opt/cribl/certs/client.key"
reject_unauthorized = true
}
token = "otelBearerToken_abc123xyz"
token_attribute_name = "access_token"
token_timeout_secs = 3600
type = "open_telemetry"
use_round_robin_dns = true
username = "otel_user"
}
output_prometheus = {
auth_header_expr = "`Bearer ${token}`"
auth_type = "basic"
concurrency = 8
credentials_secret = "prometheus_basic_auth"
description = "Send metrics to Prometheus remote_write with basic auth"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "prometheus_metrics_prod"
login_url = "https://auth.example.com/oauth/token"
max_payload_events = 1000
max_payload_size_kb = 2048
metric_rename_expr = "name.replace(/[^a-zA-Z0-9_]/g, '_')"
metrics_flush_period_sec = 60
oauth_headers = [
{
name = "Accept"
value = "application/json"
}
]
oauth_params = [
{
name = "grant_type"
value = "client_credentials"
}
]
on_backpressure = "block"
password = "mimir_api_key_abcd1234"
pipeline = "metrics"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
secret = "s3cr3tClientSecret"
secret_param_name = "client_secret"
send_metadata = true
streamtags = [
"prod",
"prometheus",
]
system_fields = [
"cribl_host",
"cribl_wp",
]
text_secret = "prometheus_bearer_token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "promBearerToken_abc123xyz"
token_attribute_name = "access_token"
token_timeout_secs = 3600
type = "prometheus"
url = "https://prometheus.example.com/api/v1/write"
use_round_robin_dns = true
username = "prometheus"
}
output_ring = {
compress = "gzip"
description = "Local ring buffer for short-term retention and replay"
dest_path = "/opt/cribl/state/ring_buffer_prod"
environment = "main"
format = "json"
id = "ring_buffer_prod"
max_data_size = "100GB"
max_data_time = "7d"
on_backpressure = "block"
partition_expr = "C.Time.strftime(_time || Date.now()/1000, '%Y/%m/%d/%H')"
pipeline = "main"
streamtags = [
"prod",
"ring",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "ring"
}
output_router = {
description = "Route events to outputs based on filter rules"
environment = "main"
id = "router_main"
pipeline = "main"
rules = [
{
description = "Route application errors to Splunk"
filter = "`_source == \"app\" && level == \"error\"`"
final = true
output = "OutputSplunk"
}
]
streamtags = [
"prod",
"routing",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "router"
}
output_s3 = {
add_id_to_stage_path = true
assume_role_arn = "arn:aws:iam::123456789012:role/cribl-s3-writer"
assume_role_external_id = "cribl-external-123"
automatic_schema = true
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws-credentials-secret"
aws_secret_key = "***REDACTED***"
base_file_name = "`CriblOut`"
bucket = "cribl-data-bucket"
compress = "gzip"
compression_level = "normal"
deadletter_enabled = true
deadletter_path = "/var/lib/cribl/state/outputs/dead-letter"
description = "Write objects to S3 with date-based partitioning"
dest_path = "logs/ingest"
duration_seconds = 3600
empty_dir_cleanup_sec = 600
enable_assume_role = true
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
endpoint = "https://s3.us-east-1.amazonaws.com"
environment = "main"
file_name_suffix = "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`"
format = "json"
header_line = "timestamp,host,message"
id = "s3-out"
key_value_metadata = [
{
key = "team"
value = "platform"
}
]
kms_key_id = "arn:aws:kms:us-east-1:123456789012:key/abcd1234-5678-90ab-cdef-EXAMPLEKEY"
max_closing_files_to_backpressure = 500
max_concurrent_file_parts = 4
max_file_idle_time_sec = 30
max_file_open_time_sec = 300
max_file_size_mb = 64
max_open_files = 200
max_retry_num = 20
object_acl = "private"
on_backpressure = "block"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "4MB"
parquet_row_group_length = 10000
parquet_version = "PARQUET_2_6"
partition_expr = "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d') + '/host=' + host"
pipeline = "default"
region = "us-east-1"
reject_unauthorized = true
remove_empty_dirs = true
reuse_connections = true
server_side_encryption = "AES256"
should_log_invalid_rows = true
signature_version = "v4"
stage_path = "/var/lib/cribl/state/outputs/staging"
storage_class = "STANDARD_IA"
streamtags = [
"s3",
"prod",
]
system_fields = [
"cribl_pipe",
]
type = "s3"
verify_permissions = true
write_high_water_mark = 256
}
output_security_lake = {
account_id = "123456789012"
add_id_to_stage_path = true
assume_role_arn = "arn:aws:iam::123456789012:role/SecurityLakeIngestRole"
assume_role_external_id = "external-id-abc123"
automatic_schema = true
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws_security_lake_credentials"
aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
base_file_name = "app-logs"
bucket = "security-lake-us-east-1-123456789012"
custom_source = "cribl_custom_source"
deadletter_enabled = true
deadletter_path = "/opt/cribl/state/outputs/dead-letter"
description = "Deliver OCSF-compliant logs to Amazon Security Lake"
duration_seconds = 3600
empty_dir_cleanup_sec = 600
enable_assume_role = true
enable_page_checksum = true
enable_statistics = true
enable_write_page_index = true
endpoint = "https://security-lake.us-east-1.amazonaws.com"
environment = "main"
header_line = "timestamp,host,level,message"
id = "security_lake_export_prod"
key_value_metadata = [
{
key = "OCSF Event Class"
value = "9001"
}
]
kms_key_id = "arn:aws:kms:us-east-1:123456789012:key/abcd-1234-efgh-5678"
max_closing_files_to_backpressure = 500
max_concurrent_file_parts = 5
max_file_idle_time_sec = 120
max_file_open_time_sec = 600
max_file_size_mb = 256
max_open_files = 200
max_retry_num = 20
object_acl = "private"
on_backpressure = "block"
on_disk_full_backpressure = "block"
parquet_data_page_version = "DATA_PAGE_V2"
parquet_page_size = "128MB"
parquet_row_group_length = 100000
parquet_schema = "ocsf_1_1_0"
parquet_version = "PARQUET_2_6"
pipeline = "main"
region = "us-east-1"
reject_unauthorized = true
remove_empty_dirs = true
reuse_connections = true
server_side_encryption = "aws:kms"
should_log_invalid_rows = true
signature_version = "v4"
stage_path = "/opt/cribl/state/outputs/staging"
storage_class = "INTELLIGENT_TIERING"
streamtags = [
"prod",
"securitylake",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "security_lake"
verify_permissions = true
write_high_water_mark = 256
}
output_sentinel = {
advanced_content_type = "application/json"
auth_type = "oauth"
client_id = "11111111-2222-3333-4444-555555555555"
compress = true
concurrency = 8
custom_content_type = "application/x-ndjson"
custom_drop_when_null = false
custom_event_delimiter = "\n"
custom_payload_expression = "`{ \"items\": [${events}] }`"
custom_source_expression = "raw=${_raw}"
dce_endpoint = "https://mydce-abc123.eastus.ingest.monitor.azure.com"
dcr_id = "12345678-90ab-cdef-1234-567890abcdef"
description = "Send events to Microsoft Sentinel (DCR/DCE)"
endpoint_url_configuration = "url"
environment = "main"
extra_http_headers = [
{
name = "Content-Type"
value = "application/json"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
format = "ndjson"
format_event_code = "if (__e.level === 'error') { __e.__eventOut = JSON.stringify(__e); }"
format_payload_code = "__e.__payloadOut = JSON.stringify({ records: __e.payload });"
id = "sentinel-out"
keep_alive = true
login_url = "https://login.microsoftonline.com/<tenant>/oauth2/v2.0/token"
max_payload_events = 500
max_payload_size_kb = 1000
on_backpressure = "queue"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"X-Request-ID",
]
scope = "https://monitor.azure.com/.default"
secret = "***REDACTED***"
stream_name = "Custom-MyTable_CL"
streamtags = [
"azure",
"sentinel",
]
system_fields = [
"cribl_pipe",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
total_memory_limit_kb = 20480
type = "sentinel"
url = "https://example.dce.ingest.monitor.azure.com"
use_round_robin_dns = true
}
output_service_now = {
auth_token_name = "lightstep-access-token"
compress = "gzip"
concurrency = 5
connection_timeout = 10000
description = "Export telemetry to ServiceNow (Lightstep) OTLP ingest"
endpoint = "ingest.lightstep.com:443"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
http_compress = "gzip"
http_logs_endpoint_override = "https://ingest.lightstep.com/v1/logs"
http_metrics_endpoint_override = "https://ingest.lightstep.com/v1/metrics"
http_traces_endpoint_override = "https://ingest.lightstep.com/v1/traces"
id = "servicenow_otel_export"
keep_alive = true
keep_alive_time = 30
max_payload_size_kb = 2048
metadata = [
{
key = "x-tenant-id"
value = "acme-prod"
}
]
on_backpressure = "block"
otlp_version = "1.3.1"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "grpc"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"servicenow",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/opt/cribl/certs/client.crt"
certificate_name = "otel-client"
disabled = true
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "s3cr3t"
priv_key_path = "/opt/cribl/certs/client.key"
reject_unauthorized = true
}
token_secret = "servicenow_access_token"
type = "service_now"
use_round_robin_dns = true
}
output_signalfx = {
auth_type = "manual"
compress = true
concurrency = 8
description = "Send metrics to Splunk Observability (SignalFx)"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "signalfx-out"
max_payload_events = 0
max_payload_size_kb = 4096
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
realm = "us1"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
safe_headers = [
"X-Request-ID",
]
streamtags = [
"signalfx",
"metrics",
]
system_fields = [
"cribl_pipe",
]
text_secret = "signalfx-api-token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "***REDACTED***"
type = "signalfx"
use_round_robin_dns = true
}
output_snmp = {
description = "Forward SNMP traps to network monitoring systems"
dns_resolve_period_sec = 300
environment = "main"
hosts = [
{
host = "snmp01.example.com"
port = 162
}
]
id = "snmp_trap_forwarder"
pipeline = "main"
streamtags = [
"prod",
"snmp",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "snmp"
}
output_sns = {
assume_role_arn = "arn:aws:iam::123456789012:role/SNSPublisher"
assume_role_external_id = "external-id-abc123"
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws_sns_credentials"
aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
description = "Publish alerts to Amazon SNS FIFO topic"
duration_seconds = 3600
enable_assume_role = true
endpoint = "https://sns.us-east-1.amazonaws.com"
environment = "main"
id = "sns_alerts_prod"
max_retries = 5
message_group_id = "`alerts-${C.vars.service}`"
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
region = "us-east-1"
reject_unauthorized = true
reuse_connections = true
signature_version = "v4"
streamtags = [
"prod",
"alerts",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
topic_arn = "arn:aws:sns:us-east-1:123456789012:alerts-topic"
type = "sns"
}
output_splunk = {
auth_token = "***REDACTED***"
auth_type = "manual"
compress = "auto"
connection_timeout = 10000
description = "Send events to Splunk indexers over S2S"
enable_ack = true
enable_multi_metrics = false
environment = "main"
host = "splunk-indexer.example.com"
id = "splunk-main"
log_failed_requests = false
max_failed_health_checks = 1
max_s2_sversion = "v3"
nested_fields = "none"
on_backpressure = "block"
pipeline = "default"
port = 9997
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
streamtags = [
"splunk",
"prod",
]
system_fields = [
"cribl_pipe",
]
text_secret = "splunk-indexer-token"
throttle_rate_per_sec = "50 MB"
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "splunk-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "splunk-indexer.example.com"
}
type = "splunk"
write_timeout = 60000
}
output_splunk_hec = {
auth_type = "manual"
compress = true
concurrency = 8
description = "Send events to Splunk HEC"
dns_resolve_period_sec = 300
enable_multi_metrics = false
environment = "main"
exclude_self = false
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "splunk-hec-main"
load_balance_stats_period_sec = 300
load_balanced = true
max_payload_events = 0
max_payload_size_kb = 4096
next_queue = "indexQueue"
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
safe_headers = [
"X-Request-ID",
]
streamtags = [
"splunk",
"hec",
]
system_fields = [
"cribl_pipe",
]
tcp_routing = "default_route"
text_secret = "splunk-hec-token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "***REDACTED***"
type = "splunk_hec"
url = "http://splunk-hec.example.com:8088/services/collector/event"
urls = [
{
url = "http://splunk-hec-2.example.com:8088/services/collector/event"
weight = 2
}
]
use_round_robin_dns = true
}
output_splunk_lb = {
auth_token = "***REDACTED***"
auth_type = "manual"
compress = "auto"
connection_timeout = 10000
description = "Load-balance events across Splunk indexers"
dns_resolve_period_sec = 300
enable_ack = true
enable_multi_metrics = false
environment = "main"
exclude_self = false
hosts = [
{
host = "idx1.example.com"
port = 9997
servername = "idx1.example.com"
tls = "inherit"
weight = 2
}
]
id = "splunk-lb-main"
indexer_discovery = true
indexer_discovery_configs = {
auth_token = "***REDACTED***"
auth_tokens = [
{
auth_type = "secret"
}
]
auth_type = "manual"
master_uri = "https://cm.example.com:8089"
refresh_interval_sec = 300
reject_unauthorized = true
site = "site1"
text_secret = "cluster-manager-token"
}
load_balance_stats_period_sec = 300
log_failed_requests = false
max_concurrent_senders = 8
max_failed_health_checks = 1
max_s2_sversion = "v3"
nested_fields = "none"
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
sender_unhealthy_time_allowance = 500
streamtags = [
"splunk",
"prod",
]
system_fields = [
"cribl_pipe",
]
text_secret = "splunk-indexer-token"
throttle_rate_per_sec = "50 MB"
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "splunk-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "splunk-lb.example.com"
}
type = "splunk_lb"
write_timeout = 60000
}
output_sqs = {
assume_role_arn = "arn:aws:iam::123456789012:role/SQSPublisher"
assume_role_external_id = "external-id-abc123"
aws_account_id = "123456789012"
aws_api_key = "AKIAIOSFODNN7EXAMPLE"
aws_authentication_method = "auto"
aws_secret = "aws_sqs_credentials"
aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
create_queue = true
description = "Send events to Amazon SQS FIFO queue with batching"
duration_seconds = 3600
enable_assume_role = true
endpoint = "https://sqs.us-east-1.amazonaws.com"
environment = "main"
flush_period_sec = 2
id = "sqs_events_prod"
max_in_progress = 20
max_queue_size = 200
max_record_size_kb = 256
message_group_id = "logs"
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
queue_name = "https://sqs.us-east-1.amazonaws.com/123456789012/my-queue.fifo"
queue_type = "fifo"
region = "us-east-1"
reject_unauthorized = true
reuse_connections = true
signature_version = "v4"
streamtags = [
"prod",
"sqs",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
type = "sqs"
}
output_statsd = {
connection_timeout = 10000
description = "Send StatsD metrics to central aggregator"
dns_resolve_period_sec = 300
environment = "main"
flush_period_sec = 1
host = "statsd.example.com"
id = "statsd_metrics_prod"
mtu = 1400
on_backpressure = "block"
pipeline = "metrics"
port = 8125
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "udp"
streamtags = [
"prod",
"statsd",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
throttle_rate_per_sec = "10 MB"
type = "statsd"
write_timeout = 30000
}
output_statsd_ext = {
connection_timeout = 10000
description = "Send extended StatsD metrics to external aggregator"
dns_resolve_period_sec = 300
environment = "main"
flush_period_sec = 1
host = "statsd-ext.example.com"
id = "statsd_ext_metrics_prod"
mtu = 1400
on_backpressure = "block"
pipeline = "metrics"
port = 8125
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "udp"
streamtags = [
"prod",
"statsd",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
throttle_rate_per_sec = "10 MB"
type = "statsd_ext"
write_timeout = 30000
}
output_sumo_logic = {
compress = true
concurrency = 8
custom_category = "prod/app/logs"
custom_source = "cribl-stream"
description = "Send logs to Sumo Logic with retries and batching"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
format = "json"
id = "sumologic_logs_prod"
max_payload_events = 500
max_payload_size_kb = 512
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"sumologic",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
total_memory_limit_kb = 51200
type = "sumo_logic"
url = "https://endpoint1.collection.us2.sumologic.com/receiver/v1/http/ABCDEFG1234567890"
use_round_robin_dns = true
}
output_syslog = {
app_name = "Cribl"
connection_timeout = 10000
description = "Send syslog to upstream collector"
environment = "main"
facility = 1
host = "syslog.receiver.example.com"
id = "syslog-out"
load_balanced = true
log_failed_requests = false
max_record_size = 1200
message_format = "rfc3164"
octet_count_framing = true
on_backpressure = "block"
pipeline = "default"
port = 514
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
protocol = "tcp"
severity = 5
streamtags = [
"syslog",
]
system_fields = [
"cribl_pipe",
]
throttle_rate_per_sec = "0"
timestamp_format = "syslog"
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "syslog-client-cert"
disabled = true
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "syslog.example.com"
}
type = "syslog"
udp_dns_resolve_period_sec = 300
write_timeout = 60000
}
output_tcpjson = {
auth_token = "***REDACTED***"
auth_type = "manual"
compression = "gzip"
connection_timeout = 10000
description = "Send JSON events over TCP to downstream services"
dns_resolve_period_sec = 300
environment = "main"
exclude_self = false
host = "tcp.receiver.example.com"
hosts = [
{
host = "tcp1.example.com"
port = 10300
servername = "tcp1.example.com"
tls = "inherit"
weight = 2
}
]
id = "tcpjson-out"
load_balance_stats_period_sec = 300
load_balanced = true
log_failed_requests = false
max_concurrent_senders = 8
on_backpressure = "block"
pipeline = "default"
port = 10300
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
send_header = true
streamtags = [
"tcpjson",
"prod",
]
system_fields = [
"cribl_pipe",
]
text_secret = "tcpjson-auth-token"
throttle_rate_per_sec = "50 MB"
tls = {
ca_path = "/etc/ssl/certs/ca.pem"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "tcpjson-client-cert"
disabled = false
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
reject_unauthorized = true
servername = "tcp.receiver.example.com"
}
token_ttl_minutes = 60
type = "tcpjson"
write_timeout = 60000
}
output_wavefront = {
auth_type = "manual"
compress = true
concurrency = 8
description = "Send metrics to WaveFront"
domain = "longboard"
environment = "main"
extra_http_headers = [
{
name = "X-Request-ID"
value = "abc123"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
id = "wavefront-out"
max_payload_events = 0
max_payload_size_kb = 4096
on_backpressure = "block"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 503
initial_backoff = 1000
max_backoff = 60000
}
]
safe_headers = [
"X-Request-ID",
]
streamtags = [
"wavefront",
"metrics",
]
system_fields = [
"cribl_pipe",
]
text_secret = "wavefront-api-token"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "***REDACTED***"
type = "wavefront"
use_round_robin_dns = true
}
output_webhook = {
advanced_content_type = "application/json"
auth_header_expr = "`Bearer ${token}`"
auth_type = "token"
compress = true
concurrency = 10
credentials_secret = "webhook-credentials"
custom_content_type = "application/x-ndjson"
custom_drop_when_null = false
custom_event_delimiter = "\n"
custom_payload_expression = "`{ \"items\": [${events}] }`"
custom_source_expression = "raw=${_raw}"
description = "Robust webhook delivery with backoff and retries"
dns_resolve_period_sec = 600
environment = "main"
exclude_self = true
extra_http_headers = [
{
name = "X-Custom-Header"
value = "demo"
}
]
failed_request_logging_mode = "payload"
flush_period_sec = 1
format = "ndjson"
format_event_code = "if (__e.severity === 'error') { __e.__eventOut = JSON.stringify(__e); }"
format_payload_code = "__e.__payloadOut = JSON.stringify({ items: __e.payload });"
id = "webhook-out"
keep_alive = true
load_balance_stats_period_sec = 300
load_balanced = true
login_url = "https://auth.example.com/oauth/token"
max_payload_events = 1000
max_payload_size_kb = 8192
method = "POST"
oauth_headers = [
{
name = "Accept"
value = "application/json"
}
]
oauth_params = [
{
name = "grant_type"
value = "client_credentials"
}
]
on_backpressure = "queue"
password = "***REDACTED***"
pipeline = "default"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"X-Trace-Id",
]
secret = "s3cr3t"
secret_param_name = "client_secret"
streamtags = [
"webhook",
]
system_fields = [
"cribl_pipe",
]
text_secret = "webhook-token-secret"
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
tls = {
ca_path = "/etc/ssl/certs/ca-bundle.crt"
cert_path = "/etc/ssl/certs/client.crt"
certificate_name = "webhook-client-cert"
disabled = true
max_version = "TLSv1.3"
min_version = "TLSv1.2"
passphrase = "***REDACTED***"
priv_key_path = "/etc/ssl/private/client.key"
servername = "api.example.com"
}
token = "***REDACTED***"
token_attribute_name = "access_token"
token_timeout_secs = 3600
total_memory_limit_kb = 20480
type = "webhook"
url = "https://hooks.example.com/ingest"
urls = [
{
url = "https://hooks1.example.com/ingest"
weight = 2
}
]
use_round_robin_dns = true
username = "api-user"
}
output_xsiam = {
auth_type = "secret"
concurrency = 8
description = "Send logs to Palo Alto Networks XSIAM with token auth"
dns_resolve_period_sec = 300
environment = "main"
exclude_self = false
extra_http_headers = [
{
name = "X-Request-ID"
value = "123e4567-e89b-12d3-a456-426614174000"
}
]
failed_request_logging_mode = "payloadAndHeaders"
flush_period_sec = 2
id = "xsiam_export_prod"
load_balance_stats_period_sec = 300
load_balanced = true
max_payload_events = 2000
max_payload_size_kb = 8192
on_backpressure = "block"
pipeline = "main"
pq_compress = "gzip"
pq_controls = {
    # ...
}
pq_max_file_size = "100 MB"
pq_max_size = "10GB"
pq_mode = "backpressure"
pq_on_backpressure = "block"
pq_path = "/opt/cribl/state/queues"
reject_unauthorized = true
response_honor_retry_after_header = true
response_retry_settings = [
{
backoff_rate = 2
http_status = 429
initial_backoff = 1000
max_backoff = 30000
}
]
safe_headers = [
"content-type",
"x-request-id",
]
streamtags = [
"prod",
"xsiam",
]
system_fields = [
"cribl_pipe",
"cribl_breaker",
]
text_secret = "xsiam_token"
throttle_rate_req_per_sec = 500
timeout_retry_settings = {
backoff_rate = 2
initial_backoff = 1000
max_backoff = 30000
timeout_retry = true
}
timeout_sec = 30
token = "xsiam-0123456789abcdef0123456789abcdef"
total_memory_limit_kb = 51200
type = "xsiam"
url = "https://api-tenant.paloaltonetworks.com/logs/v1/event"
urls = [
{
url = "{ \"see\": \"documentation\" }"
weight = 3.29
}
]
use_round_robin_dns = true
}
}