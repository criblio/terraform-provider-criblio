// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package provider

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	tfTypes "github.com/criblio/terraform-provider-criblio/internal/provider/types"
	"github.com/criblio/terraform-provider-criblio/internal/sdk"
	speakeasy_objectvalidators "github.com/criblio/terraform-provider-criblio/internal/validators/objectvalidators"
	speakeasy_stringvalidators "github.com/criblio/terraform-provider-criblio/internal/validators/stringvalidators"
	"github.com/hashicorp/terraform-plugin-framework-validators/float64validator"
	"github.com/hashicorp/terraform-plugin-framework-validators/int64validator"
	"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator"
	"github.com/hashicorp/terraform-plugin-framework/attr"
	"github.com/hashicorp/terraform-plugin-framework/path"
	"github.com/hashicorp/terraform-plugin-framework/resource"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/booldefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/float64default"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/listdefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringdefault"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types"
	"github.com/hashicorp/terraform-plugin-framework/types/basetypes"
	"regexp"
)

// Ensure provider defined types fully satisfy framework interfaces.
var _ resource.Resource = &CollectorResource{}
var _ resource.ResourceWithImportState = &CollectorResource{}

func NewCollectorResource() resource.Resource {
	return &CollectorResource{}
}

// CollectorResource defines the resource implementation.
type CollectorResource struct {
	// Provider configured SDK client.
	client *sdk.CriblIo
}

// CollectorResourceModel describes the resource data model.
type CollectorResourceModel struct {
	Collector            tfTypes.InputCollectorCollector `tfsdk:"collector"`
	Environment          types.String                    `tfsdk:"environment"`
	GroupID              types.String                    `tfsdk:"group_id"`
	ID                   types.String                    `tfsdk:"id"`
	IgnoreGroupJobsLimit types.Bool                      `tfsdk:"ignore_group_jobs_limit"`
	Input                *tfTypes.InputCollectorInput    `tfsdk:"input"`
	RemoveFields         []types.String                  `tfsdk:"remove_fields"`
	ResumeOnBoot         types.Bool                      `tfsdk:"resume_on_boot"`
	Schedule             *tfTypes.InputCollectorSchedule `tfsdk:"schedule"`
	Streamtags           []types.String                  `tfsdk:"streamtags"`
	TTL                  types.String                    `tfsdk:"ttl"`
	WorkerAffinity       types.Bool                      `tfsdk:"worker_affinity"`
}

func (r *CollectorResource) Metadata(ctx context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) {
	resp.TypeName = req.ProviderTypeName + "_collector"
}

func (r *CollectorResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) {
	resp.Schema = schema.Schema{
		MarkdownDescription: "Collector Resource",
		Attributes: map[string]schema.Attribute{
			"collector": schema.SingleNestedAttribute{
				Required: true,
				Attributes: map[string]schema.Attribute{
					"conf": schema.SingleNestedAttribute{
						Computed: true,
						Optional: true,
						Attributes: map[string]schema.Attribute{
							"auth_type": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `must be one of ["manual", "secret", "clientSecret", "clientCert"]`,
								Validators: []validator.String{
									stringvalidator.OneOf(
										"manual",
										"secret",
										"clientSecret",
										"clientCert",
									),
								},
							},
							"authentication": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `must be one of ["none", "basic", "basicSecret", "token", "tokenSecret", "login", "loginSecret", "oauth", "oauthSecret", "google_oauth", "google_oauthSecret", "hmac"]`,
								Validators: []validator.String{
									stringvalidator.OneOf(
										"none",
										"basic",
										"basicSecret",
										"token",
										"tokenSecret",
										"login",
										"loginSecret",
										"oauth",
										"oauthSecret",
										"google_oauth",
										"google_oauthSecret",
										"hmac",
									),
								},
							},
							"aws_api_key": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"aws_authentication_method": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `must be one of ["auto", "manual", "secret"]`,
								Validators: []validator.String{
									stringvalidator.OneOf(
										"auto",
										"manual",
										"secret",
									),
								},
							},
							"aws_secret": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"aws_secret_key": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"bucket": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `S3 Bucket from which to collect data`,
							},
							"collect_method": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `must be one of ["get", "post", "post_with_body", "other"]`,
								Validators: []validator.String{
									stringvalidator.OneOf(
										"get",
										"post",
										"post_with_body",
										"other",
									),
								},
							},
							"collect_url": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `URL to use for the Collect operation`,
							},
							"connection_id": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Select an existing Database Connection`,
							},
							"connection_string": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Azure storage account Connection String`,
							},
							"container_name": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Azure container to collect from`,
							},
							"credentials_secret": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"dataset": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Lake dataset to collect data from`,
							},
							"disable_time_filter": schema.BoolAttribute{
								Computed: true,
								Optional: true,
							},
							"earliest": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Earliest time boundary for the search`,
							},
							"endpoint": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `REST API endpoint used to create a search`,
							},
							"extractors": schema.ListNestedAttribute{
								Computed: true,
								Optional: true,
								NestedObject: schema.NestedAttributeObject{
									Validators: []validator.Object{
										speakeasy_objectvalidators.NotNull(),
									},
									Attributes: map[string]schema.Attribute{},
								},
							},
							"handle_escaped_chars": schema.BoolAttribute{
								Computed: true,
								Optional: true,
							},
							"latest": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Latest time boundary for the search`,
							},
							"max_batch_size": schema.Int64Attribute{
								Computed: true,
								Optional: true,
								Validators: []validator.Int64{
									int64validator.AtLeast(1),
								},
							},
							"output_mode": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `must be one of ["csv", "json"]`,
								Validators: []validator.String{
									stringvalidator.OneOf(
										"csv",
										"json",
									),
								},
							},
							"password": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"path": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Directory where data will be collected`,
							},
							"query": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Query string for selecting data from the database`,
							},
							"query_validation_enabled": schema.BoolAttribute{
								Computed: true,
								Optional: true,
							},
							"recurse": schema.BoolAttribute{
								Computed: true,
								Optional: true,
							},
							"region": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `AWS region from which to retrieve data`,
							},
							"reject_unauthorized": schema.BoolAttribute{
								Computed: true,
								Optional: true,
							},
							"search": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Splunk search query`,
							},
							"search_head": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Search head base URL`,
							},
							"service_account_credentials": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"storage_account_name": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"timeout": schema.Int64Attribute{
								Computed: true,
								Optional: true,
								Validators: []validator.Int64{
									int64validator.AtMost(1800),
								},
							},
							"token": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"token_secret": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
							"use_round_robin_dns": schema.BoolAttribute{
								Computed: true,
								Optional: true,
							},
							"username": schema.StringAttribute{
								Computed: true,
								Optional: true,
							},
						},
					},
					"destructive": schema.BoolAttribute{
						Computed:    true,
						Optional:    true,
						Default:     booldefault.StaticBool(false),
						Description: `Delete any files collected (where applicable). Default: false`,
					},
					"encoding": schema.StringAttribute{
						Computed:    true,
						Optional:    true,
						Default:     stringdefault.StaticString(`utf8`),
						Description: `Character encoding to use when parsing ingested data. Default: "utf8"`,
					},
					"type": schema.StringAttribute{
						Required:    true,
						Description: `must be one of ["splunk", "s3", "azureblob", "cribllake", "database", "gcs", "healthcheck", "rest"]`,
						Validators: []validator.String{
							stringvalidator.OneOf(
								"splunk",
								"s3",
								"azureblob",
								"cribllake",
								"database",
								"gcs",
								"healthcheck",
								"rest",
							),
						},
					},
				},
			},
			"environment": schema.StringAttribute{
				Computed: true,
				Optional: true,
			},
			"group_id": schema.StringAttribute{
				Required:    true,
				Description: `The consumer group to which this instance belongs. Defaults to 'default'.`,
			},
			"id": schema.StringAttribute{
				Computed:    true,
				Optional:    true,
				Description: `Unique ID to PATCH`,
			},
			"ignore_group_jobs_limit": schema.BoolAttribute{
				Computed:    true,
				Optional:    true,
				Default:     booldefault.StaticBool(false),
				Description: `Default: false`,
			},
			"input": schema.SingleNestedAttribute{
				Computed: true,
				Optional: true,
				Attributes: map[string]schema.Attribute{
					"breaker_rulesets": schema.ListAttribute{
						Computed:    true,
						Optional:    true,
						ElementType: types.StringType,
						Description: `A list of event-breaking rulesets that will be applied, in order, to the input data stream`,
					},
					"metadata": schema.ListNestedAttribute{
						Computed: true,
						Optional: true,
						NestedObject: schema.NestedAttributeObject{
							Validators: []validator.Object{
								speakeasy_objectvalidators.NotNull(),
							},
							Attributes: map[string]schema.Attribute{
								"name": schema.StringAttribute{
									Computed:    true,
									Optional:    true,
									Description: `Not Null`,
									Validators: []validator.String{
										speakeasy_stringvalidators.NotNull(),
									},
								},
								"value": schema.StringAttribute{
									Computed:    true,
									Optional:    true,
									Description: `JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.). Not Null`,
									Validators: []validator.String{
										speakeasy_stringvalidators.NotNull(),
									},
								},
							},
						},
						Description: `Fields to add to events from this input`,
					},
					"output": schema.StringAttribute{
						Computed:    true,
						Optional:    true,
						Description: `Destination to send results to`,
					},
					"pipeline": schema.StringAttribute{
						Computed:    true,
						Optional:    true,
						Description: `Pipeline to process results`,
					},
					"preprocess": schema.SingleNestedAttribute{
						Computed: true,
						Optional: true,
						Attributes: map[string]schema.Attribute{
							"args": schema.ListAttribute{
								Computed:    true,
								Optional:    true,
								ElementType: types.StringType,
								Description: `Arguments to be added to the custom command`,
							},
							"command": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Description: `Command to feed the data through (via stdin) and process its output (stdout)`,
							},
							"disabled": schema.BoolAttribute{
								Computed:    true,
								Optional:    true,
								Default:     booldefault.StaticBool(true),
								Description: `Default: true`,
							},
						},
					},
					"send_to_routes": schema.BoolAttribute{
						Computed:    true,
						Optional:    true,
						Default:     booldefault.StaticBool(true),
						Description: `Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination. Default: true`,
					},
					"stale_channel_flush_ms": schema.Float64Attribute{
						Computed:    true,
						Optional:    true,
						Default:     float64default.StaticFloat64(10000),
						Description: `How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines. Default: 10000`,
						Validators: []validator.Float64{
							float64validator.Between(10, 43200000),
						},
					},
					"throttle_rate_per_sec": schema.StringAttribute{
						Computed:    true,
						Optional:    true,
						Default:     stringdefault.StaticString(`0`),
						Description: `Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling. Default: "0"`,
						Validators: []validator.String{
							stringvalidator.RegexMatches(regexp.MustCompile(`^[\d.]+(\s[KMGTPEZYkmgtpezy][Bb])?$`), "must match pattern "+regexp.MustCompile(`^[\d.]+(\s[KMGTPEZYkmgtpezy][Bb])?$`).String()),
						},
					},
					"type": schema.StringAttribute{
						Computed:    true,
						Optional:    true,
						Default:     stringdefault.StaticString(`collection`),
						Description: `Default: "collection"; must be "collection"`,
						Validators: []validator.String{
							stringvalidator.OneOf(
								"collection",
							),
						},
					},
				},
			},
			"remove_fields": schema.ListAttribute{
				Computed:    true,
				Optional:    true,
				Default:     listdefault.StaticValue(types.ListValueMust(types.StringType, []attr.Value{})),
				ElementType: types.StringType,
			},
			"resume_on_boot": schema.BoolAttribute{
				Computed:    true,
				Optional:    true,
				Default:     booldefault.StaticBool(true),
				Description: `Default: true`,
			},
			"schedule": schema.SingleNestedAttribute{
				Computed: true,
				Optional: true,
				Attributes: map[string]schema.Attribute{
					"cron_schedule": schema.StringAttribute{
						Computed:    true,
						Optional:    true,
						Default:     stringdefault.StaticString(`*/5 * * * *`),
						Description: `A cron schedule on which to run this job. Default: "*/5 * * * *"`,
					},
					"enabled": schema.BoolAttribute{
						Computed:    true,
						Optional:    true,
						Description: `Enable to configure scheduling for this Collector`,
					},
					"max_concurrent_runs": schema.Float64Attribute{
						Computed:    true,
						Optional:    true,
						Default:     float64default.StaticFloat64(1),
						Description: `The maximum number of instances of this scheduled job that may be running at any time. Default: 1`,
						Validators: []validator.Float64{
							float64validator.AtLeast(1),
						},
					},
					"run": schema.SingleNestedAttribute{
						Computed: true,
						Optional: true,
						Attributes: map[string]schema.Attribute{
							"earliest": schema.Float64Attribute{
								Computed:    true,
								Optional:    true,
								Description: `Earliest time to collect data for the selected timezone`,
							},
							"expression": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`true`),
								Description: `A filter for tokens in the provided collect path and/or the events being collected. Default: "true"`,
							},
							"job_timeout": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`0`),
								Description: `Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time. Default: "0"`,
								Validators: []validator.String{
									stringvalidator.RegexMatches(regexp.MustCompile(`\d+[sm]?$`), "must match pattern "+regexp.MustCompile(`\d+[sm]?$`).String()),
								},
							},
							"latest": schema.Float64Attribute{
								Computed:    true,
								Optional:    true,
								Description: `Latest time to collect data for the selected timezone`,
							},
							"log_level": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`info`),
								Description: `Level at which to set task logging. Default: "info"; must be one of ["error", "warn", "info", "debug", "silly"]`,
								Validators: []validator.String{
									stringvalidator.OneOf(
										"error",
										"warn",
										"info",
										"debug",
										"silly",
									),
								},
							},
							"max_task_reschedule": schema.Float64Attribute{
								Computed:    true,
								Optional:    true,
								Default:     float64default.StaticFloat64(1),
								Description: `Maximum number of times a task can be rescheduled. Default: 1`,
								Validators: []validator.Float64{
									float64validator.AtLeast(1),
								},
							},
							"max_task_size": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`10MB`),
								Description: `Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB, you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks. Default: "10MB"`,
								Validators: []validator.String{
									stringvalidator.RegexMatches(regexp.MustCompile(`^((\d*\.?\d+)((KB|MB|GB|TB|PB|EB|ZB|YB|kb|mb|gb|tb|pb|eb|zb|yb){1}))$`), "must match pattern "+regexp.MustCompile(`^((\d*\.?\d+)((KB|MB|GB|TB|PB|EB|ZB|YB|kb|mb|gb|tb|pb|eb|zb|yb){1}))$`).String()),
								},
							},
							"min_task_size": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`1MB`),
								Description: `Limits the bundle size for small tasks. For example, if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task. Default: "1MB"`,
								Validators: []validator.String{
									stringvalidator.RegexMatches(regexp.MustCompile(`^((\d*\.?\d+)((KB|MB|GB|TB|PB|EB|ZB|YB|kb|mb|gb|tb|pb|eb|zb|yb){1}))$`), "must match pattern "+regexp.MustCompile(`^((\d*\.?\d+)((KB|MB|GB|TB|PB|EB|ZB|YB|kb|mb|gb|tb|pb|eb|zb|yb){1}))$`).String()),
								},
							},
							"mode": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`list`),
								Description: `Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job. Default: "list"`,
							},
							"reschedule_dropped_tasks": schema.BoolAttribute{
								Computed:    true,
								Optional:    true,
								Default:     booldefault.StaticBool(true),
								Description: `Reschedule tasks that failed with non-fatal errors. Default: true`,
							},
							"time_range_type": schema.StringAttribute{
								Computed:    true,
								Optional:    true,
								Default:     stringdefault.StaticString(`relative`),
								Description: `Default: "relative"`,
							},
						},
					},
					"skippable": schema.BoolAttribute{
						Computed:    true,
						Optional:    true,
						Default:     booldefault.StaticBool(true),
						Description: `Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits. Default: true`,
					},
				},
				Description: `Configuration for a scheduled job`,
			},
			"streamtags": schema.ListAttribute{
				Computed:    true,
				Optional:    true,
				Default:     listdefault.StaticValue(types.ListValueMust(types.StringType, []attr.Value{})),
				ElementType: types.StringType,
				Description: `Tags for filtering and grouping`,
			},
			"ttl": schema.StringAttribute{
				Computed:    true,
				Optional:    true,
				Default:     stringdefault.StaticString(`4h`),
				Description: `Default: "4h"`,
			},
			"worker_affinity": schema.BoolAttribute{
				Computed:    true,
				Optional:    true,
				Default:     booldefault.StaticBool(false),
				Description: `If enabled, tasks are created and run by the same Worker Node. Default: false`,
			},
		},
	}
}

func (r *CollectorResource) Configure(ctx context.Context, req resource.ConfigureRequest, resp *resource.ConfigureResponse) {
	// Prevent panic if the provider has not been configured.
	if req.ProviderData == nil {
		return
	}

	client, ok := req.ProviderData.(*sdk.CriblIo)

	if !ok {
		resp.Diagnostics.AddError(
			"Unexpected Resource Configure Type",
			fmt.Sprintf("Expected *sdk.CriblIo, got: %T. Please report this issue to the provider developers.", req.ProviderData),
		)

		return
	}

	r.client = client
}

func (r *CollectorResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {
	var data *CollectorResourceModel
	var plan types.Object

	resp.Diagnostics.Append(req.Plan.Get(ctx, &plan)...)
	if resp.Diagnostics.HasError() {
		return
	}

	resp.Diagnostics.Append(plan.As(ctx, &data, basetypes.ObjectAsOptions{
		UnhandledNullAsEmpty:    true,
		UnhandledUnknownAsEmpty: true,
	})...)

	if resp.Diagnostics.HasError() {
		return
	}

	request, requestDiags := data.ToOperationsCreateSavedJobRequest(ctx)
	resp.Diagnostics.Append(requestDiags...)

	if resp.Diagnostics.HasError() {
		return
	}
	res, err := r.client.SavedJobs.CreateSavedJob(ctx, *request)
	if err != nil {
		resp.Diagnostics.AddError("failure to invoke API", err.Error())
		if res != nil && res.RawResponse != nil {
			resp.Diagnostics.AddError("unexpected http request/response", debugResponse(res.RawResponse))
		}
		return
	}
	if res == nil {
		resp.Diagnostics.AddError("unexpected response from API", fmt.Sprintf("%v", res))
		return
	}
	if res.StatusCode != 200 {
		resp.Diagnostics.AddError(fmt.Sprintf("unexpected response from API. Got an unexpected response code %v", res.StatusCode), debugResponse(res.RawResponse))
		return
	}
	if !(res.Object != nil && res.Object.Items != nil && len(res.Object.Items) > 0) {
		resp.Diagnostics.AddError("unexpected response from API. Got an unexpected response body", debugResponse(res.RawResponse))
		return
	}
	resp.Diagnostics.Append(data.RefreshFromSharedInputCollector(ctx, &res.Object.Items[0])...)

	if resp.Diagnostics.HasError() {
		return
	}

	resp.Diagnostics.Append(refreshPlan(ctx, plan, &data)...)

	if resp.Diagnostics.HasError() {
		return
	}

	// Save updated data into Terraform state
	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *CollectorResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) {
	var data *CollectorResourceModel
	var item types.Object

	resp.Diagnostics.Append(req.State.Get(ctx, &item)...)
	if resp.Diagnostics.HasError() {
		return
	}

	resp.Diagnostics.Append(item.As(ctx, &data, basetypes.ObjectAsOptions{
		UnhandledNullAsEmpty:    true,
		UnhandledUnknownAsEmpty: true,
	})...)

	if resp.Diagnostics.HasError() {
		return
	}

	request, requestDiags := data.ToOperationsGetSavedJobByIDRequest(ctx)
	resp.Diagnostics.Append(requestDiags...)

	if resp.Diagnostics.HasError() {
		return
	}
	res, err := r.client.SavedJobs.GetSavedJobByID(ctx, *request)
	if err != nil {
		resp.Diagnostics.AddError("failure to invoke API", err.Error())
		if res != nil && res.RawResponse != nil {
			resp.Diagnostics.AddError("unexpected http request/response", debugResponse(res.RawResponse))
		}
		return
	}
	if res == nil {
		resp.Diagnostics.AddError("unexpected response from API", fmt.Sprintf("%v", res))
		return
	}
	if res.StatusCode == 404 {
		resp.State.RemoveResource(ctx)
		return
	}
	if res.StatusCode != 200 {
		resp.Diagnostics.AddError(fmt.Sprintf("unexpected response from API. Got an unexpected response code %v", res.StatusCode), debugResponse(res.RawResponse))
		return
	}
	if !(res.Object != nil && res.Object.Items != nil && len(res.Object.Items) > 0) {
		resp.Diagnostics.AddError("unexpected response from API. Got an unexpected response body", debugResponse(res.RawResponse))
		return
	}
	resp.Diagnostics.Append(data.RefreshFromSharedInputCollector(ctx, &res.Object.Items[0])...)

	if resp.Diagnostics.HasError() {
		return
	}

	// Save updated data into Terraform state
	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *CollectorResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) {
	var data *CollectorResourceModel
	var plan types.Object

	resp.Diagnostics.Append(req.Plan.Get(ctx, &plan)...)
	if resp.Diagnostics.HasError() {
		return
	}

	merge(ctx, req, resp, &data)
	if resp.Diagnostics.HasError() {
		return
	}

	request, requestDiags := data.ToOperationsUpdateCollectorByIDRequest(ctx)
	resp.Diagnostics.Append(requestDiags...)

	if resp.Diagnostics.HasError() {
		return
	}
	res, err := r.client.SavedJobs.UpdateCollectorByID(ctx, *request)
	if err != nil {
		resp.Diagnostics.AddError("failure to invoke API", err.Error())
		if res != nil && res.RawResponse != nil {
			resp.Diagnostics.AddError("unexpected http request/response", debugResponse(res.RawResponse))
		}
		return
	}
	if res == nil {
		resp.Diagnostics.AddError("unexpected response from API", fmt.Sprintf("%v", res))
		return
	}
	if res.StatusCode != 200 {
		resp.Diagnostics.AddError(fmt.Sprintf("unexpected response from API. Got an unexpected response code %v", res.StatusCode), debugResponse(res.RawResponse))
		return
	}
	if !(res.Object != nil && res.Object.Items != nil && len(res.Object.Items) > 0) {
		resp.Diagnostics.AddError("unexpected response from API. Got an unexpected response body", debugResponse(res.RawResponse))
		return
	}
	resp.Diagnostics.Append(data.RefreshFromSharedInputCollector(ctx, &res.Object.Items[0])...)

	if resp.Diagnostics.HasError() {
		return
	}

	resp.Diagnostics.Append(refreshPlan(ctx, plan, &data)...)

	if resp.Diagnostics.HasError() {
		return
	}

	// Save updated data into Terraform state
	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *CollectorResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {
	var data *CollectorResourceModel
	var item types.Object

	resp.Diagnostics.Append(req.State.Get(ctx, &item)...)
	if resp.Diagnostics.HasError() {
		return
	}

	resp.Diagnostics.Append(item.As(ctx, &data, basetypes.ObjectAsOptions{
		UnhandledNullAsEmpty:    true,
		UnhandledUnknownAsEmpty: true,
	})...)

	if resp.Diagnostics.HasError() {
		return
	}

	request, requestDiags := data.ToOperationsDeleteCollectorByIDRequest(ctx)
	resp.Diagnostics.Append(requestDiags...)

	if resp.Diagnostics.HasError() {
		return
	}
	res, err := r.client.SavedJobs.DeleteCollectorByID(ctx, *request)
	if err != nil {
		resp.Diagnostics.AddError("failure to invoke API", err.Error())
		if res != nil && res.RawResponse != nil {
			resp.Diagnostics.AddError("unexpected http request/response", debugResponse(res.RawResponse))
		}
		return
	}
	if res == nil {
		resp.Diagnostics.AddError("unexpected response from API", fmt.Sprintf("%v", res))
		return
	}
	if res.StatusCode != 200 {
		resp.Diagnostics.AddError(fmt.Sprintf("unexpected response from API. Got an unexpected response code %v", res.StatusCode), debugResponse(res.RawResponse))
		return
	}

}

func (r *CollectorResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {
	dec := json.NewDecoder(bytes.NewReader([]byte(req.ID)))
	dec.DisallowUnknownFields()
	var data struct {
		GroupID string `json:"group_id"`
		ID      string `json:"id"`
	}

	if err := dec.Decode(&data); err != nil {
		resp.Diagnostics.AddError("Invalid ID", `The import ID is not valid. It is expected to be a JSON object string with the format: '{"group_id": "default", "id": "..."}': `+err.Error())
		return
	}

	if len(data.GroupID) == 0 {
		resp.Diagnostics.AddError("Missing required field", `The field group_id is required but was not found in the json encoded ID. It's expected to be a value alike '"default"`)
		return
	}
	resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root("group_id"), data.GroupID)...)
	if len(data.ID) == 0 {
		resp.Diagnostics.AddError("Missing required field", `The field id is required but was not found in the json encoded ID. It's expected to be a value alike '""`)
		return
	}
	resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root("id"), data.ID)...)
}
